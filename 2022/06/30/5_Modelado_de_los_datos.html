<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>5) GENERACION DEL MODELO PREDICTIVO | Football Match Prediction</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="5) GENERACION DEL MODELO PREDICTIVO" />
<meta name="author" content="Iván Fernández Aguirre" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Balanceo del dataset y modelado de los resultados en función de las variables desarrolladas" />
<meta property="og:description" content="Balanceo del dataset y modelado de los resultados en función de las variables desarrolladas" />
<link rel="canonical" href="https://faegru.github.io/FMP_UB_final_project_2022/2022/06/30/5_Modelado_de_los_datos.html" />
<meta property="og:url" content="https://faegru.github.io/FMP_UB_final_project_2022/2022/06/30/5_Modelado_de_los_datos.html" />
<meta property="og:site_name" content="Football Match Prediction" />
<meta property="og:image" content="https://faegru.github.io/FMP_UB_final_project_2022/images/N5.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-06-30T00:00:00-05:00" />
<meta name="twitter:card" content="summary_large_image" />
<meta property="twitter:image" content="https://faegru.github.io/FMP_UB_final_project_2022/images/N5.png" />
<meta property="twitter:title" content="5) GENERACION DEL MODELO PREDICTIVO" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Iván Fernández Aguirre"},"dateModified":"2022-06-30T00:00:00-05:00","datePublished":"2022-06-30T00:00:00-05:00","description":"Balanceo del dataset y modelado de los resultados en función de las variables desarrolladas","headline":"5) GENERACION DEL MODELO PREDICTIVO","image":"https://faegru.github.io/FMP_UB_final_project_2022/images/N5.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://faegru.github.io/FMP_UB_final_project_2022/2022/06/30/5_Modelado_de_los_datos.html"},"url":"https://faegru.github.io/FMP_UB_final_project_2022/2022/06/30/5_Modelado_de_los_datos.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/FMP_UB_final_project_2022/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://faegru.github.io/FMP_UB_final_project_2022/feed.xml" title="Football Match Prediction" /><link rel="shortcut icon" type="image/x-icon" href="/FMP_UB_final_project_2022/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/FMP_UB_final_project_2022/">Football Match Prediction</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/FMP_UB_final_project_2022/about/">About Me</a><a class="page-link" href="/FMP_UB_final_project_2022/search/">Search</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">5) GENERACION DEL MODELO PREDICTIVO</h1><p class="page-description">Balanceo del dataset y modelado de los resultados en función de las variables desarrolladas</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-06-30T00:00:00-05:00" itemprop="datePublished">
        Jun 30, 2022
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Iván Fernández Aguirre</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      772 min read
    
</span></p>

    

    
      
        <div class="pb-5 d-flex flex-justify-center">
          <div class="px-2">

    <a href="https://github.com/Faegru/FMP_UB_final_project_2022/tree/master/_notebooks/2022-06-30-5_Modelado_de_los_datos.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/FMP_UB_final_project_2022/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          
          
          
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#CARGA-DE-DATOS">CARGA DE DATOS </a></li>
<li class="toc-entry toc-h2"><a href="#BALANCE-DE-CLASES">BALANCE DE CLASES </a>
<ul>
<li class="toc-entry toc-h3"><a href="#1)-Balance-de-derrotas-y-victorias">1) Balance de derrotas y victorias </a></li>
<li class="toc-entry toc-h3"><a href="#2)-Balance-de-los-empates">2) Balance de los empates </a>
<ul>
<li class="toc-entry toc-h4"><a href="#Separación-de-los-datos-en-Train-y-Test">Separación de los datos en Train y Test </a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#MODELADO">MODELADO </a>
<ul>
<li class="toc-entry toc-h3"><a href="#1)-Random-Forest">1) Random Forest </a>
<ul>
<li class="toc-entry toc-h4"><a href="#A)-RF-con-todas-las-variables">A) RF con todas las variables </a></li>
<li class="toc-entry toc-h4"><a href="#B)-PCA-y-RF">B) PCA y RF </a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#2)-Red-Neuronal">2) Red Neuronal </a></li>
</ul>
</li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2022-06-30-5_Modelado_de_los_datos.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>La mayoria de las funciones utilizadas en este projecto están en el modulo:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">Functions_for_FMP</span> <span class="k">as</span> <span class="nn">fffmp</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="CARGA-DE-DATOS">
<a class="anchor" href="#CARGA-DE-DATOS" aria-hidden="true"><span class="octicon octicon-link"></span></a>CARGA DE DATOS<a class="anchor-link" href="#CARGA-DE-DATOS"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Empezamos, como siempre, cargando los datos. En este caso el <em>dataset</em> generado en el bloque 2).</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data</span><span class="o">=</span><span class="s1">'FMP_final'</span>
<span class="n">FMP</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_pickle</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">FMP</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>target</th>
      <th>is_cup</th>
      <th>home_team_history_is_play_home_1</th>
      <th>home_team_history_is_play_home_2</th>
      <th>home_team_history_is_play_home_3</th>
      <th>home_team_history_is_play_home_4</th>
      <th>home_team_history_is_play_home_5</th>
      <th>home_team_history_is_play_home_6</th>
      <th>home_team_history_is_play_home_7</th>
      <th>home_team_history_is_play_home_8</th>
      <th>home_team_history_is_play_home_9</th>
      <th>home_team_history_is_play_home_10</th>
      <th>home_team_history_is_cup_1</th>
      <th>home_team_history_is_cup_2</th>
      <th>home_team_history_is_cup_3</th>
      <th>home_team_history_is_cup_4</th>
      <th>home_team_history_is_cup_5</th>
      <th>home_team_history_is_cup_6</th>
      <th>home_team_history_is_cup_7</th>
      <th>home_team_history_is_cup_8</th>
      <th>home_team_history_is_cup_9</th>
      <th>home_team_history_is_cup_10</th>
      <th>away_team_history_is_play_home_1</th>
      <th>away_team_history_is_play_home_2</th>
      <th>away_team_history_is_play_home_3</th>
      <th>away_team_history_is_play_home_4</th>
      <th>away_team_history_is_play_home_5</th>
      <th>away_team_history_is_play_home_6</th>
      <th>away_team_history_is_play_home_7</th>
      <th>away_team_history_is_play_home_8</th>
      <th>away_team_history_is_play_home_9</th>
      <th>away_team_history_is_play_home_10</th>
      <th>away_team_history_is_cup_1</th>
      <th>away_team_history_is_cup_2</th>
      <th>away_team_history_is_cup_3</th>
      <th>away_team_history_is_cup_4</th>
      <th>away_team_history_is_cup_5</th>
      <th>away_team_history_is_cup_6</th>
      <th>away_team_history_is_cup_7</th>
      <th>away_team_history_is_cup_8</th>
      <th>away_team_history_is_cup_9</th>
      <th>away_team_history_is_cup_10</th>
      <th>home_rating_diff_1</th>
      <th>home_rating_diff_2</th>
      <th>home_rating_diff_3</th>
      <th>home_rating_diff_4</th>
      <th>home_rating_diff_5</th>
      <th>home_rating_diff_6</th>
      <th>home_rating_diff_7</th>
      <th>home_rating_diff_8</th>
      <th>home_rating_diff_9</th>
      <th>home_rating_diff_10</th>
      <th>away_rating_diff_1</th>
      <th>away_rating_diff_2</th>
      <th>away_rating_diff_3</th>
      <th>away_rating_diff_4</th>
      <th>away_rating_diff_5</th>
      <th>away_rating_diff_6</th>
      <th>away_rating_diff_7</th>
      <th>away_rating_diff_8</th>
      <th>away_rating_diff_9</th>
      <th>away_rating_diff_10</th>
      <th>Rating_diff</th>
      <th>home_goal_diff_1</th>
      <th>home_goal_diff_2</th>
      <th>home_goal_diff_3</th>
      <th>home_goal_diff_4</th>
      <th>home_goal_diff_5</th>
      <th>home_goal_diff_6</th>
      <th>home_goal_diff_7</th>
      <th>home_goal_diff_8</th>
      <th>home_goal_diff_9</th>
      <th>home_goal_diff_10</th>
      <th>away_goal_diff_1</th>
      <th>away_goal_diff_2</th>
      <th>away_goal_diff_3</th>
      <th>away_goal_diff_4</th>
      <th>away_goal_diff_5</th>
      <th>away_goal_diff_6</th>
      <th>away_goal_diff_7</th>
      <th>away_goal_diff_8</th>
      <th>away_goal_diff_9</th>
      <th>away_goal_diff_10</th>
      <th>home_coach_continuity_1</th>
      <th>home_coach_continuity_2</th>
      <th>home_coach_continuity_3</th>
      <th>home_coach_continuity_4</th>
      <th>home_coach_continuity_5</th>
      <th>home_coach_continuity_6</th>
      <th>home_coach_continuity_7</th>
      <th>home_coach_continuity_8</th>
      <th>home_coach_continuity_9</th>
      <th>home_coach_continuity</th>
      <th>home_coach_continuity_10</th>
      <th>away_coach_continuity_1</th>
      <th>away_coach_continuity_2</th>
      <th>away_coach_continuity_3</th>
      <th>away_coach_continuity_4</th>
      <th>away_coach_continuity_5</th>
      <th>away_coach_continuity_6</th>
      <th>away_coach_continuity_7</th>
      <th>away_coach_continuity_8</th>
      <th>away_coach_continuity_9</th>
      <th>away_coach_continuity</th>
      <th>away_coach_continuity_10</th>
      <th>home_partidos_tres_semanas</th>
      <th>home_partidos_diez_dias</th>
      <th>home_partidos_cuatro_dias</th>
      <th>away_partidos_tres_semanas</th>
      <th>away_partidos_diez_dias</th>
      <th>away_partidos_cuatro_dias</th>
      <th>diff_num_partidos_tres_semanas</th>
      <th>diff_num_partidos_diez_dias</th>
      <th>diff_num_partidos_cuatro_dias</th>
      <th>home_relevance_1</th>
      <th>home_relevance_2</th>
      <th>home_relevance_3</th>
      <th>home_relevance_4</th>
      <th>home_relevance_5</th>
      <th>home_relevance_6</th>
      <th>home_relevance_7</th>
      <th>home_relevance_8</th>
      <th>home_relevance_9</th>
      <th>home_relevance_10</th>
      <th>away_relevance_1</th>
      <th>away_relevance_2</th>
      <th>away_relevance_3</th>
      <th>away_relevance_4</th>
      <th>away_relevance_5</th>
      <th>away_relevance_6</th>
      <th>away_relevance_7</th>
      <th>away_relevance_8</th>
      <th>away_relevance_9</th>
      <th>away_relevance_10</th>
      <th>home_outcome_V_1</th>
      <th>home_outcome_D_1</th>
      <th>home_outcome_V_2</th>
      <th>home_outcome_D_2</th>
      <th>home_outcome_V_3</th>
      <th>home_outcome_D_3</th>
      <th>home_outcome_V_4</th>
      <th>home_outcome_D_4</th>
      <th>home_outcome_V_5</th>
      <th>home_outcome_D_5</th>
      <th>home_outcome_V_6</th>
      <th>home_outcome_D_6</th>
      <th>home_outcome_V_7</th>
      <th>home_outcome_D_7</th>
      <th>home_outcome_V_8</th>
      <th>home_outcome_D_8</th>
      <th>home_outcome_V_9</th>
      <th>home_outcome_D_9</th>
      <th>home_outcome_V_10</th>
      <th>home_outcome_D_10</th>
      <th>away_outcome_V_1</th>
      <th>away_outcome_D_1</th>
      <th>away_outcome_V_2</th>
      <th>away_outcome_D_2</th>
      <th>away_outcome_V_3</th>
      <th>away_outcome_D_3</th>
      <th>away_outcome_V_4</th>
      <th>away_outcome_D_4</th>
      <th>away_outcome_V_5</th>
      <th>away_outcome_D_5</th>
      <th>away_outcome_V_6</th>
      <th>away_outcome_D_6</th>
      <th>away_outcome_V_7</th>
      <th>away_outcome_D_7</th>
      <th>away_outcome_V_8</th>
      <th>away_outcome_D_8</th>
      <th>away_outcome_V_9</th>
      <th>away_outcome_D_9</th>
      <th>away_outcome_V_10</th>
      <th>away_outcome_D_10</th>
      <th>home_is_friendly_1</th>
      <th>home_is_friendly_2</th>
      <th>home_is_friendly_3</th>
      <th>home_is_friendly_4</th>
      <th>home_is_friendly_5</th>
      <th>home_is_friendly_6</th>
      <th>home_is_friendly_7</th>
      <th>home_is_friendly_8</th>
      <th>home_is_friendly_9</th>
      <th>home_is_friendly_10</th>
      <th>away_is_friendly_1</th>
      <th>away_is_friendly_2</th>
      <th>away_is_friendly_3</th>
      <th>away_is_friendly_4</th>
      <th>away_is_friendly_5</th>
      <th>away_is_friendly_6</th>
      <th>away_is_friendly_7</th>
      <th>away_is_friendly_8</th>
      <th>away_is_friendly_9</th>
      <th>away_is_friendly_10</th>
      <th>is_friendly</th>
      <th>home_result_ponderado_1</th>
      <th>home_result_ponderado_2</th>
      <th>home_result_ponderado_3</th>
      <th>home_result_ponderado_4</th>
      <th>home_result_ponderado_5</th>
      <th>home_result_ponderado_6</th>
      <th>home_result_ponderado_7</th>
      <th>home_result_ponderado_8</th>
      <th>home_result_ponderado_9</th>
      <th>home_result_ponderado_10</th>
      <th>away_result_ponderado_1</th>
      <th>away_result_ponderado_2</th>
      <th>away_result_ponderado_3</th>
      <th>away_result_ponderado_4</th>
      <th>away_result_ponderado_5</th>
      <th>away_result_ponderado_6</th>
      <th>away_result_ponderado_7</th>
      <th>away_result_ponderado_8</th>
      <th>away_result_ponderado_9</th>
      <th>away_result_ponderado_10</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>11906497</td>
      <td>away</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-1.342980</td>
      <td>2.030337</td>
      <td>-0.796066</td>
      <td>3.485692</td>
      <td>1.398131</td>
      <td>1.959759</td>
      <td>-4.030432</td>
      <td>5.093217</td>
      <td>-0.487941</td>
      <td>3.651948</td>
      <td>-0.655225</td>
      <td>9.005604</td>
      <td>6.260047</td>
      <td>4.982057</td>
      <td>8.195400</td>
      <td>-1.206981</td>
      <td>1.859646</td>
      <td>9.929757</td>
      <td>7.913075</td>
      <td>1.239743</td>
      <td>-3.218928</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>-4.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>5</td>
      <td>20</td>
      <td>27</td>
      <td>32</td>
      <td>42</td>
      <td>57</td>
      <td>63</td>
      <td>69</td>
      <td>76</td>
      <td>90</td>
      <td>7</td>
      <td>16</td>
      <td>20</td>
      <td>28</td>
      <td>32</td>
      <td>39</td>
      <td>43</td>
      <td>50</td>
      <td>55</td>
      <td>60</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.183486</td>
      <td>0.458626</td>
      <td>-0.114532</td>
      <td>-1.657239</td>
      <td>0.307454</td>
      <td>-0.008029</td>
      <td>-0.018004</td>
      <td>1.191019</td>
      <td>-0.002609</td>
      <td>1.277268</td>
      <td>-0.096775</td>
      <td>2.126546</td>
      <td>-0.775091</td>
      <td>0.571638</td>
      <td>0.948841</td>
      <td>-0.166340</td>
      <td>-0.007594</td>
      <td>2.347528</td>
      <td>1.865301</td>
      <td>0.269580</td>
    </tr>
    <tr>
      <th>1</th>
      <td>11984383</td>
      <td>home</td>
      <td>0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>9.997190</td>
      <td>-0.153606</td>
      <td>5.810161</td>
      <td>-0.033087</td>
      <td>3.685993</td>
      <td>2.520600</td>
      <td>0.976328</td>
      <td>9.513444</td>
      <td>2.707193</td>
      <td>3.916450</td>
      <td>-0.881175</td>
      <td>-2.148550</td>
      <td>-4.238788</td>
      <td>0.145350</td>
      <td>4.425333</td>
      <td>-0.976328</td>
      <td>-1.943804</td>
      <td>-0.007279</td>
      <td>-3.834623</td>
      <td>-5.890736</td>
      <td>2.893940</td>
      <td>-2.0</td>
      <td>-2.0</td>
      <td>4.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-4.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-4.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>3.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3</td>
      <td>7</td>
      <td>21</td>
      <td>28</td>
      <td>35</td>
      <td>38</td>
      <td>41</td>
      <td>44</td>
      <td>56</td>
      <td>62</td>
      <td>3</td>
      <td>7</td>
      <td>21</td>
      <td>28</td>
      <td>34</td>
      <td>41</td>
      <td>45</td>
      <td>63</td>
      <td>71</td>
      <td>77</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-2.449570</td>
      <td>-0.065908</td>
      <td>2.749671</td>
      <td>-0.018337</td>
      <td>-0.450559</td>
      <td>0.282696</td>
      <td>-0.108928</td>
      <td>2.247980</td>
      <td>-0.327153</td>
      <td>-0.016532</td>
      <td>0.116627</td>
      <td>-0.285051</td>
      <td>-2.137610</td>
      <td>-0.000143</td>
      <td>-0.543773</td>
      <td>0.127797</td>
      <td>-1.010141</td>
      <td>-0.000520</td>
      <td>-0.497628</td>
      <td>2.166224</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11983301</td>
      <td>draw</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-1.088475</td>
      <td>5.692117</td>
      <td>0.391980</td>
      <td>2.249836</td>
      <td>-3.912490</td>
      <td>3.762830</td>
      <td>-2.639839</td>
      <td>9.112116</td>
      <td>-2.128075</td>
      <td>1.532352</td>
      <td>2.892081</td>
      <td>-2.847612</td>
      <td>4.490550</td>
      <td>1.803989</td>
      <td>1.790383</td>
      <td>3.918600</td>
      <td>2.639839</td>
      <td>2.319737</td>
      <td>1.151759</td>
      <td>6.159063</td>
      <td>-1.936780</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2</td>
      <td>9</td>
      <td>27</td>
      <td>35</td>
      <td>41</td>
      <td>45</td>
      <td>56</td>
      <td>63</td>
      <td>69</td>
      <td>73</td>
      <td>9</td>
      <td>20</td>
      <td>35</td>
      <td>41</td>
      <td>45</td>
      <td>49</td>
      <td>56</td>
      <td>63</td>
      <td>69</td>
      <td>73</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.287141</td>
      <td>1.334228</td>
      <td>-0.035255</td>
      <td>0.250912</td>
      <td>-0.507446</td>
      <td>-0.015865</td>
      <td>-0.011961</td>
      <td>1.056451</td>
      <td>-0.555203</td>
      <td>-0.006171</td>
      <td>0.664686</td>
      <td>-0.373188</td>
      <td>-0.019027</td>
      <td>0.404502</td>
      <td>0.196978</td>
      <td>0.446802</td>
      <td>-0.010984</td>
      <td>0.259117</td>
      <td>0.501601</td>
      <td>-0.026278</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11983471</td>
      <td>away</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.000000</td>
      <td>-0.163546</td>
      <td>3.748775</td>
      <td>3.382067</td>
      <td>0.296500</td>
      <td>3.746475</td>
      <td>-0.026269</td>
      <td>8.459475</td>
      <td>1.716836</td>
      <td>3.508762</td>
      <td>0.000000</td>
      <td>-1.646854</td>
      <td>3.836917</td>
      <td>-3.382067</td>
      <td>2.604050</td>
      <td>-0.296250</td>
      <td>-3.497111</td>
      <td>0.928375</td>
      <td>-2.725695</td>
      <td>-2.332587</td>
      <td>1.948494</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3</td>
      <td>7</td>
      <td>20</td>
      <td>26</td>
      <td>29</td>
      <td>35</td>
      <td>41</td>
      <td>56</td>
      <td>64</td>
      <td>66</td>
      <td>3</td>
      <td>7</td>
      <td>22</td>
      <td>26</td>
      <td>30</td>
      <td>36</td>
      <td>42</td>
      <td>56</td>
      <td>59</td>
      <td>65</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.000488</td>
      <td>0.032387</td>
      <td>1.754879</td>
      <td>-0.014210</td>
      <td>-0.023217</td>
      <td>0.426597</td>
      <td>0.016272</td>
      <td>-0.036276</td>
      <td>-0.202291</td>
      <td>-0.014760</td>
      <td>-0.000488</td>
      <td>-0.435951</td>
      <td>0.437214</td>
      <td>-0.015186</td>
      <td>1.505773</td>
      <td>-0.001776</td>
      <td>0.423703</td>
      <td>-0.102883</td>
      <td>0.333149</td>
      <td>-0.308254</td>
    </tr>
    <tr>
      <th>4</th>
      <td>11883005</td>
      <td>home</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.760229</td>
      <td>2.777133</td>
      <td>-0.958567</td>
      <td>3.247532</td>
      <td>-2.595750</td>
      <td>7.854948</td>
      <td>-0.296412</td>
      <td>2.539583</td>
      <td>-0.512550</td>
      <td>6.050450</td>
      <td>0.774210</td>
      <td>-0.007142</td>
      <td>0.361725</td>
      <td>3.602960</td>
      <td>-4.391750</td>
      <td>3.172159</td>
      <td>-5.302729</td>
      <td>0.526319</td>
      <td>3.559067</td>
      <td>-2.741227</td>
      <td>0.456468</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-2.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3</td>
      <td>7</td>
      <td>20</td>
      <td>28</td>
      <td>34</td>
      <td>38</td>
      <td>41</td>
      <td>55</td>
      <td>59</td>
      <td>63</td>
      <td>3</td>
      <td>6</td>
      <td>20</td>
      <td>28</td>
      <td>35</td>
      <td>38</td>
      <td>41</td>
      <td>56</td>
      <td>59</td>
      <td>62</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.081683</td>
      <td>0.312809</td>
      <td>0.125712</td>
      <td>0.749681</td>
      <td>-0.011769</td>
      <td>0.908877</td>
      <td>0.047984</td>
      <td>-0.010548</td>
      <td>-0.078787</td>
      <td>-0.025806</td>
      <td>0.238832</td>
      <td>-0.015066</td>
      <td>-0.001084</td>
      <td>0.409750</td>
      <td>-0.019574</td>
      <td>0.731658</td>
      <td>-1.341916</td>
      <td>0.048594</td>
      <td>1.663329</td>
      <td>-0.012401</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>110933</th>
      <td>18030016</td>
      <td>draw</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>4.314395</td>
      <td>3.097305</td>
      <td>-1.018100</td>
      <td>2.500928</td>
      <td>4.521543</td>
      <td>4.543730</td>
      <td>-1.999606</td>
      <td>0.715786</td>
      <td>0.236309</td>
      <td>0.972825</td>
      <td>-2.231214</td>
      <td>-6.059825</td>
      <td>-3.341627</td>
      <td>-1.723150</td>
      <td>-0.445700</td>
      <td>-3.777150</td>
      <td>-0.336883</td>
      <td>0.480100</td>
      <td>1.219725</td>
      <td>4.648450</td>
      <td>0.000000</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7</td>
      <td>12</td>
      <td>141</td>
      <td>142</td>
      <td>148</td>
      <td>151</td>
      <td>155</td>
      <td>158</td>
      <td>161</td>
      <td>166</td>
      <td>8</td>
      <td>15</td>
      <td>61</td>
      <td>64</td>
      <td>141</td>
      <td>143</td>
      <td>147</td>
      <td>152</td>
      <td>154</td>
      <td>157</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>2.027838</td>
      <td>-0.012972</td>
      <td>-0.280139</td>
      <td>-0.010381</td>
      <td>-0.555903</td>
      <td>1.599069</td>
      <td>0.762104</td>
      <td>0.070835</td>
      <td>-0.030718</td>
      <td>0.205755</td>
      <td>0.560392</td>
      <td>-0.778178</td>
      <td>-0.015011</td>
      <td>0.215463</td>
      <td>-0.070359</td>
      <td>0.456576</td>
      <td>0.052734</td>
      <td>-0.001598</td>
      <td>-0.139616</td>
      <td>0.532477</td>
    </tr>
    <tr>
      <th>110934</th>
      <td>18030096</td>
      <td>away</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-1.531022</td>
      <td>5.721350</td>
      <td>8.418071</td>
      <td>6.470581</td>
      <td>1.865955</td>
      <td>-7.140900</td>
      <td>-1.218767</td>
      <td>1.557886</td>
      <td>-12.441148</td>
      <td>10.097321</td>
      <td>-1.250494</td>
      <td>-3.458150</td>
      <td>0.664461</td>
      <td>-0.759332</td>
      <td>-11.728517</td>
      <td>-12.716384</td>
      <td>5.605381</td>
      <td>6.632644</td>
      <td>-7.996775</td>
      <td>-1.669872</td>
      <td>1.758697</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>8.0</td>
      <td>0.0</td>
      <td>-3.0</td>
      <td>3.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>-3.0</td>
      <td>-3.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>6</td>
      <td>21</td>
      <td>27</td>
      <td>46</td>
      <td>50</td>
      <td>57</td>
      <td>64</td>
      <td>120</td>
      <td>122</td>
      <td>124</td>
      <td>6</td>
      <td>20</td>
      <td>26</td>
      <td>41</td>
      <td>47</td>
      <td>120</td>
      <td>122</td>
      <td>124</td>
      <td>127</td>
      <td>129</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.207194</td>
      <td>0.658421</td>
      <td>2.997131</td>
      <td>1.520373</td>
      <td>1.700141</td>
      <td>-0.031522</td>
      <td>-0.491907</td>
      <td>0.521623</td>
      <td>-1.582725</td>
      <td>-0.043394</td>
      <td>-0.005923</td>
      <td>-0.450163</td>
      <td>0.132019</td>
      <td>-0.109901</td>
      <td>-4.375715</td>
      <td>-4.740775</td>
      <td>-0.692552</td>
      <td>0.765395</td>
      <td>-0.035241</td>
      <td>0.426164</td>
    </tr>
    <tr>
      <th>110935</th>
      <td>17715497</td>
      <td>draw</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>2.019156</td>
      <td>-1.600320</td>
      <td>4.409308</td>
      <td>-1.214033</td>
      <td>3.572422</td>
      <td>-0.831350</td>
      <td>3.157175</td>
      <td>2.964567</td>
      <td>-1.796125</td>
      <td>0.782550</td>
      <td>3.839781</td>
      <td>5.507413</td>
      <td>5.618307</td>
      <td>-1.072485</td>
      <td>5.490396</td>
      <td>-0.282014</td>
      <td>4.877673</td>
      <td>2.145550</td>
      <td>-3.490375</td>
      <td>-0.990867</td>
      <td>-0.967183</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>4.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2</td>
      <td>5</td>
      <td>8</td>
      <td>9</td>
      <td>47</td>
      <td>51</td>
      <td>55</td>
      <td>61</td>
      <td>135</td>
      <td>139</td>
      <td>2</td>
      <td>6</td>
      <td>8</td>
      <td>9</td>
      <td>47</td>
      <td>51</td>
      <td>55</td>
      <td>62</td>
      <td>142</td>
      <td>145</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.008287</td>
      <td>-0.215931</td>
      <td>1.027484</td>
      <td>-0.005764</td>
      <td>0.406166</td>
      <td>-0.004101</td>
      <td>-0.013232</td>
      <td>-0.012395</td>
      <td>-0.705266</td>
      <td>-0.002913</td>
      <td>-0.016199</td>
      <td>0.633308</td>
      <td>1.316578</td>
      <td>-0.005149</td>
      <td>0.631310</td>
      <td>0.142308</td>
      <td>0.559385</td>
      <td>0.486176</td>
      <td>0.422912</td>
      <td>-0.139092</td>
    </tr>
    <tr>
      <th>110936</th>
      <td>17944153</td>
      <td>away</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.685053</td>
      <td>0.483100</td>
      <td>-0.914850</td>
      <td>-1.181650</td>
      <td>1.605083</td>
      <td>1.526717</td>
      <td>1.056593</td>
      <td>-0.252250</td>
      <td>-4.249755</td>
      <td>2.833663</td>
      <td>-0.721922</td>
      <td>-1.544379</td>
      <td>-0.983140</td>
      <td>0.661400</td>
      <td>-1.600300</td>
      <td>1.017229</td>
      <td>-1.910996</td>
      <td>-2.063026</td>
      <td>0.873972</td>
      <td>-2.219119</td>
      <td>0.981163</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-3.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7</td>
      <td>12</td>
      <td>18</td>
      <td>28</td>
      <td>34</td>
      <td>75</td>
      <td>78</td>
      <td>83</td>
      <td>101</td>
      <td>106</td>
      <td>6</td>
      <td>13</td>
      <td>19</td>
      <td>26</td>
      <td>31</td>
      <td>75</td>
      <td>79</td>
      <td>84</td>
      <td>96</td>
      <td>102</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.002489</td>
      <td>-0.091875</td>
      <td>-0.004464</td>
      <td>0.151899</td>
      <td>-0.006487</td>
      <td>0.166027</td>
      <td>-0.348937</td>
      <td>-0.090353</td>
      <td>-0.018957</td>
      <td>0.319445</td>
      <td>-0.003626</td>
      <td>-0.007200</td>
      <td>-0.004761</td>
      <td>0.131287</td>
      <td>-0.215929</td>
      <td>-0.224238</td>
      <td>-0.008793</td>
      <td>-0.274268</td>
      <td>0.182118</td>
      <td>-0.010132</td>
    </tr>
    <tr>
      <th>110937</th>
      <td>17786297</td>
      <td>home</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.176178</td>
      <td>3.723710</td>
      <td>-3.523262</td>
      <td>2.112239</td>
      <td>1.595383</td>
      <td>1.261392</td>
      <td>1.785570</td>
      <td>-1.355685</td>
      <td>2.002817</td>
      <td>-1.703872</td>
      <td>0.247417</td>
      <td>-2.152850</td>
      <td>-4.360537</td>
      <td>-1.550680</td>
      <td>0.018106</td>
      <td>-5.484850</td>
      <td>-1.279433</td>
      <td>0.290300</td>
      <td>-1.478107</td>
      <td>-0.511200</td>
      <td>1.250977</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>-5.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>5.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>5</td>
      <td>12</td>
      <td>19</td>
      <td>26</td>
      <td>34</td>
      <td>40</td>
      <td>44</td>
      <td>47</td>
      <td>55</td>
      <td>62</td>
      <td>2</td>
      <td>5</td>
      <td>8</td>
      <td>12</td>
      <td>19</td>
      <td>27</td>
      <td>34</td>
      <td>39</td>
      <td>46</td>
      <td>54</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.008047</td>
      <td>-0.015695</td>
      <td>-0.458373</td>
      <td>-0.008691</td>
      <td>0.354621</td>
      <td>-0.004994</td>
      <td>-0.210956</td>
      <td>0.351036</td>
      <td>0.221915</td>
      <td>0.655388</td>
      <td>-0.000587</td>
      <td>0.265905</td>
      <td>-0.563935</td>
      <td>0.195218</td>
      <td>0.011063</td>
      <td>-0.024325</td>
      <td>-0.175474</td>
      <td>-0.000773</td>
      <td>-0.200523</td>
      <td>-0.382244</td>
    </tr>
  </tbody>
</table>
<p>102912 rows × 216 columns</p>
</div>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="BALANCE-DE-CLASES">
<a class="anchor" href="#BALANCE-DE-CLASES" aria-hidden="true"><span class="octicon octicon-link"></span></a>BALANCE DE CLASES<a class="anchor-link" href="#BALANCE-DE-CLASES"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Un aspecto importante a la hora de trabjar con clasificadores es tener clases de <em>targets</em> (nuestros tipos de resultados) balanceadas. Como ya sabemos esto es algo que no ocurre en nuestro <em>dataset</em>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fffmp</span><span class="o">.</span><span class="n">Classes_counts</span><span class="p">(</span><span class="n">FMP</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CLASES

away :  32530   31.61 %
draw :  25859   25.13 %
home :  44523   43.26 %
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como podemos ver, el <em>dataset</em> presenta un desbalance entre las distintas categorías a predecir (esto es: <em>home, draw</em> y <em>away</em>). Esto puede ser un reflejo de la realidad, o simplemente un sesgo del dataset (yo creo, y lo usaré de esa manera, que en este caso la razón es la primera). De cualquiera de las dos maneras, dejarlo así podría producir (y de hecho pude comprobar que lo hace) un sesgo en el modelo. Al una categoria ser más "frequente", el modelo puede adquirir un bías en el proceso de apendizaje. Esto lo vamos a corregir con dos estrategias distintas.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1)-Balance-de-derrotas-y-victorias">
<a class="anchor" href="#1)-Balance-de-derrotas-y-victorias" aria-hidden="true"><span class="octicon octicon-link"></span></a>1) Balance de derrotas y victorias<a class="anchor-link" href="#1)-Balance-de-derrotas-y-victorias"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>En primer lugar, es interesante notar que, de cierta forma, en realidad hay dos tipos fundamentales de resultados en un partido de futbol: un equipo gana, o hay un empate. Dicho esto, a priori uno no esperararía un desbalance entre dos categorías que son "Victoria Equipo A" y "Victoria Equipo B"; sin más información, los equipos A y B son indistinguibles. Pero, ¿por qué si hay diferencia entre "away" y "home"? ¿Es un <em>artifact</em> o estamos escondiendo un parámetro en la denominación de los equipos? Tiene sentido en este caso ir por la segunda suposición: <strong>estamos codificando la localía en el orden del dataset</strong>. Una forma de borrar ese sesgo es generar una variable que sea "localía". Luego podemos reconvertir nuestras variables de forma que refieran a "Equipo A" y "Equipo B", y enviar (aleatoriamente) mitad de los locales y visitantes a cada una. Esto automáticamente nos da un balance de las victorias y las derrotas. Veamos como queda:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">FMP2</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">data_balancing_Victorias_y_Derrotas</span><span class="p">(</span><span class="n">FMP</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>target</th>
      <th>is_cup</th>
      <th>Equipo_A_play_home_1</th>
      <th>Equipo_A_play_home_2</th>
      <th>Equipo_A_play_home_3</th>
      <th>Equipo_A_play_home_4</th>
      <th>Equipo_A_play_home_5</th>
      <th>Equipo_A_play_home_6</th>
      <th>Equipo_A_play_home_7</th>
      <th>Equipo_A_play_home_8</th>
      <th>Equipo_A_play_home_9</th>
      <th>Equipo_A_play_home_10</th>
      <th>Equipo_A_is_cup_1</th>
      <th>Equipo_A_is_cup_2</th>
      <th>Equipo_A_is_cup_3</th>
      <th>Equipo_A_is_cup_4</th>
      <th>Equipo_A_is_cup_5</th>
      <th>Equipo_A_is_cup_6</th>
      <th>Equipo_A_is_cup_7</th>
      <th>Equipo_A_is_cup_8</th>
      <th>Equipo_A_is_cup_9</th>
      <th>Equipo_A_is_cup_10</th>
      <th>Equipo_B_play_home_1</th>
      <th>Equipo_B_play_home_2</th>
      <th>Equipo_B_play_home_3</th>
      <th>Equipo_B_play_home_4</th>
      <th>Equipo_B_play_home_5</th>
      <th>Equipo_B_play_home_6</th>
      <th>Equipo_B_play_home_7</th>
      <th>Equipo_B_play_home_8</th>
      <th>Equipo_B_play_home_9</th>
      <th>Equipo_B_play_home_10</th>
      <th>Equipo_B_is_cup_1</th>
      <th>Equipo_B_is_cup_2</th>
      <th>Equipo_B_is_cup_3</th>
      <th>Equipo_B_is_cup_4</th>
      <th>Equipo_B_is_cup_5</th>
      <th>Equipo_B_is_cup_6</th>
      <th>Equipo_B_is_cup_7</th>
      <th>Equipo_B_is_cup_8</th>
      <th>Equipo_B_is_cup_9</th>
      <th>Equipo_B_is_cup_10</th>
      <th>Equipo_A_rating_diff_1</th>
      <th>Equipo_A_rating_diff_2</th>
      <th>Equipo_A_rating_diff_3</th>
      <th>Equipo_A_rating_diff_4</th>
      <th>Equipo_A_rating_diff_5</th>
      <th>Equipo_A_rating_diff_6</th>
      <th>Equipo_A_rating_diff_7</th>
      <th>Equipo_A_rating_diff_8</th>
      <th>Equipo_A_rating_diff_9</th>
      <th>Equipo_A_rating_diff_10</th>
      <th>Equipo_B_rating_diff_1</th>
      <th>Equipo_B_rating_diff_2</th>
      <th>Equipo_B_rating_diff_3</th>
      <th>Equipo_B_rating_diff_4</th>
      <th>Equipo_B_rating_diff_5</th>
      <th>Equipo_B_rating_diff_6</th>
      <th>Equipo_B_rating_diff_7</th>
      <th>Equipo_B_rating_diff_8</th>
      <th>Equipo_B_rating_diff_9</th>
      <th>Equipo_B_rating_diff_10</th>
      <th>Rating_diff</th>
      <th>Equipo_A_goal_diff_1</th>
      <th>Equipo_A_goal_diff_2</th>
      <th>Equipo_A_goal_diff_3</th>
      <th>Equipo_A_goal_diff_4</th>
      <th>Equipo_A_goal_diff_5</th>
      <th>Equipo_A_goal_diff_6</th>
      <th>Equipo_A_goal_diff_7</th>
      <th>Equipo_A_goal_diff_8</th>
      <th>Equipo_A_goal_diff_9</th>
      <th>Equipo_A_goal_diff_10</th>
      <th>Equipo_B_goal_diff_1</th>
      <th>Equipo_B_goal_diff_2</th>
      <th>Equipo_B_goal_diff_3</th>
      <th>Equipo_B_goal_diff_4</th>
      <th>Equipo_B_goal_diff_5</th>
      <th>Equipo_B_goal_diff_6</th>
      <th>Equipo_B_goal_diff_7</th>
      <th>Equipo_B_goal_diff_8</th>
      <th>Equipo_B_goal_diff_9</th>
      <th>Equipo_B_goal_diff_10</th>
      <th>Equipo_A_coach_continuity_1</th>
      <th>Equipo_A_coach_continuity_2</th>
      <th>Equipo_A_coach_continuity_3</th>
      <th>Equipo_A_coach_continuity_4</th>
      <th>Equipo_A_coach_continuity_5</th>
      <th>Equipo_A_coach_continuity_6</th>
      <th>Equipo_A_coach_continuity_7</th>
      <th>Equipo_A_coach_continuity_8</th>
      <th>Equipo_A_coach_continuity_9</th>
      <th>Equipo_A_coach_continuity</th>
      <th>Equipo_A_coach_continuity_10</th>
      <th>Equipo_B_coach_continuity_1</th>
      <th>Equipo_B_coach_continuity_2</th>
      <th>Equipo_B_coach_continuity_3</th>
      <th>Equipo_B_coach_continuity_4</th>
      <th>Equipo_B_coach_continuity_5</th>
      <th>Equipo_B_coach_continuity_6</th>
      <th>Equipo_B_coach_continuity_7</th>
      <th>Equipo_B_coach_continuity_8</th>
      <th>Equipo_B_coach_continuity_9</th>
      <th>Equipo_B_coach_continuity</th>
      <th>Equipo_B_coach_continuity_10</th>
      <th>Equipo_A_partidos_tres_semanas</th>
      <th>Equipo_A_partidos_diez_dias</th>
      <th>Equipo_A_partidos_cuatro_dias</th>
      <th>Equipo_B_partidos_tres_semanas</th>
      <th>Equipo_B_partidos_diez_dias</th>
      <th>Equipo_B_partidos_cuatro_dias</th>
      <th>diff_num_partidos_tres_semanas</th>
      <th>diff_num_partidos_diez_dias</th>
      <th>diff_num_partidos_cuatro_dias</th>
      <th>Equipo_A_relevance_1</th>
      <th>Equipo_A_relevance_2</th>
      <th>Equipo_A_relevance_3</th>
      <th>Equipo_A_relevance_4</th>
      <th>Equipo_A_relevance_5</th>
      <th>Equipo_A_relevance_6</th>
      <th>Equipo_A_relevance_7</th>
      <th>Equipo_A_relevance_8</th>
      <th>Equipo_A_relevance_9</th>
      <th>Equipo_A_relevance_10</th>
      <th>Equipo_B_relevance_1</th>
      <th>Equipo_B_relevance_2</th>
      <th>Equipo_B_relevance_3</th>
      <th>Equipo_B_relevance_4</th>
      <th>Equipo_B_relevance_5</th>
      <th>Equipo_B_relevance_6</th>
      <th>Equipo_B_relevance_7</th>
      <th>Equipo_B_relevance_8</th>
      <th>Equipo_B_relevance_9</th>
      <th>Equipo_B_relevance_10</th>
      <th>Equipo_A_outcome_V_1</th>
      <th>Equipo_A_outcome_D_1</th>
      <th>Equipo_A_outcome_V_2</th>
      <th>Equipo_A_outcome_D_2</th>
      <th>Equipo_A_outcome_V_3</th>
      <th>Equipo_A_outcome_D_3</th>
      <th>Equipo_A_outcome_V_4</th>
      <th>Equipo_A_outcome_D_4</th>
      <th>Equipo_A_outcome_V_5</th>
      <th>Equipo_A_outcome_D_5</th>
      <th>Equipo_A_outcome_V_6</th>
      <th>Equipo_A_outcome_D_6</th>
      <th>Equipo_A_outcome_V_7</th>
      <th>Equipo_A_outcome_D_7</th>
      <th>Equipo_A_outcome_V_8</th>
      <th>Equipo_A_outcome_D_8</th>
      <th>Equipo_A_outcome_V_9</th>
      <th>Equipo_A_outcome_D_9</th>
      <th>Equipo_A_outcome_V_10</th>
      <th>Equipo_A_outcome_D_10</th>
      <th>Equipo_B_outcome_V_1</th>
      <th>Equipo_B_outcome_D_1</th>
      <th>Equipo_B_outcome_V_2</th>
      <th>Equipo_B_outcome_D_2</th>
      <th>Equipo_B_outcome_V_3</th>
      <th>Equipo_B_outcome_D_3</th>
      <th>Equipo_B_outcome_V_4</th>
      <th>Equipo_B_outcome_D_4</th>
      <th>Equipo_B_outcome_V_5</th>
      <th>Equipo_B_outcome_D_5</th>
      <th>Equipo_B_outcome_V_6</th>
      <th>Equipo_B_outcome_D_6</th>
      <th>Equipo_B_outcome_V_7</th>
      <th>Equipo_B_outcome_D_7</th>
      <th>Equipo_B_outcome_V_8</th>
      <th>Equipo_B_outcome_D_8</th>
      <th>Equipo_B_outcome_V_9</th>
      <th>Equipo_B_outcome_D_9</th>
      <th>Equipo_B_outcome_V_10</th>
      <th>Equipo_B_outcome_D_10</th>
      <th>Equipo_A_is_friendly_1</th>
      <th>Equipo_A_is_friendly_2</th>
      <th>Equipo_A_is_friendly_3</th>
      <th>Equipo_A_is_friendly_4</th>
      <th>Equipo_A_is_friendly_5</th>
      <th>Equipo_A_is_friendly_6</th>
      <th>Equipo_A_is_friendly_7</th>
      <th>Equipo_A_is_friendly_8</th>
      <th>Equipo_A_is_friendly_9</th>
      <th>Equipo_A_is_friendly_10</th>
      <th>Equipo_B_is_friendly_1</th>
      <th>Equipo_B_is_friendly_2</th>
      <th>Equipo_B_is_friendly_3</th>
      <th>Equipo_B_is_friendly_4</th>
      <th>Equipo_B_is_friendly_5</th>
      <th>Equipo_B_is_friendly_6</th>
      <th>Equipo_B_is_friendly_7</th>
      <th>Equipo_B_is_friendly_8</th>
      <th>Equipo_B_is_friendly_9</th>
      <th>Equipo_B_is_friendly_10</th>
      <th>is_friendly</th>
      <th>Equipo_A_result_ponderado_1</th>
      <th>Equipo_A_result_ponderado_2</th>
      <th>Equipo_A_result_ponderado_3</th>
      <th>Equipo_A_result_ponderado_4</th>
      <th>Equipo_A_result_ponderado_5</th>
      <th>Equipo_A_result_ponderado_6</th>
      <th>Equipo_A_result_ponderado_7</th>
      <th>Equipo_A_result_ponderado_8</th>
      <th>Equipo_A_result_ponderado_9</th>
      <th>Equipo_A_result_ponderado_10</th>
      <th>Equipo_B_result_ponderado_1</th>
      <th>Equipo_B_result_ponderado_2</th>
      <th>Equipo_B_result_ponderado_3</th>
      <th>Equipo_B_result_ponderado_4</th>
      <th>Equipo_B_result_ponderado_5</th>
      <th>Equipo_B_result_ponderado_6</th>
      <th>Equipo_B_result_ponderado_7</th>
      <th>Equipo_B_result_ponderado_8</th>
      <th>Equipo_B_result_ponderado_9</th>
      <th>Equipo_B_result_ponderado_10</th>
      <th>EqA_Local</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>16839679</td>
      <td>EqA_Empate</td>
      <td>0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.227273</td>
      <td>1.492695</td>
      <td>-0.159562</td>
      <td>-3.586300</td>
      <td>0.533050</td>
      <td>0.930450</td>
      <td>3.465388</td>
      <td>2.033775</td>
      <td>2.844950</td>
      <td>-9.100417</td>
      <td>-1.291129</td>
      <td>-2.322800</td>
      <td>-1.103103</td>
      <td>-1.050675</td>
      <td>1.592775</td>
      <td>-9.355625</td>
      <td>2.446275</td>
      <td>-6.429625</td>
      <td>4.093375</td>
      <td>0.671250</td>
      <td>1.126130</td>
      <td>-2.0</td>
      <td>3.0</td>
      <td>-1.0</td>
      <td>-3.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>3.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>7</td>
      <td>11</td>
      <td>23</td>
      <td>27</td>
      <td>32</td>
      <td>35</td>
      <td>40</td>
      <td>49</td>
      <td>56</td>
      <td>68</td>
      <td>23</td>
      <td>27</td>
      <td>31</td>
      <td>34</td>
      <td>41</td>
      <td>47</td>
      <td>55</td>
      <td>70</td>
      <td>101</td>
      <td>104</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.028478</td>
      <td>0.498098</td>
      <td>-0.034283</td>
      <td>-1.366813</td>
      <td>-0.104253</td>
      <td>-0.003555</td>
      <td>-0.830918</td>
      <td>-0.242250</td>
      <td>0.986062</td>
      <td>-1.161531</td>
      <td>-0.176949</td>
      <td>-0.307020</td>
      <td>0.142678</td>
      <td>-0.146633</td>
      <td>-0.366864</td>
      <td>-0.041147</td>
      <td>-0.010143</td>
      <td>0.767941</td>
      <td>0.467318</td>
      <td>-0.138501</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>12021423</td>
      <td>EqA_Empate</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.250250</td>
      <td>0.500009</td>
      <td>2.607200</td>
      <td>4.354869</td>
      <td>-4.083405</td>
      <td>1.681650</td>
      <td>1.729845</td>
      <td>-0.619958</td>
      <td>0.764650</td>
      <td>0.277088</td>
      <td>1.653675</td>
      <td>-2.587142</td>
      <td>0.943600</td>
      <td>-1.254788</td>
      <td>-0.867925</td>
      <td>-0.024456</td>
      <td>-0.136037</td>
      <td>3.561950</td>
      <td>2.789087</td>
      <td>3.810900</td>
      <td>-0.187742</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>3.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>10</td>
      <td>15</td>
      <td>20</td>
      <td>28</td>
      <td>35</td>
      <td>39</td>
      <td>53</td>
      <td>56</td>
      <td>64</td>
      <td>71</td>
      <td>10</td>
      <td>23</td>
      <td>28</td>
      <td>35</td>
      <td>42</td>
      <td>49</td>
      <td>54</td>
      <td>59</td>
      <td>71</td>
      <td>251</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.000599</td>
      <td>0.139886</td>
      <td>0.900269</td>
      <td>0.498014</td>
      <td>0.492526</td>
      <td>-0.006820</td>
      <td>-0.203931</td>
      <td>0.085964</td>
      <td>-0.082240</td>
      <td>-0.000716</td>
      <td>-0.006698</td>
      <td>-0.011732</td>
      <td>-0.307182</td>
      <td>-0.005941</td>
      <td>-0.004260</td>
      <td>-0.000595</td>
      <td>-0.061554</td>
      <td>-0.014992</td>
      <td>-0.663324</td>
      <td>1.334626</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>11940784</td>
      <td>EqA_Victoria</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-2.147600</td>
      <td>-3.848700</td>
      <td>2.143658</td>
      <td>-0.316183</td>
      <td>1.120410</td>
      <td>-1.339575</td>
      <td>-2.243313</td>
      <td>2.679490</td>
      <td>-3.791425</td>
      <td>1.982828</td>
      <td>-3.120700</td>
      <td>-6.364683</td>
      <td>-2.610340</td>
      <td>0.594840</td>
      <td>-6.372400</td>
      <td>-4.592233</td>
      <td>3.685750</td>
      <td>-0.777687</td>
      <td>-0.158750</td>
      <td>-2.679490</td>
      <td>0.431840</td>
      <td>-1.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>-2.0</td>
      <td>-3.0</td>
      <td>0.0</td>
      <td>-4.0</td>
      <td>-3.0</td>
      <td>-3.0</td>
      <td>-3.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>13</td>
      <td>24</td>
      <td>41</td>
      <td>55</td>
      <td>90</td>
      <td>97</td>
      <td>112</td>
      <td>132</td>
      <td>152</td>
      <td>185</td>
      <td>6</td>
      <td>20</td>
      <td>24</td>
      <td>55</td>
      <td>62</td>
      <td>90</td>
      <td>97</td>
      <td>104</td>
      <td>125</td>
      <td>132</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.284931</td>
      <td>-0.981592</td>
      <td>-0.008828</td>
      <td>0.102471</td>
      <td>-0.127094</td>
      <td>-0.359804</td>
      <td>-0.870521</td>
      <td>-0.011157</td>
      <td>-1.917832</td>
      <td>-0.691221</td>
      <td>-1.194753</td>
      <td>-2.393546</td>
      <td>-0.011833</td>
      <td>-0.002097</td>
      <td>-1.606992</td>
      <td>0.552255</td>
      <td>-0.450528</td>
      <td>-0.220562</td>
      <td>-0.034180</td>
      <td>-0.012133</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>11927019</td>
      <td>EqA_Derrota</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.675180</td>
      <td>4.412433</td>
      <td>-5.385517</td>
      <td>0.737725</td>
      <td>-0.454567</td>
      <td>2.722759</td>
      <td>0.162283</td>
      <td>2.728475</td>
      <td>0.411706</td>
      <td>1.334997</td>
      <td>3.061683</td>
      <td>-1.936575</td>
      <td>3.159650</td>
      <td>-0.882525</td>
      <td>3.996617</td>
      <td>1.404617</td>
      <td>-3.303375</td>
      <td>3.133475</td>
      <td>-3.420250</td>
      <td>1.386297</td>
      <td>-0.429687</td>
      <td>-3.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-3.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>1.0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>7</td>
      <td>14</td>
      <td>35</td>
      <td>43</td>
      <td>49</td>
      <td>57</td>
      <td>63</td>
      <td>70</td>
      <td>76</td>
      <td>83</td>
      <td>7</td>
      <td>14</td>
      <td>21</td>
      <td>28</td>
      <td>35</td>
      <td>42</td>
      <td>56</td>
      <td>70</td>
      <td>91</td>
      <td>98</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.291028</td>
      <td>-0.018688</td>
      <td>0.645376</td>
      <td>-0.154974</td>
      <td>-0.002464</td>
      <td>-0.011345</td>
      <td>-0.000217</td>
      <td>-0.329836</td>
      <td>-0.001301</td>
      <td>0.143522</td>
      <td>-0.012817</td>
      <td>-0.258326</td>
      <td>-0.013243</td>
      <td>-0.004324</td>
      <td>0.455960</td>
      <td>-0.477547</td>
      <td>1.232571</td>
      <td>0.354639</td>
      <td>-0.875418</td>
      <td>0.149544</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>12070651</td>
      <td>EqA_Derrota</td>
      <td>0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.385609</td>
      <td>-1.608497</td>
      <td>-3.122686</td>
      <td>2.287922</td>
      <td>-1.623197</td>
      <td>3.110983</td>
      <td>0.616144</td>
      <td>4.011375</td>
      <td>10.922178</td>
      <td>-3.091981</td>
      <td>2.094376</td>
      <td>3.841724</td>
      <td>0.933463</td>
      <td>3.697159</td>
      <td>5.551204</td>
      <td>2.143917</td>
      <td>3.715547</td>
      <td>1.391611</td>
      <td>6.902513</td>
      <td>2.545350</td>
      <td>-1.764230</td>
      <td>-1.0</td>
      <td>-6.0</td>
      <td>-3.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>-1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-3.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3</td>
      <td>6</td>
      <td>14</td>
      <td>20</td>
      <td>28</td>
      <td>34</td>
      <td>41</td>
      <td>122</td>
      <td>144</td>
      <td>160</td>
      <td>4</td>
      <td>7</td>
      <td>14</td>
      <td>21</td>
      <td>27</td>
      <td>34</td>
      <td>42</td>
      <td>129</td>
      <td>154</td>
      <td>202</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>-0.062782</td>
      <td>-1.264379</td>
      <td>-1.195487</td>
      <td>-0.274292</td>
      <td>0.203730</td>
      <td>0.351999</td>
      <td>-0.124845</td>
      <td>0.457693</td>
      <td>3.900742</td>
      <td>-0.403997</td>
      <td>0.715216</td>
      <td>0.437778</td>
      <td>0.396254</td>
      <td>-0.015579</td>
      <td>-0.023637</td>
      <td>0.238478</td>
      <td>1.738843</td>
      <td>-0.005560</td>
      <td>0.797074</td>
      <td>-0.899098</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>102907</th>
      <td>17680446</td>
      <td>EqA_Derrota</td>
      <td>0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-2.449655</td>
      <td>-6.801235</td>
      <td>-1.374567</td>
      <td>-3.357715</td>
      <td>-5.657720</td>
      <td>-7.385085</td>
      <td>0.455519</td>
      <td>-1.723700</td>
      <td>0.662587</td>
      <td>-1.607044</td>
      <td>1.291365</td>
      <td>-0.066643</td>
      <td>6.459935</td>
      <td>4.542846</td>
      <td>7.037018</td>
      <td>4.310293</td>
      <td>5.892379</td>
      <td>3.601024</td>
      <td>5.542900</td>
      <td>4.959433</td>
      <td>-4.157078</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>-3.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>7</td>
      <td>13</td>
      <td>23</td>
      <td>28</td>
      <td>32</td>
      <td>36</td>
      <td>42</td>
      <td>50</td>
      <td>57</td>
      <td>70</td>
      <td>6</td>
      <td>9</td>
      <td>14</td>
      <td>20</td>
      <td>31</td>
      <td>33</td>
      <td>41</td>
      <td>49</td>
      <td>53</td>
      <td>57</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>-0.011134</td>
      <td>-1.713262</td>
      <td>-0.549482</td>
      <td>-0.015081</td>
      <td>-0.727482</td>
      <td>-1.857947</td>
      <td>-0.043266</td>
      <td>-0.231487</td>
      <td>-0.069372</td>
      <td>-0.007472</td>
      <td>-0.292172</td>
      <td>-0.022568</td>
      <td>1.517828</td>
      <td>-0.019254</td>
      <td>0.812863</td>
      <td>1.003807</td>
      <td>1.382114</td>
      <td>-0.439846</td>
      <td>-0.684674</td>
      <td>-0.021065</td>
      <td>0</td>
    </tr>
    <tr>
      <th>102908</th>
      <td>13973450</td>
      <td>EqA_Derrota</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-6.037900</td>
      <td>-2.249000</td>
      <td>-2.825450</td>
      <td>2.955731</td>
      <td>11.284325</td>
      <td>5.646480</td>
      <td>4.107761</td>
      <td>-2.118160</td>
      <td>-2.789100</td>
      <td>0.258211</td>
      <td>-4.017425</td>
      <td>1.909288</td>
      <td>-4.650607</td>
      <td>2.140950</td>
      <td>-9.699517</td>
      <td>-6.361954</td>
      <td>6.072812</td>
      <td>5.668042</td>
      <td>0.351962</td>
      <td>-1.905050</td>
      <td>0.000000</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>-4.0</td>
      <td>-3.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-4.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>-2.0</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>4.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>5.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>3</td>
      <td>10</td>
      <td>14</td>
      <td>17</td>
      <td>117</td>
      <td>128</td>
      <td>131</td>
      <td>139</td>
      <td>145</td>
      <td>159</td>
      <td>4</td>
      <td>6</td>
      <td>10</td>
      <td>14</td>
      <td>17</td>
      <td>117</td>
      <td>122</td>
      <td>131</td>
      <td>145</td>
      <td>242</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>-0.775414</td>
      <td>-0.010262</td>
      <td>-1.443272</td>
      <td>-1.050751</td>
      <td>4.031423</td>
      <td>-0.024051</td>
      <td>0.955378</td>
      <td>-1.095797</td>
      <td>-0.012609</td>
      <td>-0.018389</td>
      <td>0.484781</td>
      <td>0.210936</td>
      <td>-0.020699</td>
      <td>-0.255762</td>
      <td>-2.431488</td>
      <td>0.759997</td>
      <td>3.602004</td>
      <td>1.328471</td>
      <td>-0.030209</td>
      <td>-0.008767</td>
      <td>1</td>
    </tr>
    <tr>
      <th>102909</th>
      <td>17639407</td>
      <td>EqA_Empate</td>
      <td>0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-1.268515</td>
      <td>1.196983</td>
      <td>-0.654343</td>
      <td>3.142433</td>
      <td>-2.539479</td>
      <td>1.927360</td>
      <td>-4.602917</td>
      <td>-0.744920</td>
      <td>-0.141034</td>
      <td>1.475750</td>
      <td>-0.326533</td>
      <td>-2.833663</td>
      <td>0.725700</td>
      <td>-2.869942</td>
      <td>-0.224250</td>
      <td>-2.834929</td>
      <td>-3.448488</td>
      <td>0.594516</td>
      <td>-1.139558</td>
      <td>-0.862852</td>
      <td>0.304534</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>3.0</td>
      <td>-2.0</td>
      <td>-2.0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>3.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>6</td>
      <td>17</td>
      <td>21</td>
      <td>25</td>
      <td>30</td>
      <td>35</td>
      <td>38</td>
      <td>41</td>
      <td>47</td>
      <td>59</td>
      <td>18</td>
      <td>22</td>
      <td>25</td>
      <td>29</td>
      <td>32</td>
      <td>40</td>
      <td>48</td>
      <td>51</td>
      <td>55</td>
      <td>58</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.330192</td>
      <td>-0.004714</td>
      <td>0.183332</td>
      <td>-0.013168</td>
      <td>0.311290</td>
      <td>-0.007888</td>
      <td>0.553509</td>
      <td>-0.108084</td>
      <td>0.091435</td>
      <td>0.160044</td>
      <td>-0.108761</td>
      <td>-0.371429</td>
      <td>0.146663</td>
      <td>0.350082</td>
      <td>-0.001463</td>
      <td>-0.012809</td>
      <td>-0.448945</td>
      <td>0.173989</td>
      <td>-0.310237</td>
      <td>-0.241667</td>
      <td>1</td>
    </tr>
    <tr>
      <th>102910</th>
      <td>11987098</td>
      <td>EqA_Derrota</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.778017</td>
      <td>2.395232</td>
      <td>3.405850</td>
      <td>1.236388</td>
      <td>2.173467</td>
      <td>-1.702931</td>
      <td>3.026175</td>
      <td>1.737887</td>
      <td>-6.380690</td>
      <td>-4.681675</td>
      <td>-7.302900</td>
      <td>-7.145125</td>
      <td>-4.906037</td>
      <td>6.203850</td>
      <td>-1.444350</td>
      <td>-4.814425</td>
      <td>-3.026175</td>
      <td>5.304275</td>
      <td>-1.196555</td>
      <td>0.909658</td>
      <td>1.989225</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>2.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>3.0</td>
      <td>0.0</td>
      <td>-4.0</td>
      <td>0.0</td>
      <td>-3.0</td>
      <td>-1.0</td>
      <td>0.0</td>
      <td>-2.0</td>
      <td>-2.0</td>
      <td>0.0</td>
      <td>4.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>2.0</td>
      <td>2.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3</td>
      <td>10</td>
      <td>312</td>
      <td>318</td>
      <td>321</td>
      <td>328</td>
      <td>335</td>
      <td>347</td>
      <td>391</td>
      <td>397</td>
      <td>3</td>
      <td>10</td>
      <td>318</td>
      <td>321</td>
      <td>326</td>
      <td>330</td>
      <td>335</td>
      <td>346</td>
      <td>353</td>
      <td>360</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.195526</td>
      <td>0.267979</td>
      <td>-0.014313</td>
      <td>-0.004885</td>
      <td>0.492851</td>
      <td>-0.449847</td>
      <td>-0.012663</td>
      <td>0.586576</td>
      <td>-0.028218</td>
      <td>-2.355190</td>
      <td>-0.032226</td>
      <td>-2.681953</td>
      <td>-0.632711</td>
      <td>-0.026473</td>
      <td>-0.385768</td>
      <td>-1.220909</td>
      <td>-0.013640</td>
      <td>2.505538</td>
      <td>-0.005688</td>
      <td>-0.003465</td>
      <td>1</td>
    </tr>
    <tr>
      <th>102911</th>
      <td>17012205</td>
      <td>EqA_Derrota</td>
      <td>0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>-0.211667</td>
      <td>1.606796</td>
      <td>-2.285403</td>
      <td>1.622922</td>
      <td>1.168667</td>
      <td>0.119013</td>
      <td>-0.626957</td>
      <td>-5.730332</td>
      <td>1.923965</td>
      <td>-4.398405</td>
      <td>4.584513</td>
      <td>2.291772</td>
      <td>2.535528</td>
      <td>-1.727919</td>
      <td>2.352086</td>
      <td>-1.097665</td>
      <td>1.316322</td>
      <td>6.272992</td>
      <td>1.779469</td>
      <td>-1.756280</td>
      <td>-0.723624</td>
      <td>3.0</td>
      <td>2.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>-1.0</td>
      <td>-3.0</td>
      <td>0.0</td>
      <td>-1.0</td>
      <td>-1.0</td>
      <td>1.0</td>
      <td>3.0</td>
      <td>3.0</td>
      <td>5.0</td>
      <td>1.0</td>
      <td>-2.0</td>
      <td>1.0</td>
      <td>-4.0</td>
      <td>5.0</td>
      <td>-2.0</td>
      <td>-1.0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>1.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>15</td>
      <td>22</td>
      <td>29</td>
      <td>36</td>
      <td>39</td>
      <td>43</td>
      <td>49</td>
      <td>56</td>
      <td>64</td>
      <td>72</td>
      <td>17</td>
      <td>22</td>
      <td>29</td>
      <td>36</td>
      <td>41</td>
      <td>45</td>
      <td>51</td>
      <td>58</td>
      <td>63</td>
      <td>71</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0.116923</td>
      <td>0.357350</td>
      <td>-0.302305</td>
      <td>0.177320</td>
      <td>-0.133178</td>
      <td>-0.002461</td>
      <td>-0.003213</td>
      <td>-0.736636</td>
      <td>-0.228405</td>
      <td>0.529503</td>
      <td>1.613785</td>
      <td>0.786447</td>
      <td>1.464364</td>
      <td>0.216023</td>
      <td>-0.555030</td>
      <td>0.142040</td>
      <td>-0.591481</td>
      <td>3.722976</td>
      <td>-0.413129</td>
      <td>-0.235594</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>102912 rows × 217 columns</p>
</div>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Podemos ver como ahora tenemos nuevos targets y las <em>features</em> han cambiado su nombre. El resultado, como se ve abajo, muestra un balance entre "EqA_Derrota" (equivalente a "EqB_Victoria") y "EqA_Victoria".</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fffmp</span><span class="o">.</span><span class="n">Classes_counts</span><span class="p">(</span><span class="n">FMP2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CLASES

EqA_Derrota :  38460   37.37 %
EqA_Empate :  25859   25.13 %
EqA_Victoria :  38593   37.5 %
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Guardaremos este dataset para hacer un pequeño análisis en un bloque posterior.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_clean</span><span class="o">=</span><span class="s1">'FMP_Balance_V_vs_D'</span>
<span class="n">FMP2</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="n">data_clean</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2)-Balance-de-los-empates">
<a class="anchor" href="#2)-Balance-de-los-empates" aria-hidden="true"><span class="octicon octicon-link"></span></a>2) Balance de los empates<a class="anchor-link" href="#2)-Balance-de-los-empates"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>En segundo lugar tenemos el problema de los empates, y aquí ya no nos quedrá otra opción que hacer un <em>re-sampling</em>. Dado que las derrotas y las victorias son clases con $1.5$ veces la cantidad de ocurrrencias que los empates, lo que haremos será hacer un poco de <em>oversampling</em> de los empates, y un poco de <em>undersampling</em> de las otras dos clases. En este sentido, creo que antes de realizar estos procesos lo mejor es separar el <em>dataset</em> en <em>Train</em> y <em>Test</em>. Porque, sí, quiero que aprenda con un conjunto balanceado; sin embargo, la realidad probablemente no lo es, asique a la hora de testear quiero un conjunto que refleje ello. Por otro lado, utilizar <em>oversampling</em> va a generar casos duplicados que podrían acabar uno en el <em>trainset</em> y otro en el <em>testset</em>, incrementando de forma artificial el <em>test accuracy</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="Separación-de-los-datos-en-Train-y-Test">
<a class="anchor" href="#Separaci%C3%B3n-de-los-datos-en-Train-y-Test" aria-hidden="true"><span class="octicon octicon-link"></span></a>Separación de los datos en <em>Train</em> y <em>Test</em><a class="anchor-link" href="#Separaci%C3%B3n-de-los-datos-en-Train-y-Test"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Separaremos entonces el <em>dataset</em>, de forma de mantener la proporcion de clases para ambos grupos (<em>test</em> y <em>training</em>)</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">FMP_train</span><span class="p">,</span><span class="n">FMP_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Training_Test_separation</span><span class="p">(</span><span class="n">FMP2</span><span class="p">,</span><span class="kc">True</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>TRAIN: 92620 TEST: 10292
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train</th>
      <th>train %</th>
      <th>test</th>
      <th>test %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>EqA_Derrota</th>
      <td>34614</td>
      <td>0.373721</td>
      <td>3846</td>
      <td>0.373688</td>
    </tr>
    <tr>
      <th>EqA_Empate</th>
      <td>23273</td>
      <td>0.251274</td>
      <td>2586</td>
      <td>0.251263</td>
    </tr>
    <tr>
      <th>EqA_Victoria</th>
      <td>34733</td>
      <td>0.375005</td>
      <td>3860</td>
      <td>0.375049</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Y ahora si, procederemos a hacer el balance de los datos en el <em>dataset</em> de entrenamiento. Para ello modificaremos en la misma proporcion los datos de las tres clases:</p>
<ul>
<li>Incrementamos los empates en $\approx 20\%$.</li>
<li>Redcimos las victorias y las derrotas en $\approx 20\%$.</li>
</ul>
<p>Nota: Ambas estrategias tienen sus problemas. El <em>oversampling</em> puede generar un mayor <em>overfitting</em>, una reducción en la performance del modelo, y un aumento de los costos computacionales. Por su lado, el <em>underfitting</em> nos genera pérdida de información. 
Cuando hice este paso pensé que lo mejor era hacer ambos procesos en igual proporción, pero ahora creo que quizas debería haber apostado a un mayor <em>overfitting</em> dadas las resultados obtenidos para los modelos.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fffmp</span><span class="o">.</span><span class="n">Classes_counts</span><span class="p">(</span><span class="n">FMP_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CLASES

EqA_Derrota :  34614   37.37 %
EqA_Empate :  23273   25.13 %
EqA_Victoria :  34733   37.5 %
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">FMP_train_2</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Balance_de_empates_por_under_and_over_sampling</span><span class="p">(</span><span class="n">FMP_train</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Ratio Victorias(Derrotas)/Empates=  1.49
Proporcion a cambiar clases=  19.67 %
</pre>
</div>
</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fffmp</span><span class="o">.</span><span class="n">Classes_counts</span><span class="p">(</span><span class="n">FMP_train_2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>CLASES

EqA_Derrota :  27784   33.31 %
EqA_Empate :  27886   33.44 %
EqA_Victoria :  27729   33.25 %
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Efectivamente conseguimos lo que estabamos buscando, terminando con aproximadamente $33\%$ de cada clase!</p>
<p>Guardaremos este dataset para hacer un pequeño análisis en un bloque posterior.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">data_clean</span><span class="o">=</span><span class="s1">'FMP_training_bal_tot'</span>
<span class="n">FMP_train_2</span><span class="o">.</span><span class="n">to_pickle</span><span class="p">(</span><span class="n">data_clean</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>NOTA:</strong> Para ganar un mayor entendimiento de los procesos realizados en el <em>dataset</em> se agregó un bloque adicional al proyecto (el número 6) donde se comparan el <em>dataset</em> generado con el original.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como ultima cosa recordemos las <em>features</em> con las que contamos luego de las transformaciones realizadas:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fffmp</span><span class="o">.</span><span class="n">Print_features_data_set_modelos</span><span class="p">(</span><span class="n">FMP_train_2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Today Features

id
target
is_cup
Rating_diff
Equipo_A_coach_continuity
Equipo_B_coach_continuity
Equipo_A_partidos_tres_semanas
Equipo_A_partidos_diez_dias
Equipo_A_partidos_cuatro_dias
Equipo_B_partidos_tres_semanas
Equipo_B_partidos_diez_dias
Equipo_B_partidos_cuatro_dias
diff_num_partidos_tres_semanas
diff_num_partidos_diez_dias
diff_num_partidos_cuatro_dias
is_friendly
EqA_Local


Historic Features

Equipo_(A/B)_play_home_i
Equipo_(A/B)_is_cup_i
Equipo_(A/B)_rating_diff_i
Equipo_(A/B)_goal_diff_i
Equipo_(A/B)_coach_continuity_i
Equipo_(A/B)_relevance_i
Equipo_(A/B)_outcome_V_i
Equipo_(A/B)_outcome_D_i
Equipo_(A/B)_is_friendly_i
Equipo_(A/B)_result_ponderado_i
</pre>
</div>
</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="MODELADO">
<a class="anchor" href="#MODELADO" aria-hidden="true"><span class="octicon octicon-link"></span></a>MODELADO<a class="anchor-link" href="#MODELADO"> </a>
</h2>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finalmente llegamos a la última etapa de este proyecto: la generación de un modelo predictivo que pueda aprender a modelar los resultados de los partidos en función de las variables generadas, permitiendonos inferir información sobre futuro partidos. En particular, el objetivo que nos habíamos propuesto (que es el que figuraba en eñ desafío de Kaggle de "<a href="https://www.kaggle.com/c/football-match-probability-prediction">Football Match Probability Prediction</a>") era poder predecir la probabilidad de cada uno de los tres posibles resultados. Esto la haremos hacia el final de este <em>notebook</em>, sin embargo, empezaremos por un modelo más sencillo que simplemente buscará predecir el resultado sin darnos la probabilidad.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="1)-Random-Forest">
<a class="anchor" href="#1)-Random-Forest" aria-hidden="true"><span class="octicon octicon-link"></span></a>1) Random Forest<a class="anchor-link" href="#1)-Random-Forest"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Empezaremos considerando un <em>Random Forest</em>. Si bien hay clasificadores más simples (como una regresión logística o SVM), dado que tenemos un modelo multiclases preferí comenzar con este clasificador para el cual la implementación era fácil y estaba familiarizado de trabajos anteriores.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Para llevar un registro de la performance del modelo, generamos un diccionario (Report_RF) para ir guardando el resultado de los distinto modelos de <em>Random Forest</em>. En el mismo registraremos las métricas "<em>Accuracy</em>" y "<em>F1_macro</em>" tanto en el <em>train dataset</em> como en el <em>Test dataset</em>.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Report1_RF</span><span class="o">=</span><span class="p">{</span><span class="s1">'Nombre'</span><span class="p">:[],</span><span class="s1">'Accuracy'</span><span class="p">:[],</span><span class="s1">'F1_macro'</span><span class="p">:[]}</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="A)-RF-con-todas-las-variables">
<a class="anchor" href="#A)-RF-con-todas-las-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>A) RF con todas las variables<a class="anchor-link" href="#A)-RF-con-todas-las-variables"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>En una primera instancia trabajaré con todas las <em>features</em> exceptuando el "resultado ponderado" (el cual terminé por ver que no aportaba ningún valor) y "diff_num_partidos_cuatro_dias" (el cual no aportaba informacion adicional a las variables calculadas a 10 y 21 días). Es verdad que, como vimos en el bloque 4, muchas de las variables (sobetodo las correspondientes a un mismo aspectos para cada partido historial) estan quizás bastante correlacionadas. Esto es algo que trataré de forma más cuidadosa en modelos posteriores, pero por el momento veremos que tal funciona directamente así.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Comenzamos por separar ambos <em>datasets</em> en su <em>input</em> y <em>output</em>. Aprovechamos al mismo tiempo para normalizar todas la variables (si bien para las <em>One-Hot Encoded</em> no es fundamental, tampoco afecta). Verificaremos al acabar que no tenemos valores extraños, como puede pasar cuando la desviación estandar es $0$.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F_h</span><span class="o">=</span><span class="p">[</span><span class="s1">'play_home'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">,</span><span class="s1">'rating_diff'</span><span class="p">,</span><span class="s1">'goal_diff'</span><span class="p">,</span><span class="s1">'coach_continuity'</span><span class="p">,</span><span class="s1">'relevance'</span><span class="p">,</span><span class="s1">'is_friendly'</span><span class="p">]</span>
<span class="n">F_m</span><span class="o">=</span><span class="p">[</span><span class="s1">'Rating_diff'</span><span class="p">,</span><span class="s1">'EqA_Local'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">,</span><span class="s1">'Equipo_A_coach_continuity'</span><span class="p">,</span><span class="s1">'Equipo_B_coach_continuity'</span><span class="p">,</span><span class="s1">'diff_num_partidos_diez_dias'</span><span class="p">,</span><span class="s1">'diff_num_partidos_tres_semanas'</span><span class="p">,</span><span class="s1">'is_friendly'</span><span class="p">]</span>
<span class="n">F_h_to_std</span><span class="o">=</span><span class="n">F_h</span>
<span class="n">F_m_to_std</span><span class="o">=</span><span class="n">F_m</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X_t</span><span class="p">,</span><span class="n">y_t</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Prepara_DF_para_Random_Forest</span><span class="p">(</span><span class="n">FMP_train_2</span><span class="p">,</span><span class="n">FMP_test</span><span class="p">,</span><span class="n">F_h</span><span class="p">,</span><span class="n">F_m</span><span class="p">,</span><span class="n">F_h_to_std</span><span class="p">,</span><span class="n">F_m_to_std</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train
No hay missing values en el dataset :)
test
No hay missing values en el dataset :)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Selección de los hiperparámetros</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Para hacer la selección de los hiperparámetros, utilizamos una <em>K-fold cross validation</em>. Esto nos ayuda a evitar <em>overfitting</em>, lo que sucede bastante facilmente con los modelos de Random Forest en mi (breve) experiencia. Aquí abajo solo mantengo el último <em>grid</em> de parámetros utilizados.</p>
<p>Nota: El número de estimadores (es decir el número de árboles) lo dejo bajo para estas pruebas para reducir los tiempo de computo. Para el training final siempre lo subo ya que no puede hacer más que mejorar el resultado.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">'n_estimators'</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="s1">'max_features'</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="s1">'max_depth'</span><span class="p">:[</span><span class="mi">10</span><span class="p">],</span><span class="s1">'min_samples_leaf'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">'class_weight'</span><span class="p">:[</span><span class="kc">None</span><span class="p">],</span><span class="s1">'bootstrap'</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">]},</span>
  <span class="p">]</span>
<span class="n">fffmp</span><span class="o">.</span><span class="n">Cross_Validation_RandomForest</span><span class="p">(</span><span class="n">param_grid</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 7 folds for each of 1 candidates, totalling 7 fits
[CV 1/7] END bootstrap=True, class_weight=None, max_depth=10, max_features=None, min_samples_leaf=1, n_estimators=10; total time=  20.1s
[CV 2/7] END bootstrap=True, class_weight=None, max_depth=10, max_features=None, min_samples_leaf=1, n_estimators=10; total time=  13.9s
[CV 3/7] END bootstrap=True, class_weight=None, max_depth=10, max_features=None, min_samples_leaf=1, n_estimators=10; total time=  14.6s
[CV 4/7] END bootstrap=True, class_weight=None, max_depth=10, max_features=None, min_samples_leaf=1, n_estimators=10; total time=  14.3s
[CV 5/7] END bootstrap=True, class_weight=None, max_depth=10, max_features=None, min_samples_leaf=1, n_estimators=10; total time=  14.4s
[CV 6/7] END bootstrap=True, class_weight=None, max_depth=10, max_features=None, min_samples_leaf=1, n_estimators=10; total time=  14.0s
[CV 7/7] END bootstrap=True, class_weight=None, max_depth=10, max_features=None, min_samples_leaf=1, n_estimators=10; total time=  15.6s
Scorer
make_scorer(accuracy_score)
Best Score
0.4329787633264904
the best trained model:
RandomForestClassifier(max_depth=10, max_features=None, n_estimators=10,
                       n_jobs=4, random_state=42)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RandomForestClassifier(max_depth=10, max_features=None, n_estimators=10,
                       n_jobs=4, random_state=42)</pre>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Entrenamiento del modelo</strong></p>
<p>Una vez seleccionados los hiperparámetros procedemos a entrenar el modelo.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">forest_reg</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span> <span class="mi">50</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">forest_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RandomForestClassifier(max_depth=10, max_features=None, n_estimators=50,
                       n_jobs=4)</pre>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Resultado</strong></p>
<p>Miremos entonces que tal funciona este modelo:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_name</span><span class="o">=</span><span class="s1">'RF completo'</span>
<span class="n">Report1_RF</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Report_RF_results</span><span class="p">(</span><span class="n">forest_reg</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X_t</span><span class="p">,</span><span class="n">y_t</span><span class="p">,</span><span class="n">Report1_RF</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Nombre</th>
      <th>Accuracy</th>
      <th>F1_macro</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>RF completo Train</td>
      <td>0.564323</td>
      <td>0.563954</td>
    </tr>
    <tr>
      <th>1</th>
      <td>RF completo Test</td>
      <td>0.467256</td>
      <td>0.454680</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A la hora de analizar la performance del modelo es siempre útil tener en mente una referencia. Dado que hemos balanceado nuestros <em>targets</em>, sabemos que tenemos $\approx33\%$ de cada categoría, luego un clasificador aleatorio nos daría esa precision (0.33). Este deberá ser nuestro piso, un modelo que tenga una performance mejor ya podremos decir que esta aprendiendo algo. Es un objetivo bastante poco ambicioso, pero veremos que no es tán facil el problema.</p>
<p>Dicho esto, efectivamente podemos decir que nuestro modelo tiene cierto poder predictivo, sin embargo es claro (o al menos esa era mi expectativa) que deberíamos poder mejorar nuestro poder predictivo.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h4 id="B)-PCA-y-RF">
<a class="anchor" href="#B)-PCA-y-RF" aria-hidden="true"><span class="octicon octicon-link"></span></a>B) PCA y RF<a class="anchor-link" href="#B)-PCA-y-RF"> </a>
</h4>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Una forma de mejorar el resultado obtenido pasa por hacer una selección de variables o <em>features</em>. Para hacer esto, en lugar de ir probando distintas combinaciones, decidí intentar aplicar el metodo de <em>Principal Component Analysis</em> (PCA). Esta técnica permite transformar nuestro dataset a una base vectorial nueva base vectorial con variables no correlacionadas, lo que nos soluciona un primer problema. Pero lo mejor aun es que estas variables quedan ordenadas por la cantidad de varianza que decriben, lo que está codificado en los autovalores. De esta forma, mirando a los autovalores podemos seleccionar las variables que deberían tener una mayor importancia.</p>
<p>Decidí entonces considerar en este caso todas las variables del <em>dataset</em> ya que la multicolinearidad original no debería ser un problema al utilizar PCA. Luego, nuevamente preparamos y normalizamos los datos:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F_h</span><span class="o">=</span><span class="p">[</span><span class="s1">'play_home'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">,</span><span class="s1">'rating_diff'</span><span class="p">,</span><span class="s1">'goal_diff'</span><span class="p">,</span><span class="s1">'coach_continuity'</span><span class="p">,</span><span class="s1">'relevance'</span><span class="p">,</span><span class="s1">'is_friendly'</span><span class="p">,</span><span class="s1">'result_ponderado'</span><span class="p">]</span>
<span class="n">F_m</span><span class="o">=</span><span class="p">[</span><span class="s1">'Rating_diff'</span><span class="p">,</span><span class="s1">'EqA_Local'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">,</span><span class="s1">'Equipo_A_coach_continuity'</span><span class="p">,</span><span class="s1">'Equipo_B_coach_continuity'</span><span class="p">,</span><span class="s1">'diff_num_partidos_diez_dias'</span><span class="p">,</span><span class="s1">'diff_num_partidos_tres_semanas'</span><span class="p">,</span><span class="s1">'diff_num_partidos_cuatro_dias'</span><span class="p">,</span><span class="s1">'is_friendly'</span><span class="p">]</span>
<span class="n">F_h_to_std</span><span class="o">=</span><span class="n">F_h</span>
<span class="n">F_m_to_std</span><span class="o">=</span><span class="n">F_m</span>
<span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X_t</span><span class="p">,</span><span class="n">y_t</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Prepara_DF_para_Random_Forest</span><span class="p">(</span><span class="n">FMP_train_2</span><span class="p">,</span><span class="n">FMP_test</span><span class="p">,</span><span class="n">F_h</span><span class="p">,</span><span class="n">F_m</span><span class="p">,</span><span class="n">F_h_to_std</span><span class="p">,</span><span class="n">F_m_to_std</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>train
No hay missing values en el dataset :)
test
No hay missing values en el dataset :)
</pre>
</div>
</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>PCA</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Aplicamos entonces PCA para ver los autovalores de la transformación:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fffmp</span><span class="o">.</span><span class="n">PCA_analisis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X_t</span><span class="p">,</span><span class="n">Action</span><span class="o">=</span><span class="s1">'Fit'</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>[1056.1739394   807.96171312  753.66794049  628.42023238  591.27775932
  515.02889343  490.04746795  482.52275821  454.1742522   453.64978963
  452.82754626  432.83850214  418.16116484  416.31818982  404.57505498
  401.18584814  400.33061564  398.12251238  392.39832499  387.09088544
  386.56925738  385.27216034  383.01572586  380.89921633  378.94618226
  376.9887087   375.87541731  369.39209423  345.57649742  344.49797834
  338.87460261  337.75459298  332.53610435  325.18162231  320.15963613
  318.2957323   313.00213268  308.94519621  306.30238777  303.76183913
  302.91205679  302.33023283  299.47071691  298.79971346  295.4341473
  294.3680493   291.3710512   288.0788367   287.39477909  285.79443611
  284.84864226  284.08018509  282.79943749  282.32693659  281.48563651
  281.35338659  280.19507932  277.34008654  277.07918728  276.32109161
  274.99544928  274.3810467   273.07276934  270.9509744   270.49776048
  268.85894847  268.68314006  264.49816597  262.8302743   262.21643616
  261.64810068  260.16689545  258.456953    257.00963917  255.07523905
  252.60551983  251.47233064  249.70182872  244.16869221  243.51070442
  241.85984168  239.01318194  237.87253313  237.01931473  232.70860474
  232.5873407   231.10751289  227.59224697  225.94738229  224.44516122
  223.58149311  216.57302534  215.7415944   214.62380553  212.54079381
  210.17934297  209.93912476  206.59873175  203.72622804  200.09006121
  195.7802494   193.80191349  193.25056921  193.08557451  192.03673637
  191.27257869  191.17653325  190.21552967  190.10094763  189.68686704
  188.92691611  188.7173538   188.7073066   187.85899142  186.94963811
  186.14589353  185.42720143  185.14190397  184.37114792  183.73150039
  182.71114539  182.3535036   181.28338561  179.92818212  177.90276095
  176.93237394  173.66770908  171.8602939   171.78052959  171.02205687
  165.34381939  165.27411789  161.47060142  158.62626678  150.23578794
  137.07277973  135.27658302  134.31499465  132.78040857  132.19527532
  132.04627094  131.82628088  131.25635713  130.91698377  130.22925411
  129.90438683  129.48461439  129.31571133  129.22452473  128.79099185
  128.35587461  127.93377419  127.45330077  126.90731911  126.31113872
  121.28540515  104.27363594   99.80412756   85.81580248   84.38506303
   72.76822779   71.76189234   61.90764481   60.17994899   53.09944599
   49.06577033   44.51217705]
</pre>
</div>
</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Desgraciadamente no había una clara línea para trazar que permitiera separar los relevantes de los que no lo son. Por esa razón, optamos por probar con distintos umbrales arbitrarios: $0$, $100$, $200$, $300$, y $400$.</p>
<p>Transformamos entonces nuestro <em>dataset</em>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_2</span><span class="p">,</span><span class="n">X_t_2</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">PCA_analisis</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">X_t</span><span class="p">,</span><span class="n">Action</span><span class="o">=</span><span class="s1">'Fit_Transform'</span><span class="p">,</span><span class="n">umbral</span><span class="o">=</span><span class="mi">400</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Selección de los hiperparámetros</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Nuevamente, para hacer la selección de los hiperparámetros utilizamos una <em>K-fold cross validation</em>.</p>
<p>Nota: El número de estimadores (es decir el número de árboles) lo dejo bajo para estas pruebas para reducir los tiempo de computo. Para el training final siempre lo subo ya que no puede hacer más que mejorar el resultado.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">param_grid</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">{</span><span class="s1">'n_estimators'</span><span class="p">:</span> <span class="p">[</span><span class="mi">10</span><span class="p">],</span> <span class="s1">'max_features'</span><span class="p">:</span> <span class="p">[</span><span class="kc">None</span><span class="p">],</span> <span class="s1">'max_depth'</span><span class="p">:[</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">],</span><span class="s1">'min_samples_leaf'</span> <span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="s1">'class_weight'</span><span class="p">:[</span><span class="kc">None</span><span class="p">],</span><span class="s1">'bootstrap'</span><span class="p">:</span> <span class="p">[</span><span class="kc">True</span><span class="p">]},</span>
  <span class="p">]</span>
<span class="n">fffmp</span><span class="o">.</span><span class="n">Cross_Validation_RandomForest</span><span class="p">(</span><span class="n">param_grid</span><span class="p">,</span><span class="n">X_2</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Fitting 7 folds for each of 2 candidates, totalling 14 fits
[CV 1/7] END bootstrap=True, class_weight=None, max_depth=6, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   3.7s
[CV 2/7] END bootstrap=True, class_weight=None, max_depth=6, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   3.7s
[CV 3/7] END bootstrap=True, class_weight=None, max_depth=6, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   3.6s
[CV 4/7] END bootstrap=True, class_weight=None, max_depth=6, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   3.8s
[CV 5/7] END bootstrap=True, class_weight=None, max_depth=6, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   3.7s
[CV 6/7] END bootstrap=True, class_weight=None, max_depth=6, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   3.7s
[CV 7/7] END bootstrap=True, class_weight=None, max_depth=6, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   3.7s
[CV 1/7] END bootstrap=True, class_weight=None, max_depth=7, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   4.3s
[CV 2/7] END bootstrap=True, class_weight=None, max_depth=7, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   6.0s
[CV 3/7] END bootstrap=True, class_weight=None, max_depth=7, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   5.7s
[CV 4/7] END bootstrap=True, class_weight=None, max_depth=7, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   5.2s
[CV 5/7] END bootstrap=True, class_weight=None, max_depth=7, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   5.5s
[CV 6/7] END bootstrap=True, class_weight=None, max_depth=7, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   4.7s
[CV 7/7] END bootstrap=True, class_weight=None, max_depth=7, max_features=None, min_samples_leaf=1, n_estimators=10; total time=   4.6s
Scorer
make_scorer(accuracy_score)
Best Score
0.43546078088137136
the best trained model:
RandomForestClassifier(max_depth=6, max_features=None, n_estimators=10,
                       n_jobs=4, random_state=42)
</pre>
</div>
</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RandomForestClassifier(max_depth=6, max_features=None, n_estimators=10,
                       n_jobs=4, random_state=42)</pre>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Entrenamiento del modelo</strong></p>
<p>Una vez seleccionados los hiperparámetros procedemos a entrenar el modelo.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">forest_reg</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">bootstrap</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span> <span class="mi">6</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">n_estimators</span><span class="o">=</span> <span class="mi">50</span><span class="p">,</span><span class="n">n_jobs</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">forest_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_2</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>RandomForestClassifier(max_depth=6, max_features=None, n_estimators=50,
                       n_jobs=4)</pre>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Resultado</strong></p>
<p>Miremos entonces que tal funciona este segundo modelo:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model_name</span><span class="o">=</span><span class="s1">'RF PCA all th=400'</span>
<span class="n">Report1_RF</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Report_RF_results</span><span class="p">(</span><span class="n">forest_reg</span><span class="p">,</span><span class="n">model_name</span><span class="p">,</span><span class="n">X_2</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">X_t_2</span><span class="p">,</span><span class="n">y_t</span><span class="p">,</span><span class="n">Report1_RF</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Nombre</th>
      <th>Accuracy</th>
      <th>F1_macro</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>RF completo Train</td>
      <td>0.564323</td>
      <td>0.563954</td>
    </tr>
    <tr>
      <th>1</th>
      <td>RF completo Test</td>
      <td>0.467256</td>
      <td>0.454680</td>
    </tr>
    <tr>
      <th>2</th>
      <td>RF PCA all features Train</td>
      <td>0.454214</td>
      <td>0.454808</td>
    </tr>
    <tr>
      <th>3</th>
      <td>RF PCA all features Test</td>
      <td>0.445103</td>
      <td>0.439398</td>
    </tr>
    <tr>
      <th>4</th>
      <td>RF PCA all th=100 Train</td>
      <td>0.453243</td>
      <td>0.453759</td>
    </tr>
    <tr>
      <th>5</th>
      <td>RF PCA all th=100 Test</td>
      <td>0.447143</td>
      <td>0.440709</td>
    </tr>
    <tr>
      <th>6</th>
      <td>RF PCA all th=200 Train</td>
      <td>0.453830</td>
      <td>0.454605</td>
    </tr>
    <tr>
      <th>7</th>
      <td>RF PCA all th=200 Test</td>
      <td>0.447435</td>
      <td>0.442002</td>
    </tr>
    <tr>
      <th>8</th>
      <td>RF PCA all th=300 Train</td>
      <td>0.442104</td>
      <td>0.443010</td>
    </tr>
    <tr>
      <th>9</th>
      <td>RF PCA all th=300 Test</td>
      <td>0.443937</td>
      <td>0.438920</td>
    </tr>
    <tr>
      <th>10</th>
      <td>RF PCA all th=400 Train</td>
      <td>0.448806</td>
      <td>0.449645</td>
    </tr>
    <tr>
      <th>11</th>
      <td>RF PCA all th=400 Test</td>
      <td>0.449281</td>
      <td>0.444494</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Lo primero que salta a la vista es que para todos los umbrales probados la performance fue ligeramente peor que para el primer modelo sin PCA. Donde sí se vió una mejora fue en la varianza del modelo (es decir, un el <em>overfitting</em>) que en todos los casos se redujo sensiblemente. Esto era algo esperado para umbrales mayores a cero, dado que estamos reduciendo el numero de <em>features</em>. Sin embargo, lo interesante es que lo pudimos hacer sin una mayor perdida de precisión.</p>
<p>Hasta este punto llegaremos con los modelos de <em>Random Forest</em>. Si bien creo que todavía habría lugar para seguir optimizando este modelo, decidí invertir mi tiempo en un tipo de modelo distinto donde podía optar por una predicción probabilística.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="2)-Red-Neuronal">
<a class="anchor" href="#2)-Red-Neuronal" aria-hidden="true"><span class="octicon octicon-link"></span></a>2) Red Neuronal<a class="anchor-link" href="#2)-Red-Neuronal"> </a>
</h3>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como vimos en la sección anterior, los resultados obtenidos con modelos de <em>Random Forest</em> no fueron los mejores. Por otro lado, este tipo de modelos tampoco nos permitía obtener las probabilidades de cada output, que era uno de nuestros objetios. Por esta razón, decidimos probar con un tipo de modelo distinto: una red neuronal. Dado que el objetivo del problema es predecir probabilidades de un problema multiclase, la idea fue utilizar una red con:</p>
<ul>
<li>Función de costo: Categorical crossentropy</li>
<li>Funcion de activación de ultima layer: Softmax</li>
</ul>
<p>tal como se recomienda para este tipo de problemas.</p>
<p>Lo primer que probé en algunos test preliminares era utilizar simplemente una red densa. Sin embargo los resultados no fueron mejores a los obtenidos con los modelos de <em>Random Forest</em>. Por otro lado, había algo que me hacía ruido de este tipo de modelo: ¿por qué <em>inputs</em> como la localía o el tipo de competencia de un partido podrían influir en el peso de las variables de otro partido? Aún peor, ¿por qué mezclar directamente los datos de lo ocurrido para el equipo local y visitante 9 partidos atrás? Esto no me convencía, sin embargo una red densa permitía esas interconexiones (claro esta que los pesos podrían ir a cero durante el entrenamiento de la red).</p>
<p>Además de ello, había otro aspecto a tener en cuenta para mí. Como sabemos, nuestro dataset contiene información de una serie de partidos. Estos partidos tienen un orden temporal. ¿Sería lo mismo ganar 7 partidos y perder los tres inmediatamente anteriores al partido a modelar que el caso inverso? Más aún, ¿el resultado de los partidos presentaría correlaciones temporales que afectarían en un orden deteriminado? Si bien no tengo claro hasta que punto puede "aprender" una red densa (quizás puede llegar a "descubrir" ese orden), me parecía que cambiar a una arquitectura que contemplara todas estas cuestiones podría ayudar.</p>
<p>La solución que encontré pasó por armar una red mixta, combinando redes recurrentes (para tratar la información con orden temporal) con redes densas (donde mezclar el output de las redes recurrentes con las variables referentes al partido que se busca modelar). El esquema es el que se presenta en la figura siguiente:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><figure>
  
    <img class="docimage" src="/FMP_UB_final_project_2022/images/copied_from_nb/imges/red.png" alt="">
    
    
</figure>
</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Tendríamos una red recurrente (RNN) para tratar los datos históricos de cada equipo. Cada RNN sacaría un <em>output</em> de $n$ componentes, donde $n$ es el número de partidos considerados. Dentro de ella los partidos estarían interconectados del más antíguo ($i=10$) al más reciente ($i=1$). Los $2n$ outputs se combinarían con el <em>input</em> referente al partido que se desea modelar con dimension $m$. En total un vector con $2n+m$ componentes se usaría como input de la una capa densa. El <em>output</em> de esta última sería un vector de tres componentes con las probabilidades de cada uno de los posibles resultados: Victoria Equipo A, Derrota Equipo A y Empate Equipo A.</p>
<p>Esta nueva arquitectura permitiría que los partidos históricos de cada equipo se procesarían de forma independiente y teniendo en cuenta correlaciones temporales. Lo único que me quedaba pensar son la profundidad de la red y el numero de nodos en cada capa. Dado que no encontré una receta para decidir estos hiperparámetros, lo único que me quedó fue ir probando. Esto es algo que podría haber hecho de una forma más sistemática con un grid de forma similar a un <em>k-fold cross validation</em>, realmente no se me ocurrió en su momento. Terminé por ir probando algunos valores, quedándome con los que funcionaban masomenos mejor:</p>
<ul>
<li>Para la RNN use 100 unidades, con <em>drop out</em> de 0.1 y <em>recurrent drop out</em> de 0 (manteniendo los partidos siempre conectados).</li>
<li>Para la capa densa 5 capas con 500, 200, 80, 20 y 10 nodos, con <em>drop out</em> de 0.1.</li>
</ul>
<p>Adicionalmente a ello, se consideraron dos tipos de RNN: Simple y de las llamadas <em>long short-term memory</em> (LSTM). Esta segunda opción está pensada para evitar un problema llamado <em>gradient vanishing problem</em>. En la práctica esto ocurre cuando, durante el proceso de <em>backpropagation</em>, el gradiente se va haciendo cada vez más pequeño, impidiendo cambiar eficazmente los parámetros de la red (el caso opuesto también existe y es llamado <em>gradient exploding</em>. En los casos más patológicos, este fenómeno puede impedir directamente que la red neuronal continúe su entrenamiento. Esto es un fenómeno más comun cuando se consideran un mayor número de <em>inputs</em> temporales (los partidos en nuestro caso), sin embargo valía la pena probarlo. Como contrapartida, estos modelos tienen un número mayor de parámetros por unidad, lo que incrementa el tiempo de entrenamiento.</p>
<p>Veamos como nos fue con este acercamiento.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Separación de los datos en <em>input</em> y <em>target</em></strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Como primer paso, empezamos por separar nuestros <em>dataset</em> de <em>train</em> y <em>test</em> en sus respectivos <em>inputs</em> y <em>targets</em>. Dado que queríamos modelar el resultado como un vector de tres componentes, adicionalmente es necesario hacer un <em>One-Hot Encoding</em> OHE del <em>target</em>. Esto lo hacemos todo junto en un solo paso:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Split_input_and_OHE_output</span><span class="p">(</span><span class="n">FMP_train_2</span><span class="p">,</span><span class="n">FMP_test</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>train</th>
      <th>train %</th>
      <th>test</th>
      <th>test %</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>EqA_Derrota</th>
      <td>27784</td>
      <td>0.333145</td>
      <td>3846</td>
      <td>0.373688</td>
    </tr>
    <tr>
      <th>EqA_Empate</th>
      <td>27886</td>
      <td>0.334369</td>
      <td>2586</td>
      <td>0.251263</td>
    </tr>
    <tr>
      <th>EqA_Victoria</th>
      <td>27729</td>
      <td>0.332486</td>
      <td>3860</td>
      <td>0.375049</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Selección de metricas</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El siguiente paso es decidir las métricas que utilizaremos para ir evaluando el rendimiento de nuestras redes. En particular, yo elegí las siguintes:</p>
<ul>
<li>Accuracy: Para ver la precisión general.</li>
<li>F1 (macro): Para ver el rendimiento medio por clases.</li>
<li>Area under the ROC Curve (AUC): Para ver cuan bien separa la clases.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F1</span><span class="o">=</span><span class="n">tfa</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">F1Score</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span><span class="n">average</span><span class="o">=</span><span class="s1">'macro'</span><span class="p">)</span>
<span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">,</span><span class="s1">'AUC'</span><span class="p">,</span><span class="n">F1</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El resultado obtenido por cada modelo para cada una de ellas lo iremos guardando en un <em>dataframe</em>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Report</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Entrenamiento de los modelos</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Queda entonces por definir una serie de cosas:</p>
<ul>
<li>Las <em>features</em> a utilizar.</li>
<li>El tipo de red recurrente</li>
<li>El número de partidos a considerar</li>
</ul>
<p>Determinaremos estas características a traves de evaluar distintos modelos. Para ello utilizamos una serie de <em>for loops</em> anidados. Para las features consideramos tres conjuntos distintos tanto para las históricas como para las del partido a modelar. Traté de partir de las <em>features</em> que yo consideraba más fundamentales, para ir luego agregando otras adicionales. Se podría ir agregando de a una, pero ya el esquema propuesto tomó bastanate tiempo (9hs aproximadamente) por lo que no quería hacerlo más largo.</p>
<p>En total se entrenaron 180 redes neuronales:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">F_h_1</span><span class="o">=</span><span class="p">[</span><span class="s1">'outcome_V'</span><span class="p">,</span><span class="s1">'outcome_D'</span><span class="p">,</span><span class="s1">'play_home'</span><span class="p">]</span>
<span class="n">F_h_2</span><span class="o">=</span><span class="p">[</span><span class="s1">'play_home'</span><span class="p">,</span><span class="s1">'goal_diff'</span><span class="p">]</span>
<span class="n">F_h_3</span><span class="o">=</span><span class="p">[</span><span class="s1">'play_home'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">,</span><span class="s1">'rating_diff'</span><span class="p">,</span><span class="s1">'goal_diff'</span><span class="p">,</span><span class="s1">'coach_continuity'</span><span class="p">,</span><span class="s1">'relevance'</span><span class="p">,</span><span class="s1">'is_friendly'</span><span class="p">]</span>


<span class="n">N_Features_1</span><span class="o">=</span><span class="p">[</span><span class="s1">'EqA_Local'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">]</span>
<span class="n">N_Features_2</span><span class="o">=</span><span class="p">[</span><span class="s1">'Rating_diff'</span><span class="p">,</span><span class="s1">'EqA_Local'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">,</span><span class="s1">'diff_num_partidos_diez_dias'</span><span class="p">,</span><span class="s1">'is_friendly'</span><span class="p">]</span>
<span class="n">N_Features_3</span><span class="o">=</span><span class="p">[</span><span class="s1">'Rating_diff'</span><span class="p">,</span><span class="s1">'EqA_Local'</span><span class="p">,</span><span class="s1">'is_cup'</span><span class="p">,</span><span class="s1">'Equipo_A_coach_continuity'</span><span class="p">,</span><span class="s1">'Equipo_B_coach_continuity'</span><span class="p">,</span><span class="s1">'diff_num_partidos_diez_dias'</span><span class="p">,</span><span class="s1">'diff_num_partidos_tres_semanas'</span><span class="p">,</span><span class="s1">'is_friendly'</span><span class="p">]</span>
            
<span class="n">index</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">F_h</span> <span class="ow">in</span> <span class="p">[</span><span class="n">F_h_1</span><span class="p">,</span><span class="n">F_h_2</span><span class="p">,</span><span class="n">F_h_3</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">N_Features</span> <span class="ow">in</span> <span class="p">[</span><span class="n">N_Features_1</span><span class="p">,</span><span class="n">N_Features_2</span><span class="p">,</span><span class="n">N_Features_3</span><span class="p">]:</span>
        <span class="k">for</span> <span class="n">tipo_de_RNN</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">'LSTM'</span><span class="p">,</span><span class="s1">'Simple'</span><span class="p">]:</span>
            <span class="k">for</span> <span class="n">n_partidos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">11</span><span class="p">):</span>
                
                <span class="n">M_A</span><span class="p">,</span><span class="n">M_A_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_RNN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">F_h</span><span class="p">,[</span><span class="s1">'Equipo_A'</span><span class="p">],</span><span class="n">n_partidos</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                <span class="n">M_B</span><span class="p">,</span><span class="n">M_B_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_RNN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">F_h</span><span class="p">,[</span><span class="s1">'Equipo_B'</span><span class="p">],</span><span class="n">n_partidos</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="n">H_Features</span><span class="o">=</span><span class="p">[]</span>
                <span class="n">A_train</span><span class="p">,</span><span class="n">A_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_NN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">H_Features</span><span class="p">,</span><span class="n">N_Features</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 

                <span class="n">fffmp</span><span class="o">.</span><span class="n">clean_all_models</span><span class="p">()</span>
                <span class="n">time_steps</span><span class="o">=</span><span class="n">n_partidos</span>
                <span class="n">predictors</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">F_h</span><span class="p">)</span>
                <span class="n">tipo_RNN</span><span class="o">=</span><span class="n">tipo_de_RNN</span>
                <span class="n">recurrent_units</span><span class="o">=</span><span class="mi">100</span>
                <span class="n">dense_units</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span>
                <span class="n">dense_drop_out</span><span class="o">=</span><span class="mf">0.1</span>
                <span class="n">out_shape</span><span class="o">=</span><span class="mi">3</span>
                <span class="n">input_shape_history</span><span class="o">=</span><span class="p">(</span><span class="n">time_steps</span><span class="p">,</span><span class="n">predictors</span><span class="p">)</span>
                <span class="n">input_shape_now</span><span class="o">=</span><span class="n">A_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">,</span><span class="s1">'AUC'</span><span class="p">,</span><span class="n">F1</span><span class="p">]</span>
                <span class="n">IFA</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Create_Mix_Model</span><span class="p">(</span><span class="n">tipo_RNN</span> <span class="p">,</span><span class="n">recurrent_units</span><span class="p">,</span> <span class="n">dense_units</span><span class="p">,</span><span class="n">dense_drop_out</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">input_shape_history</span><span class="p">,</span><span class="n">input_shape_now</span><span class="p">,</span><span class="n">metrics</span><span class="p">)</span>

                <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">400</span>
                <span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
                <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span>
                <span class="n">y_train_pred</span><span class="p">,</span><span class="n">y_test_pred</span><span class="p">,</span><span class="n">Results</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Full_train_and_Report</span><span class="p">(</span><span class="n">IFA</span><span class="p">,[</span><span class="n">M_A</span><span class="p">,</span><span class="n">M_B</span><span class="p">,</span><span class="n">A_train</span><span class="p">],</span><span class="n">y_train</span><span class="p">,[</span><span class="n">M_A_test</span><span class="p">,</span><span class="n">M_B_test</span><span class="p">,</span><span class="n">A_test</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">validation_split</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">history_plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">Report_print</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

                <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span>
                <span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
                <span class="n">Report</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Append_line_to_report_table</span><span class="p">(</span><span class="n">Report</span><span class="p">,</span><span class="n">Results</span><span class="p">,</span><span class="n">name</span><span class="p">,</span><span class="n">tipo_de_RNN</span><span class="p">,</span><span class="n">n_partidos</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">F_h</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">N_Features</span><span class="p">))</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 202)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          101500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 302,043
Trainable params: 302,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 7s 31ms/step - loss: 1.0920 - accuracy: 0.3736 - auc: 0.5488 - f1_score: 0.3505 - val_loss: 1.0870 - val_accuracy: 0.3891 - val_auc: 0.5672 - val_f1_score: 0.3104
Epoch 2/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0871 - accuracy: 0.3856 - auc: 0.5630 - f1_score: 0.3135 - val_loss: 1.0845 - val_accuracy: 0.3902 - val_auc: 0.5693 - val_f1_score: 0.3115
Epoch 3/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0861 - accuracy: 0.3885 - auc: 0.5654 - f1_score: 0.3327 - val_loss: 1.0846 - val_accuracy: 0.3896 - val_auc: 0.5697 - val_f1_score: 0.3561
Epoch 4/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0859 - accuracy: 0.3871 - auc: 0.5657 - f1_score: 0.3382 - val_loss: 1.0846 - val_accuracy: 0.3874 - val_auc: 0.5696 - val_f1_score: 0.3429
Epoch 5/50
188/188 [==============================] - 6s 29ms/step - loss: 1.0854 - accuracy: 0.3887 - auc: 0.5672 - f1_score: 0.3402 - val_loss: 1.0848 - val_accuracy: 0.3902 - val_auc: 0.5701 - val_f1_score: 0.3551
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 402)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          201500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 402,043
Trainable params: 402,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 11s 46ms/step - loss: 1.0898 - accuracy: 0.3776 - auc: 0.5540 - f1_score: 0.3380 - val_loss: 1.0848 - val_accuracy: 0.3923 - val_auc: 0.5731 - val_f1_score: 0.3332
Epoch 2/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0823 - accuracy: 0.3970 - auc: 0.5753 - f1_score: 0.3396 - val_loss: 1.0824 - val_accuracy: 0.3964 - val_auc: 0.5746 - val_f1_score: 0.3495
Epoch 3/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0813 - accuracy: 0.3976 - auc: 0.5772 - f1_score: 0.3623 - val_loss: 1.0817 - val_accuracy: 0.3970 - val_auc: 0.5768 - val_f1_score: 0.3607
Epoch 4/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0809 - accuracy: 0.3994 - auc: 0.5781 - f1_score: 0.3733 - val_loss: 1.0815 - val_accuracy: 0.3981 - val_auc: 0.5770 - val_f1_score: 0.3711
Epoch 5/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0809 - accuracy: 0.3979 - auc: 0.5783 - f1_score: 0.3727 - val_loss: 1.0819 - val_accuracy: 0.3958 - val_auc: 0.5769 - val_f1_score: 0.3767
Epoch 6/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0806 - accuracy: 0.3991 - auc: 0.5793 - f1_score: 0.3769 - val_loss: 1.0817 - val_accuracy: 0.3974 - val_auc: 0.5772 - val_f1_score: 0.3687
Epoch 7/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0802 - accuracy: 0.4001 - auc: 0.5794 - f1_score: 0.3783 - val_loss: 1.0819 - val_accuracy: 0.3947 - val_auc: 0.5767 - val_f1_score: 0.3600
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 602)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          301500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 502,043
Trainable params: 502,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 60ms/step - loss: 1.0878 - accuracy: 0.3762 - auc: 0.5619 - f1_score: 0.3878 - val_loss: 1.0792 - val_accuracy: 0.3980 - val_auc: 0.5803 - val_f1_score: 0.3376
Epoch 2/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0784 - accuracy: 0.4013 - auc: 0.5824 - f1_score: 0.3709 - val_loss: 1.0784 - val_accuracy: 0.4036 - val_auc: 0.5819 - val_f1_score: 0.3551
Epoch 3/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0775 - accuracy: 0.4040 - auc: 0.5841 - f1_score: 0.3672 - val_loss: 1.0777 - val_accuracy: 0.4019 - val_auc: 0.5828 - val_f1_score: 0.3678
Epoch 4/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0768 - accuracy: 0.4031 - auc: 0.5857 - f1_score: 0.3716 - val_loss: 1.0786 - val_accuracy: 0.4007 - val_auc: 0.5820 - val_f1_score: 0.3619
Epoch 5/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0767 - accuracy: 0.4055 - auc: 0.5863 - f1_score: 0.3737 - val_loss: 1.0783 - val_accuracy: 0.3998 - val_auc: 0.5828 - val_f1_score: 0.3603
Epoch 6/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0763 - accuracy: 0.4062 - auc: 0.5869 - f1_score: 0.3806 - val_loss: 1.0779 - val_accuracy: 0.4046 - val_auc: 0.5848 - val_f1_score: 0.3797
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 802)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          401500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 602,043
Trainable params: 602,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 74ms/step - loss: 1.0813 - accuracy: 0.4002 - auc: 0.5763 - f1_score: 0.3697 - val_loss: 1.0738 - val_accuracy: 0.4120 - val_auc: 0.5933 - val_f1_score: 0.3755
Epoch 2/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0735 - accuracy: 0.4108 - auc: 0.5913 - f1_score: 0.3881 - val_loss: 1.0734 - val_accuracy: 0.4085 - val_auc: 0.5932 - val_f1_score: 0.3912
Epoch 3/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0728 - accuracy: 0.4127 - auc: 0.5934 - f1_score: 0.3984 - val_loss: 1.0734 - val_accuracy: 0.4091 - val_auc: 0.5929 - val_f1_score: 0.3989
Epoch 4/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0721 - accuracy: 0.4137 - auc: 0.5946 - f1_score: 0.4008 - val_loss: 1.0734 - val_accuracy: 0.4110 - val_auc: 0.5937 - val_f1_score: 0.4035
Epoch 5/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0712 - accuracy: 0.4119 - auc: 0.5954 - f1_score: 0.4014 - val_loss: 1.0731 - val_accuracy: 0.4150 - val_auc: 0.5948 - val_f1_score: 0.4028
Epoch 6/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0712 - accuracy: 0.4140 - auc: 0.5965 - f1_score: 0.4030 - val_loss: 1.0736 - val_accuracy: 0.4092 - val_auc: 0.5929 - val_f1_score: 0.4013
Epoch 7/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0713 - accuracy: 0.4140 - auc: 0.5962 - f1_score: 0.4056 - val_loss: 1.0739 - val_accuracy: 0.4083 - val_auc: 0.5929 - val_f1_score: 0.3936
Epoch 8/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0708 - accuracy: 0.4157 - auc: 0.5973 - f1_score: 0.4048 - val_loss: 1.0737 - val_accuracy: 0.4118 - val_auc: 0.5933 - val_f1_score: 0.3999
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          501500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 702,043
Trainable params: 702,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 77ms/step - loss: 1.0797 - accuracy: 0.4032 - auc: 0.5797 - f1_score: 0.3941 - val_loss: 1.0726 - val_accuracy: 0.4090 - val_auc: 0.5914 - val_f1_score: 0.3909
Epoch 2/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0707 - accuracy: 0.4126 - auc: 0.5958 - f1_score: 0.4001 - val_loss: 1.0719 - val_accuracy: 0.4097 - val_auc: 0.5933 - val_f1_score: 0.3998
Epoch 3/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0698 - accuracy: 0.4146 - auc: 0.5979 - f1_score: 0.4047 - val_loss: 1.0722 - val_accuracy: 0.4046 - val_auc: 0.5926 - val_f1_score: 0.3897
Epoch 4/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0691 - accuracy: 0.4143 - auc: 0.5990 - f1_score: 0.4030 - val_loss: 1.0723 - val_accuracy: 0.4083 - val_auc: 0.5925 - val_f1_score: 0.3929
Epoch 5/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0685 - accuracy: 0.4172 - auc: 0.6006 - f1_score: 0.4054 - val_loss: 1.0721 - val_accuracy: 0.4073 - val_auc: 0.5935 - val_f1_score: 0.3913
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1202)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          601500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 802,043
Trainable params: 802,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 21s 96ms/step - loss: 1.0780 - accuracy: 0.4069 - auc: 0.5827 - f1_score: 0.3936 - val_loss: 1.0739 - val_accuracy: 0.4085 - val_auc: 0.5885 - val_f1_score: 0.3553
Epoch 2/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0697 - accuracy: 0.4141 - auc: 0.5972 - f1_score: 0.3629 - val_loss: 1.0711 - val_accuracy: 0.4082 - val_auc: 0.5951 - val_f1_score: 0.3633
Epoch 3/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0678 - accuracy: 0.4175 - auc: 0.6013 - f1_score: 0.3759 - val_loss: 1.0705 - val_accuracy: 0.4091 - val_auc: 0.5969 - val_f1_score: 0.3767
Epoch 4/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0665 - accuracy: 0.4177 - auc: 0.6033 - f1_score: 0.3896 - val_loss: 1.0708 - val_accuracy: 0.4091 - val_auc: 0.5960 - val_f1_score: 0.3839
Epoch 5/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0660 - accuracy: 0.4202 - auc: 0.6045 - f1_score: 0.3916 - val_loss: 1.0706 - val_accuracy: 0.4048 - val_auc: 0.5963 - val_f1_score: 0.3737
Epoch 6/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0656 - accuracy: 0.4209 - auc: 0.6053 - f1_score: 0.3954 - val_loss: 1.0704 - val_accuracy: 0.4097 - val_auc: 0.5975 - val_f1_score: 0.3877
Epoch 7/50
188/188 [==============================] - 16s 88ms/step - loss: 1.0651 - accuracy: 0.4198 - auc: 0.6059 - f1_score: 0.3958 - val_loss: 1.0707 - val_accuracy: 0.4080 - val_auc: 0.5962 - val_f1_score: 0.3856
Epoch 8/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0651 - accuracy: 0.4210 - auc: 0.6066 - f1_score: 0.3967 - val_loss: 1.0706 - val_accuracy: 0.4067 - val_auc: 0.5973 - val_f1_score: 0.3811
Epoch 9/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0646 - accuracy: 0.4229 - auc: 0.6072 - f1_score: 0.3980 - val_loss: 1.0708 - val_accuracy: 0.4101 - val_auc: 0.5963 - val_f1_score: 0.3877
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1402)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          701500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 902,043
Trainable params: 902,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 28s 122ms/step - loss: 1.0814 - accuracy: 0.3984 - auc: 0.5792 - f1_score: 0.4003 - val_loss: 1.0707 - val_accuracy: 0.4113 - val_auc: 0.5957 - val_f1_score: 0.3743
Epoch 2/50
188/188 [==============================] - 19s 104ms/step - loss: 1.0674 - accuracy: 0.4195 - auc: 0.6024 - f1_score: 0.4061 - val_loss: 1.0688 - val_accuracy: 0.4118 - val_auc: 0.5986 - val_f1_score: 0.4013
Epoch 3/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0659 - accuracy: 0.4211 - auc: 0.6050 - f1_score: 0.4108 - val_loss: 1.0685 - val_accuracy: 0.4137 - val_auc: 0.5997 - val_f1_score: 0.4048
Epoch 4/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0646 - accuracy: 0.4245 - auc: 0.6073 - f1_score: 0.4157 - val_loss: 1.0688 - val_accuracy: 0.4114 - val_auc: 0.5990 - val_f1_score: 0.3987
Epoch 5/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0642 - accuracy: 0.4241 - auc: 0.6080 - f1_score: 0.4130 - val_loss: 1.0689 - val_accuracy: 0.4143 - val_auc: 0.5988 - val_f1_score: 0.4042
Epoch 6/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0630 - accuracy: 0.4260 - auc: 0.6100 - f1_score: 0.4163 - val_loss: 1.0688 - val_accuracy: 0.4144 - val_auc: 0.5996 - val_f1_score: 0.3994
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1602)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          801500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,002,043
Trainable params: 1,002,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 27s 117ms/step - loss: 1.0764 - accuracy: 0.4044 - auc: 0.5837 - f1_score: 0.3921 - val_loss: 1.0708 - val_accuracy: 0.4024 - val_auc: 0.5910 - val_f1_score: 0.3413
Epoch 2/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0652 - accuracy: 0.4213 - auc: 0.6046 - f1_score: 0.3849 - val_loss: 1.0699 - val_accuracy: 0.4079 - val_auc: 0.5943 - val_f1_score: 0.3640
Epoch 3/50
188/188 [==============================] - 21s 109ms/step - loss: 1.0644 - accuracy: 0.4228 - auc: 0.6069 - f1_score: 0.3970 - val_loss: 1.0691 - val_accuracy: 0.4091 - val_auc: 0.5963 - val_f1_score: 0.3850
Epoch 4/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0632 - accuracy: 0.4268 - auc: 0.6100 - f1_score: 0.4035 - val_loss: 1.0684 - val_accuracy: 0.4118 - val_auc: 0.5986 - val_f1_score: 0.3903
Epoch 5/50
188/188 [==============================] - 22s 115ms/step - loss: 1.0625 - accuracy: 0.4273 - auc: 0.6108 - f1_score: 0.4109 - val_loss: 1.0693 - val_accuracy: 0.4104 - val_auc: 0.5969 - val_f1_score: 0.3973
Epoch 6/50
188/188 [==============================] - 22s 115ms/step - loss: 1.0610 - accuracy: 0.4293 - auc: 0.6130 - f1_score: 0.4131 - val_loss: 1.0690 - val_accuracy: 0.4141 - val_auc: 0.5982 - val_f1_score: 0.3954
Epoch 7/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0606 - accuracy: 0.4314 - auc: 0.6139 - f1_score: 0.4135 - val_loss: 1.0703 - val_accuracy: 0.4124 - val_auc: 0.5955 - val_f1_score: 0.3963
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1802)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          901500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,102,043
Trainable params: 1,102,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 27s 126ms/step - loss: 1.0803 - accuracy: 0.4081 - auc: 0.5791 - f1_score: 0.3999 - val_loss: 1.0703 - val_accuracy: 0.4088 - val_auc: 0.5972 - val_f1_score: 0.3905
Epoch 2/50
188/188 [==============================] - 25s 131ms/step - loss: 1.0645 - accuracy: 0.4238 - auc: 0.6074 - f1_score: 0.4149 - val_loss: 1.0688 - val_accuracy: 0.4094 - val_auc: 0.5988 - val_f1_score: 0.3923
Epoch 3/50
188/188 [==============================] - 23s 124ms/step - loss: 1.0622 - accuracy: 0.4266 - auc: 0.6109 - f1_score: 0.4185 - val_loss: 1.0680 - val_accuracy: 0.4101 - val_auc: 0.5998 - val_f1_score: 0.4016
Epoch 4/50
188/188 [==============================] - 24s 125ms/step - loss: 1.0618 - accuracy: 0.4278 - auc: 0.6119 - f1_score: 0.4180 - val_loss: 1.0685 - val_accuracy: 0.4116 - val_auc: 0.5985 - val_f1_score: 0.4038
Epoch 5/50
188/188 [==============================] - 25s 132ms/step - loss: 1.0607 - accuracy: 0.4280 - auc: 0.6132 - f1_score: 0.4199 - val_loss: 1.0680 - val_accuracy: 0.4092 - val_auc: 0.5996 - val_f1_score: 0.3917
Epoch 6/50
188/188 [==============================] - 23s 125ms/step - loss: 1.0596 - accuracy: 0.4306 - auc: 0.6153 - f1_score: 0.4237 - val_loss: 1.0689 - val_accuracy: 0.4091 - val_auc: 0.5985 - val_f1_score: 0.3963
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1001500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,202,043
Trainable params: 1,202,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 34s 158ms/step - loss: 1.0812 - accuracy: 0.3919 - auc: 0.5749 - f1_score: 0.4024 - val_loss: 1.0688 - val_accuracy: 0.4098 - val_auc: 0.5988 - val_f1_score: 0.3753
Epoch 2/50
188/188 [==============================] - 29s 153ms/step - loss: 1.0641 - accuracy: 0.4235 - auc: 0.6070 - f1_score: 0.3973 - val_loss: 1.0665 - val_accuracy: 0.4129 - val_auc: 0.6031 - val_f1_score: 0.3918
Epoch 3/50
188/188 [==============================] - 30s 158ms/step - loss: 1.0619 - accuracy: 0.4265 - auc: 0.6109 - f1_score: 0.4135 - val_loss: 1.0666 - val_accuracy: 0.4127 - val_auc: 0.6028 - val_f1_score: 0.3935
Epoch 4/50
188/188 [==============================] - 31s 164ms/step - loss: 1.0609 - accuracy: 0.4296 - auc: 0.6129 - f1_score: 0.4190 - val_loss: 1.0666 - val_accuracy: 0.4140 - val_auc: 0.6029 - val_f1_score: 0.4013
Epoch 5/50
188/188 [==============================] - 29s 154ms/step - loss: 1.0592 - accuracy: 0.4315 - auc: 0.6159 - f1_score: 0.4219 - val_loss: 1.0671 - val_accuracy: 0.4125 - val_auc: 0.6017 - val_f1_score: 0.3992
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 202)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          101500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 240,243
Trainable params: 240,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 7s 27ms/step - loss: 1.0916 - accuracy: 0.3711 - auc: 0.5473 - f1_score: 0.3883 - val_loss: 1.0859 - val_accuracy: 0.3869 - val_auc: 0.5672 - val_f1_score: 0.3579
Epoch 2/50
188/188 [==============================] - 4s 24ms/step - loss: 1.0875 - accuracy: 0.3866 - auc: 0.5621 - f1_score: 0.3572 - val_loss: 1.0851 - val_accuracy: 0.3861 - val_auc: 0.5684 - val_f1_score: 0.3659
Epoch 3/50
188/188 [==============================] - 5s 24ms/step - loss: 1.0865 - accuracy: 0.3877 - auc: 0.5650 - f1_score: 0.3561 - val_loss: 1.0849 - val_accuracy: 0.3865 - val_auc: 0.5698 - val_f1_score: 0.3464
Epoch 4/50
188/188 [==============================] - 4s 24ms/step - loss: 1.0863 - accuracy: 0.3873 - auc: 0.5654 - f1_score: 0.3563 - val_loss: 1.0843 - val_accuracy: 0.3903 - val_auc: 0.5720 - val_f1_score: 0.3680
Epoch 5/50
188/188 [==============================] - 4s 24ms/step - loss: 1.0861 - accuracy: 0.3882 - auc: 0.5659 - f1_score: 0.3593 - val_loss: 1.0842 - val_accuracy: 0.3917 - val_auc: 0.5717 - val_f1_score: 0.3586
Epoch 6/50
188/188 [==============================] - 5s 24ms/step - loss: 1.0860 - accuracy: 0.3870 - auc: 0.5658 - f1_score: 0.3629 - val_loss: 1.0846 - val_accuracy: 0.3904 - val_auc: 0.5718 - val_f1_score: 0.3618
Epoch 7/50
188/188 [==============================] - 4s 24ms/step - loss: 1.0857 - accuracy: 0.3881 - auc: 0.5664 - f1_score: 0.3666 - val_loss: 1.0842 - val_accuracy: 0.3897 - val_auc: 0.5727 - val_f1_score: 0.3604
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 402)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          201500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 340,243
Trainable params: 340,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 9s 35ms/step - loss: 1.0911 - accuracy: 0.3706 - auc: 0.5490 - f1_score: 0.3688 - val_loss: 1.0861 - val_accuracy: 0.3890 - val_auc: 0.5660 - val_f1_score: 0.3286
Epoch 2/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0850 - accuracy: 0.3901 - auc: 0.5682 - f1_score: 0.3529 - val_loss: 1.0838 - val_accuracy: 0.3912 - val_auc: 0.5718 - val_f1_score: 0.3325
Epoch 3/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0824 - accuracy: 0.3940 - auc: 0.5737 - f1_score: 0.3570 - val_loss: 1.0835 - val_accuracy: 0.3909 - val_auc: 0.5739 - val_f1_score: 0.3227
Epoch 4/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0820 - accuracy: 0.3962 - auc: 0.5755 - f1_score: 0.3611 - val_loss: 1.0824 - val_accuracy: 0.3916 - val_auc: 0.5754 - val_f1_score: 0.3391
Epoch 5/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0810 - accuracy: 0.3987 - auc: 0.5777 - f1_score: 0.3658 - val_loss: 1.0829 - val_accuracy: 0.3923 - val_auc: 0.5745 - val_f1_score: 0.3439
Epoch 6/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0806 - accuracy: 0.3989 - auc: 0.5788 - f1_score: 0.3676 - val_loss: 1.0824 - val_accuracy: 0.3918 - val_auc: 0.5749 - val_f1_score: 0.3378
Epoch 7/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0801 - accuracy: 0.3991 - auc: 0.5797 - f1_score: 0.3702 - val_loss: 1.0830 - val_accuracy: 0.3924 - val_auc: 0.5746 - val_f1_score: 0.3398
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 602)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          301500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 440,243
Trainable params: 440,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 41ms/step - loss: 1.0894 - accuracy: 0.3819 - auc: 0.5540 - f1_score: 0.3368 - val_loss: 1.0846 - val_accuracy: 0.3886 - val_auc: 0.5693 - val_f1_score: 0.3431
Epoch 2/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0814 - accuracy: 0.3976 - auc: 0.5755 - f1_score: 0.3604 - val_loss: 1.0813 - val_accuracy: 0.4005 - val_auc: 0.5759 - val_f1_score: 0.3843
Epoch 3/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0797 - accuracy: 0.4000 - auc: 0.5793 - f1_score: 0.3768 - val_loss: 1.0803 - val_accuracy: 0.4001 - val_auc: 0.5774 - val_f1_score: 0.3908
Epoch 4/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0784 - accuracy: 0.4019 - auc: 0.5826 - f1_score: 0.3856 - val_loss: 1.0802 - val_accuracy: 0.3958 - val_auc: 0.5778 - val_f1_score: 0.3793
Epoch 5/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0774 - accuracy: 0.4063 - auc: 0.5849 - f1_score: 0.3878 - val_loss: 1.0796 - val_accuracy: 0.4013 - val_auc: 0.5801 - val_f1_score: 0.3935
Epoch 6/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0771 - accuracy: 0.4047 - auc: 0.5850 - f1_score: 0.3883 - val_loss: 1.0787 - val_accuracy: 0.4020 - val_auc: 0.5808 - val_f1_score: 0.3866
Epoch 7/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0767 - accuracy: 0.4059 - auc: 0.5859 - f1_score: 0.3916 - val_loss: 1.0788 - val_accuracy: 0.4001 - val_auc: 0.5811 - val_f1_score: 0.3876
Epoch 8/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0758 - accuracy: 0.4049 - auc: 0.5871 - f1_score: 0.3891 - val_loss: 1.0792 - val_accuracy: 0.4014 - val_auc: 0.5815 - val_f1_score: 0.3881
Epoch 9/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0752 - accuracy: 0.4063 - auc: 0.5887 - f1_score: 0.3915 - val_loss: 1.0787 - val_accuracy: 0.4025 - val_auc: 0.5828 - val_f1_score: 0.3946
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 802)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          401500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 540,243
Trainable params: 540,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 11s 45ms/step - loss: 1.0870 - accuracy: 0.3834 - auc: 0.5602 - f1_score: 0.3777 - val_loss: 1.0802 - val_accuracy: 0.3982 - val_auc: 0.5788 - val_f1_score: 0.3645
Epoch 2/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0780 - accuracy: 0.4013 - auc: 0.5827 - f1_score: 0.3652 - val_loss: 1.0779 - val_accuracy: 0.4028 - val_auc: 0.5831 - val_f1_score: 0.3825
Epoch 3/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0753 - accuracy: 0.4068 - auc: 0.5877 - f1_score: 0.3811 - val_loss: 1.0769 - val_accuracy: 0.4055 - val_auc: 0.5862 - val_f1_score: 0.3931
Epoch 4/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0740 - accuracy: 0.4089 - auc: 0.5905 - f1_score: 0.3926 - val_loss: 1.0765 - val_accuracy: 0.4054 - val_auc: 0.5872 - val_f1_score: 0.3947
Epoch 5/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0732 - accuracy: 0.4091 - auc: 0.5916 - f1_score: 0.3911 - val_loss: 1.0763 - val_accuracy: 0.4072 - val_auc: 0.5873 - val_f1_score: 0.3866
Epoch 6/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0730 - accuracy: 0.4105 - auc: 0.5927 - f1_score: 0.3948 - val_loss: 1.0762 - val_accuracy: 0.4029 - val_auc: 0.5868 - val_f1_score: 0.3895
Epoch 7/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0717 - accuracy: 0.4134 - auc: 0.5950 - f1_score: 0.3972 - val_loss: 1.0749 - val_accuracy: 0.4041 - val_auc: 0.5901 - val_f1_score: 0.3885
Epoch 8/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0710 - accuracy: 0.4151 - auc: 0.5960 - f1_score: 0.3992 - val_loss: 1.0766 - val_accuracy: 0.4047 - val_auc: 0.5868 - val_f1_score: 0.3888
Epoch 9/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0704 - accuracy: 0.4136 - auc: 0.5971 - f1_score: 0.4006 - val_loss: 1.0758 - val_accuracy: 0.4047 - val_auc: 0.5881 - val_f1_score: 0.3929
Epoch 10/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0708 - accuracy: 0.4133 - auc: 0.5961 - f1_score: 0.4027 - val_loss: 1.0758 - val_accuracy: 0.4070 - val_auc: 0.5888 - val_f1_score: 0.3960
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          501500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 640,243
Trainable params: 640,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 52ms/step - loss: 1.0889 - accuracy: 0.3732 - auc: 0.5533 - f1_score: 0.3943 - val_loss: 1.0826 - val_accuracy: 0.3946 - val_auc: 0.5737 - val_f1_score: 0.3745
Epoch 2/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0772 - accuracy: 0.4039 - auc: 0.5846 - f1_score: 0.3954 - val_loss: 1.0772 - val_accuracy: 0.3990 - val_auc: 0.5838 - val_f1_score: 0.3905
Epoch 3/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0733 - accuracy: 0.4111 - auc: 0.5918 - f1_score: 0.4034 - val_loss: 1.0751 - val_accuracy: 0.4064 - val_auc: 0.5875 - val_f1_score: 0.3964
Epoch 4/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0717 - accuracy: 0.4120 - auc: 0.5948 - f1_score: 0.4048 - val_loss: 1.0752 - val_accuracy: 0.4066 - val_auc: 0.5869 - val_f1_score: 0.3909
Epoch 5/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0704 - accuracy: 0.4140 - auc: 0.5964 - f1_score: 0.4017 - val_loss: 1.0749 - val_accuracy: 0.4084 - val_auc: 0.5879 - val_f1_score: 0.3981
Epoch 6/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0695 - accuracy: 0.4157 - auc: 0.5986 - f1_score: 0.4068 - val_loss: 1.0745 - val_accuracy: 0.4038 - val_auc: 0.5894 - val_f1_score: 0.3902
Epoch 7/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0684 - accuracy: 0.4160 - auc: 0.6003 - f1_score: 0.4076 - val_loss: 1.0744 - val_accuracy: 0.4102 - val_auc: 0.5890 - val_f1_score: 0.3886
Epoch 8/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0684 - accuracy: 0.4182 - auc: 0.6002 - f1_score: 0.4068 - val_loss: 1.0738 - val_accuracy: 0.4076 - val_auc: 0.5902 - val_f1_score: 0.3918
Epoch 9/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0670 - accuracy: 0.4200 - auc: 0.6029 - f1_score: 0.4093 - val_loss: 1.0739 - val_accuracy: 0.4071 - val_auc: 0.5907 - val_f1_score: 0.3968
Epoch 10/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0666 - accuracy: 0.4218 - auc: 0.6042 - f1_score: 0.4125 - val_loss: 1.0737 - val_accuracy: 0.4046 - val_auc: 0.5909 - val_f1_score: 0.3899
Epoch 11/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0659 - accuracy: 0.4224 - auc: 0.6048 - f1_score: 0.4114 - val_loss: 1.0732 - val_accuracy: 0.4070 - val_auc: 0.5926 - val_f1_score: 0.3883
Epoch 12/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0646 - accuracy: 0.4247 - auc: 0.6078 - f1_score: 0.4157 - val_loss: 1.0737 - val_accuracy: 0.4065 - val_auc: 0.5917 - val_f1_score: 0.3909
Epoch 13/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0649 - accuracy: 0.4211 - auc: 0.6065 - f1_score: 0.4114 - val_loss: 1.0741 - val_accuracy: 0.4050 - val_auc: 0.5902 - val_f1_score: 0.3945
Epoch 14/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0641 - accuracy: 0.4238 - auc: 0.6081 - f1_score: 0.4159 - val_loss: 1.0724 - val_accuracy: 0.4108 - val_auc: 0.5933 - val_f1_score: 0.4023
Epoch 15/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0634 - accuracy: 0.4241 - auc: 0.6089 - f1_score: 0.4160 - val_loss: 1.0740 - val_accuracy: 0.4078 - val_auc: 0.5919 - val_f1_score: 0.3956
Epoch 16/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0639 - accuracy: 0.4253 - auc: 0.6081 - f1_score: 0.4173 - val_loss: 1.0742 - val_accuracy: 0.4085 - val_auc: 0.5909 - val_f1_score: 0.3951
Epoch 17/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0624 - accuracy: 0.4275 - auc: 0.6103 - f1_score: 0.4206 - val_loss: 1.0730 - val_accuracy: 0.4135 - val_auc: 0.5928 - val_f1_score: 0.3987
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1202)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          601500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 740,243
Trainable params: 740,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 61ms/step - loss: 1.0839 - accuracy: 0.3900 - auc: 0.5675 - f1_score: 0.3991 - val_loss: 1.0769 - val_accuracy: 0.3990 - val_auc: 0.5846 - val_f1_score: 0.3790
Epoch 2/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0726 - accuracy: 0.4100 - auc: 0.5930 - f1_score: 0.3951 - val_loss: 1.0746 - val_accuracy: 0.4073 - val_auc: 0.5883 - val_f1_score: 0.3975
Epoch 3/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0700 - accuracy: 0.4141 - auc: 0.5970 - f1_score: 0.4023 - val_loss: 1.0750 - val_accuracy: 0.4019 - val_auc: 0.5887 - val_f1_score: 0.3893
Epoch 4/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0689 - accuracy: 0.4159 - auc: 0.5989 - f1_score: 0.4057 - val_loss: 1.0728 - val_accuracy: 0.4083 - val_auc: 0.5922 - val_f1_score: 0.3999
Epoch 5/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0669 - accuracy: 0.4173 - auc: 0.6019 - f1_score: 0.4073 - val_loss: 1.0722 - val_accuracy: 0.4134 - val_auc: 0.5929 - val_f1_score: 0.4087
Epoch 6/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0652 - accuracy: 0.4208 - auc: 0.6059 - f1_score: 0.4113 - val_loss: 1.0727 - val_accuracy: 0.4106 - val_auc: 0.5930 - val_f1_score: 0.4042
Epoch 7/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0650 - accuracy: 0.4197 - auc: 0.6058 - f1_score: 0.4082 - val_loss: 1.0735 - val_accuracy: 0.4101 - val_auc: 0.5914 - val_f1_score: 0.3946
Epoch 8/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0637 - accuracy: 0.4241 - auc: 0.6082 - f1_score: 0.4132 - val_loss: 1.0735 - val_accuracy: 0.4088 - val_auc: 0.5910 - val_f1_score: 0.4047
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1402)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          701500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 840,243
Trainable params: 840,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 15s 68ms/step - loss: 1.0902 - accuracy: 0.3604 - auc: 0.5494 - f1_score: 0.4032 - val_loss: 1.0813 - val_accuracy: 0.3963 - val_auc: 0.5730 - val_f1_score: 0.3513
Epoch 2/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0758 - accuracy: 0.4023 - auc: 0.5851 - f1_score: 0.3932 - val_loss: 1.0760 - val_accuracy: 0.4018 - val_auc: 0.5852 - val_f1_score: 0.3617
Epoch 3/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0712 - accuracy: 0.4105 - auc: 0.5938 - f1_score: 0.3990 - val_loss: 1.0747 - val_accuracy: 0.4038 - val_auc: 0.5885 - val_f1_score: 0.3824
Epoch 4/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0679 - accuracy: 0.4170 - auc: 0.6009 - f1_score: 0.4112 - val_loss: 1.0728 - val_accuracy: 0.4052 - val_auc: 0.5928 - val_f1_score: 0.3833
Epoch 5/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0662 - accuracy: 0.4215 - auc: 0.6043 - f1_score: 0.4155 - val_loss: 1.0733 - val_accuracy: 0.4085 - val_auc: 0.5912 - val_f1_score: 0.3836
Epoch 6/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0648 - accuracy: 0.4207 - auc: 0.6059 - f1_score: 0.4136 - val_loss: 1.0732 - val_accuracy: 0.4047 - val_auc: 0.5918 - val_f1_score: 0.3893
Epoch 7/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0633 - accuracy: 0.4269 - auc: 0.6098 - f1_score: 0.4195 - val_loss: 1.0721 - val_accuracy: 0.4112 - val_auc: 0.5932 - val_f1_score: 0.3981
Epoch 8/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0619 - accuracy: 0.4283 - auc: 0.6111 - f1_score: 0.4218 - val_loss: 1.0735 - val_accuracy: 0.4080 - val_auc: 0.5916 - val_f1_score: 0.3838
Epoch 9/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0612 - accuracy: 0.4291 - auc: 0.6125 - f1_score: 0.4220 - val_loss: 1.0733 - val_accuracy: 0.4066 - val_auc: 0.5918 - val_f1_score: 0.3893
Epoch 10/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0597 - accuracy: 0.4301 - auc: 0.6154 - f1_score: 0.4226 - val_loss: 1.0723 - val_accuracy: 0.4083 - val_auc: 0.5931 - val_f1_score: 0.3976
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1602)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          801500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 940,243
Trainable params: 940,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 75ms/step - loss: 1.0829 - accuracy: 0.3919 - auc: 0.5687 - f1_score: 0.3991 - val_loss: 1.0767 - val_accuracy: 0.4020 - val_auc: 0.5835 - val_f1_score: 0.3637
Epoch 2/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0706 - accuracy: 0.4100 - auc: 0.5949 - f1_score: 0.3764 - val_loss: 1.0748 - val_accuracy: 0.4006 - val_auc: 0.5869 - val_f1_score: 0.3757
Epoch 3/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0674 - accuracy: 0.4173 - auc: 0.6008 - f1_score: 0.3931 - val_loss: 1.0743 - val_accuracy: 0.4008 - val_auc: 0.5866 - val_f1_score: 0.3790
Epoch 4/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0650 - accuracy: 0.4207 - auc: 0.6054 - f1_score: 0.3992 - val_loss: 1.0731 - val_accuracy: 0.4096 - val_auc: 0.5905 - val_f1_score: 0.3905
Epoch 5/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0637 - accuracy: 0.4227 - auc: 0.6071 - f1_score: 0.4029 - val_loss: 1.0727 - val_accuracy: 0.4059 - val_auc: 0.5908 - val_f1_score: 0.3876
Epoch 6/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0619 - accuracy: 0.4263 - auc: 0.6108 - f1_score: 0.4071 - val_loss: 1.0724 - val_accuracy: 0.4077 - val_auc: 0.5911 - val_f1_score: 0.3824
Epoch 7/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0606 - accuracy: 0.4267 - auc: 0.6126 - f1_score: 0.4071 - val_loss: 1.0729 - val_accuracy: 0.4079 - val_auc: 0.5910 - val_f1_score: 0.3929
Epoch 8/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0588 - accuracy: 0.4306 - auc: 0.6153 - f1_score: 0.4143 - val_loss: 1.0721 - val_accuracy: 0.4090 - val_auc: 0.5920 - val_f1_score: 0.3886
Epoch 9/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0583 - accuracy: 0.4308 - auc: 0.6163 - f1_score: 0.4130 - val_loss: 1.0735 - val_accuracy: 0.4064 - val_auc: 0.5914 - val_f1_score: 0.3907
Epoch 10/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0572 - accuracy: 0.4310 - auc: 0.6173 - f1_score: 0.4141 - val_loss: 1.0733 - val_accuracy: 0.4071 - val_auc: 0.5905 - val_f1_score: 0.3855
Epoch 11/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0555 - accuracy: 0.4330 - auc: 0.6200 - f1_score: 0.4169 - val_loss: 1.0730 - val_accuracy: 0.4055 - val_auc: 0.5907 - val_f1_score: 0.3888
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1802)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          901500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,040,243
Trainable params: 1,040,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 15s 70ms/step - loss: 1.0839 - accuracy: 0.3898 - auc: 0.5687 - f1_score: 0.4083 - val_loss: 1.0781 - val_accuracy: 0.3975 - val_auc: 0.5816 - val_f1_score: 0.3835
Epoch 2/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0718 - accuracy: 0.4106 - auc: 0.5943 - f1_score: 0.4035 - val_loss: 1.0771 - val_accuracy: 0.3964 - val_auc: 0.5823 - val_f1_score: 0.3853
Epoch 3/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0680 - accuracy: 0.4185 - auc: 0.6007 - f1_score: 0.4144 - val_loss: 1.0737 - val_accuracy: 0.4088 - val_auc: 0.5898 - val_f1_score: 0.4009
Epoch 4/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0645 - accuracy: 0.4221 - auc: 0.6068 - f1_score: 0.4174 - val_loss: 1.0746 - val_accuracy: 0.4074 - val_auc: 0.5888 - val_f1_score: 0.3994
Epoch 5/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0624 - accuracy: 0.4254 - auc: 0.6102 - f1_score: 0.4216 - val_loss: 1.0752 - val_accuracy: 0.4023 - val_auc: 0.5891 - val_f1_score: 0.3909
Epoch 6/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0607 - accuracy: 0.4285 - auc: 0.6130 - f1_score: 0.4230 - val_loss: 1.0734 - val_accuracy: 0.4031 - val_auc: 0.5902 - val_f1_score: 0.4005
Epoch 7/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0589 - accuracy: 0.4298 - auc: 0.6156 - f1_score: 0.4261 - val_loss: 1.0750 - val_accuracy: 0.4013 - val_auc: 0.5893 - val_f1_score: 0.3942
Epoch 8/50
188/188 [==============================] - 15s 79ms/step - loss: 1.0576 - accuracy: 0.4314 - auc: 0.6178 - f1_score: 0.4267 - val_loss: 1.0732 - val_accuracy: 0.4098 - val_auc: 0.5911 - val_f1_score: 0.4048
Epoch 9/50
188/188 [==============================] - 15s 80ms/step - loss: 1.0550 - accuracy: 0.4353 - auc: 0.6212 - f1_score: 0.4306 - val_loss: 1.0728 - val_accuracy: 0.4055 - val_auc: 0.5919 - val_f1_score: 0.4003
Epoch 10/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0540 - accuracy: 0.4375 - auc: 0.6229 - f1_score: 0.4334 - val_loss: 1.0747 - val_accuracy: 0.4080 - val_auc: 0.5910 - val_f1_score: 0.3997
Epoch 11/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0527 - accuracy: 0.4390 - auc: 0.6249 - f1_score: 0.4351 - val_loss: 1.0745 - val_accuracy: 0.4076 - val_auc: 0.5926 - val_f1_score: 0.4032
Epoch 12/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0517 - accuracy: 0.4418 - auc: 0.6270 - f1_score: 0.4382 - val_loss: 1.0748 - val_accuracy: 0.4017 - val_auc: 0.5904 - val_f1_score: 0.3935
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1001500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,140,243
Trainable params: 1,140,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 19s 88ms/step - loss: 1.0872 - accuracy: 0.3731 - auc: 0.5557 - f1_score: 0.4170 - val_loss: 1.0811 - val_accuracy: 0.3966 - val_auc: 0.5763 - val_f1_score: 0.3592
Epoch 2/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0722 - accuracy: 0.4113 - auc: 0.5923 - f1_score: 0.3883 - val_loss: 1.0761 - val_accuracy: 0.4076 - val_auc: 0.5882 - val_f1_score: 0.3486
Epoch 3/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0671 - accuracy: 0.4136 - auc: 0.5998 - f1_score: 0.3723 - val_loss: 1.0742 - val_accuracy: 0.4080 - val_auc: 0.5910 - val_f1_score: 0.3498
Epoch 4/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0649 - accuracy: 0.4174 - auc: 0.6045 - f1_score: 0.3793 - val_loss: 1.0740 - val_accuracy: 0.4032 - val_auc: 0.5919 - val_f1_score: 0.3504
Epoch 5/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0622 - accuracy: 0.4216 - auc: 0.6089 - f1_score: 0.3832 - val_loss: 1.0738 - val_accuracy: 0.4109 - val_auc: 0.5948 - val_f1_score: 0.3689
Epoch 6/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0595 - accuracy: 0.4275 - auc: 0.6137 - f1_score: 0.4006 - val_loss: 1.0723 - val_accuracy: 0.4072 - val_auc: 0.5957 - val_f1_score: 0.3678
Epoch 7/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0590 - accuracy: 0.4268 - auc: 0.6143 - f1_score: 0.3996 - val_loss: 1.0714 - val_accuracy: 0.4088 - val_auc: 0.5959 - val_f1_score: 0.3769
Epoch 8/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0561 - accuracy: 0.4311 - auc: 0.6184 - f1_score: 0.4095 - val_loss: 1.0719 - val_accuracy: 0.4082 - val_auc: 0.5939 - val_f1_score: 0.3819
Epoch 9/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0552 - accuracy: 0.4337 - auc: 0.6211 - f1_score: 0.4136 - val_loss: 1.0735 - val_accuracy: 0.4086 - val_auc: 0.5947 - val_f1_score: 0.3786
Epoch 10/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0536 - accuracy: 0.4341 - auc: 0.6222 - f1_score: 0.4153 - val_loss: 1.0737 - val_accuracy: 0.4096 - val_auc: 0.5948 - val_f1_score: 0.3883
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 205)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          103000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 303,543
Trainable params: 303,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 7s 29ms/step - loss: 1.0732 - accuracy: 0.3960 - auc: 0.5901 - f1_score: 0.4172 - val_loss: 1.0606 - val_accuracy: 0.4301 - val_auc: 0.6145 - val_f1_score: 0.4004
Epoch 2/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0570 - accuracy: 0.4270 - auc: 0.6163 - f1_score: 0.4090 - val_loss: 1.0551 - val_accuracy: 0.4321 - val_auc: 0.6206 - val_f1_score: 0.4124
Epoch 3/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0541 - accuracy: 0.4294 - auc: 0.6187 - f1_score: 0.4154 - val_loss: 1.0548 - val_accuracy: 0.4321 - val_auc: 0.6216 - val_f1_score: 0.4162
Epoch 4/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0532 - accuracy: 0.4328 - auc: 0.6206 - f1_score: 0.4183 - val_loss: 1.0554 - val_accuracy: 0.4323 - val_auc: 0.6204 - val_f1_score: 0.4223
Epoch 5/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0524 - accuracy: 0.4334 - auc: 0.6213 - f1_score: 0.4224 - val_loss: 1.0558 - val_accuracy: 0.4342 - val_auc: 0.6200 - val_f1_score: 0.4170
Epoch 6/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0521 - accuracy: 0.4332 - auc: 0.6220 - f1_score: 0.4224 - val_loss: 1.0552 - val_accuracy: 0.4369 - val_auc: 0.6213 - val_f1_score: 0.4259
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 405)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          203000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 403,543
Trainable params: 403,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 42ms/step - loss: 1.0682 - accuracy: 0.4160 - auc: 0.6004 - f1_score: 0.4152 - val_loss: 1.0556 - val_accuracy: 0.4318 - val_auc: 0.6191 - val_f1_score: 0.4135
Epoch 2/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0530 - accuracy: 0.4339 - auc: 0.6214 - f1_score: 0.4174 - val_loss: 1.0544 - val_accuracy: 0.4347 - val_auc: 0.6213 - val_f1_score: 0.4231
Epoch 3/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0512 - accuracy: 0.4345 - auc: 0.6238 - f1_score: 0.4226 - val_loss: 1.0545 - val_accuracy: 0.4353 - val_auc: 0.6216 - val_f1_score: 0.4188
Epoch 4/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0501 - accuracy: 0.4357 - auc: 0.6255 - f1_score: 0.4251 - val_loss: 1.0543 - val_accuracy: 0.4366 - val_auc: 0.6223 - val_f1_score: 0.4222
Epoch 5/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0494 - accuracy: 0.4382 - auc: 0.6265 - f1_score: 0.4282 - val_loss: 1.0546 - val_accuracy: 0.4345 - val_auc: 0.6214 - val_f1_score: 0.4274
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 605)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          303000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 503,543
Trainable params: 503,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 57ms/step - loss: 1.0683 - accuracy: 0.4180 - auc: 0.5983 - f1_score: 0.4235 - val_loss: 1.0573 - val_accuracy: 0.4267 - val_auc: 0.6157 - val_f1_score: 0.3630
Epoch 2/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0532 - accuracy: 0.4330 - auc: 0.6210 - f1_score: 0.3931 - val_loss: 1.0551 - val_accuracy: 0.4264 - val_auc: 0.6194 - val_f1_score: 0.3823
Epoch 3/50
188/188 [==============================] - 10s 56ms/step - loss: 1.0511 - accuracy: 0.4352 - auc: 0.6239 - f1_score: 0.4076 - val_loss: 1.0553 - val_accuracy: 0.4240 - val_auc: 0.6197 - val_f1_score: 0.3904
Epoch 4/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0495 - accuracy: 0.4375 - auc: 0.6258 - f1_score: 0.4174 - val_loss: 1.0541 - val_accuracy: 0.4287 - val_auc: 0.6214 - val_f1_score: 0.4120
Epoch 5/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0491 - accuracy: 0.4370 - auc: 0.6266 - f1_score: 0.4237 - val_loss: 1.0537 - val_accuracy: 0.4312 - val_auc: 0.6217 - val_f1_score: 0.4137
Epoch 6/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0482 - accuracy: 0.4383 - auc: 0.6278 - f1_score: 0.4272 - val_loss: 1.0536 - val_accuracy: 0.4287 - val_auc: 0.6223 - val_f1_score: 0.4138
Epoch 7/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0486 - accuracy: 0.4393 - auc: 0.6274 - f1_score: 0.4288 - val_loss: 1.0533 - val_accuracy: 0.4302 - val_auc: 0.6224 - val_f1_score: 0.4206
Epoch 8/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0480 - accuracy: 0.4390 - auc: 0.6284 - f1_score: 0.4288 - val_loss: 1.0537 - val_accuracy: 0.4294 - val_auc: 0.6221 - val_f1_score: 0.4159
Epoch 9/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0477 - accuracy: 0.4399 - auc: 0.6290 - f1_score: 0.4306 - val_loss: 1.0534 - val_accuracy: 0.4331 - val_auc: 0.6227 - val_f1_score: 0.4251
Epoch 10/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0469 - accuracy: 0.4405 - auc: 0.6297 - f1_score: 0.4300 - val_loss: 1.0541 - val_accuracy: 0.4323 - val_auc: 0.6218 - val_f1_score: 0.4190
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 805)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          403000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 603,543
Trainable params: 603,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 18s 77ms/step - loss: 1.0660 - accuracy: 0.4241 - auc: 0.6029 - f1_score: 0.4111 - val_loss: 1.0545 - val_accuracy: 0.4290 - val_auc: 0.6207 - val_f1_score: 0.4033
Epoch 2/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0502 - accuracy: 0.4370 - auc: 0.6252 - f1_score: 0.4267 - val_loss: 1.0527 - val_accuracy: 0.4318 - val_auc: 0.6241 - val_f1_score: 0.4216
Epoch 3/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0486 - accuracy: 0.4388 - auc: 0.6274 - f1_score: 0.4313 - val_loss: 1.0529 - val_accuracy: 0.4338 - val_auc: 0.6238 - val_f1_score: 0.4285
Epoch 4/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0473 - accuracy: 0.4412 - auc: 0.6292 - f1_score: 0.4361 - val_loss: 1.0529 - val_accuracy: 0.4354 - val_auc: 0.6238 - val_f1_score: 0.4314
Epoch 5/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0473 - accuracy: 0.4417 - auc: 0.6297 - f1_score: 0.4363 - val_loss: 1.0531 - val_accuracy: 0.4371 - val_auc: 0.6236 - val_f1_score: 0.4319
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          503000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 703,543
Trainable params: 703,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 77ms/step - loss: 1.0911 - accuracy: 0.3348 - auc: 0.5557 - f1_score: 0.3933 - val_loss: 1.0799 - val_accuracy: 0.3435 - val_auc: 0.5789 - val_f1_score: 0.2291
Epoch 2/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0685 - accuracy: 0.3883 - auc: 0.5967 - f1_score: 0.3874 - val_loss: 1.0608 - val_accuracy: 0.4255 - val_auc: 0.6098 - val_f1_score: 0.3471
Epoch 3/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0558 - accuracy: 0.4290 - auc: 0.6165 - f1_score: 0.3812 - val_loss: 1.0563 - val_accuracy: 0.4281 - val_auc: 0.6154 - val_f1_score: 0.3586
Epoch 4/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0518 - accuracy: 0.4340 - auc: 0.6226 - f1_score: 0.3946 - val_loss: 1.0544 - val_accuracy: 0.4290 - val_auc: 0.6193 - val_f1_score: 0.3772
Epoch 5/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0492 - accuracy: 0.4404 - auc: 0.6267 - f1_score: 0.4109 - val_loss: 1.0534 - val_accuracy: 0.4273 - val_auc: 0.6217 - val_f1_score: 0.4032
Epoch 6/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0482 - accuracy: 0.4397 - auc: 0.6279 - f1_score: 0.4195 - val_loss: 1.0534 - val_accuracy: 0.4305 - val_auc: 0.6222 - val_f1_score: 0.4025
Epoch 7/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0468 - accuracy: 0.4414 - auc: 0.6300 - f1_score: 0.4259 - val_loss: 1.0529 - val_accuracy: 0.4315 - val_auc: 0.6227 - val_f1_score: 0.4156
Epoch 8/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0464 - accuracy: 0.4425 - auc: 0.6305 - f1_score: 0.4272 - val_loss: 1.0526 - val_accuracy: 0.4306 - val_auc: 0.6231 - val_f1_score: 0.4123
Epoch 9/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0454 - accuracy: 0.4432 - auc: 0.6321 - f1_score: 0.4302 - val_loss: 1.0528 - val_accuracy: 0.4302 - val_auc: 0.6227 - val_f1_score: 0.4091
Epoch 10/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0451 - accuracy: 0.4427 - auc: 0.6326 - f1_score: 0.4296 - val_loss: 1.0527 - val_accuracy: 0.4302 - val_auc: 0.6231 - val_f1_score: 0.4194
Epoch 11/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0447 - accuracy: 0.4446 - auc: 0.6332 - f1_score: 0.4317 - val_loss: 1.0528 - val_accuracy: 0.4311 - val_auc: 0.6227 - val_f1_score: 0.4217
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1205)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          603000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 803,543
Trainable params: 803,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 23s 103ms/step - loss: 1.0684 - accuracy: 0.4136 - auc: 0.5985 - f1_score: 0.4165 - val_loss: 1.0543 - val_accuracy: 0.4276 - val_auc: 0.6196 - val_f1_score: 0.3955
Epoch 2/50
188/188 [==============================] - 24s 127ms/step - loss: 1.0500 - accuracy: 0.4354 - auc: 0.6245 - f1_score: 0.4167 - val_loss: 1.0537 - val_accuracy: 0.4341 - val_auc: 0.6218 - val_f1_score: 0.4178
Epoch 3/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0485 - accuracy: 0.4386 - auc: 0.6276 - f1_score: 0.4293 - val_loss: 1.0526 - val_accuracy: 0.4325 - val_auc: 0.6237 - val_f1_score: 0.4205
Epoch 4/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0469 - accuracy: 0.4395 - auc: 0.6294 - f1_score: 0.4326 - val_loss: 1.0527 - val_accuracy: 0.4311 - val_auc: 0.6237 - val_f1_score: 0.4222
Epoch 5/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0462 - accuracy: 0.4429 - auc: 0.6313 - f1_score: 0.4354 - val_loss: 1.0531 - val_accuracy: 0.4326 - val_auc: 0.6233 - val_f1_score: 0.4217
Epoch 6/50
188/188 [==============================] - 20s 108ms/step - loss: 1.0463 - accuracy: 0.4418 - auc: 0.6314 - f1_score: 0.4354 - val_loss: 1.0525 - val_accuracy: 0.4336 - val_auc: 0.6237 - val_f1_score: 0.4267
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1405)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          703000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 903,543
Trainable params: 903,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 26s 116ms/step - loss: 1.0711 - accuracy: 0.4197 - auc: 0.5981 - f1_score: 0.4209 - val_loss: 1.0562 - val_accuracy: 0.4279 - val_auc: 0.6195 - val_f1_score: 0.4007
Epoch 2/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0498 - accuracy: 0.4370 - auc: 0.6264 - f1_score: 0.4224 - val_loss: 1.0534 - val_accuracy: 0.4272 - val_auc: 0.6212 - val_f1_score: 0.4113
Epoch 3/50
188/188 [==============================] - 20s 107ms/step - loss: 1.0469 - accuracy: 0.4435 - auc: 0.6302 - f1_score: 0.4343 - val_loss: 1.0529 - val_accuracy: 0.4313 - val_auc: 0.6227 - val_f1_score: 0.4192
Epoch 4/50
188/188 [==============================] - 21s 109ms/step - loss: 1.0450 - accuracy: 0.4430 - auc: 0.6324 - f1_score: 0.4352 - val_loss: 1.0528 - val_accuracy: 0.4324 - val_auc: 0.6227 - val_f1_score: 0.4288
Epoch 5/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0443 - accuracy: 0.4456 - auc: 0.6340 - f1_score: 0.4381 - val_loss: 1.0529 - val_accuracy: 0.4271 - val_auc: 0.6222 - val_f1_score: 0.4182
Epoch 6/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0434 - accuracy: 0.4463 - auc: 0.6350 - f1_score: 0.4388 - val_loss: 1.0529 - val_accuracy: 0.4320 - val_auc: 0.6229 - val_f1_score: 0.4204
Epoch 7/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0426 - accuracy: 0.4497 - auc: 0.6366 - f1_score: 0.4429 - val_loss: 1.0536 - val_accuracy: 0.4311 - val_auc: 0.6219 - val_f1_score: 0.4188
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1605)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          803000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,003,543
Trainable params: 1,003,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 27s 126ms/step - loss: 1.0794 - accuracy: 0.4086 - auc: 0.5825 - f1_score: 0.4181 - val_loss: 1.0646 - val_accuracy: 0.4213 - val_auc: 0.6047 - val_f1_score: 0.3375
Epoch 2/50
188/188 [==============================] - 23s 122ms/step - loss: 1.0574 - accuracy: 0.4354 - auc: 0.6169 - f1_score: 0.3979 - val_loss: 1.0579 - val_accuracy: 0.4306 - val_auc: 0.6157 - val_f1_score: 0.4180
Epoch 3/50
188/188 [==============================] - 23s 121ms/step - loss: 1.0494 - accuracy: 0.4407 - auc: 0.6272 - f1_score: 0.4361 - val_loss: 1.0534 - val_accuracy: 0.4295 - val_auc: 0.6218 - val_f1_score: 0.4192
Epoch 4/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0463 - accuracy: 0.4428 - auc: 0.6309 - f1_score: 0.4342 - val_loss: 1.0529 - val_accuracy: 0.4314 - val_auc: 0.6225 - val_f1_score: 0.4167
Epoch 5/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0452 - accuracy: 0.4434 - auc: 0.6326 - f1_score: 0.4324 - val_loss: 1.0527 - val_accuracy: 0.4312 - val_auc: 0.6221 - val_f1_score: 0.4205
Epoch 6/50
188/188 [==============================] - 23s 123ms/step - loss: 1.0438 - accuracy: 0.4476 - auc: 0.6351 - f1_score: 0.4401 - val_loss: 1.0533 - val_accuracy: 0.4287 - val_auc: 0.6217 - val_f1_score: 0.4218
Epoch 7/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0433 - accuracy: 0.4468 - auc: 0.6350 - f1_score: 0.4398 - val_loss: 1.0536 - val_accuracy: 0.4307 - val_auc: 0.6214 - val_f1_score: 0.4203
Epoch 8/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0423 - accuracy: 0.4503 - auc: 0.6372 - f1_score: 0.4423 - val_loss: 1.0538 - val_accuracy: 0.4291 - val_auc: 0.6214 - val_f1_score: 0.4190
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1805)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          903000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,103,543
Trainable params: 1,103,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 32s 147ms/step - loss: 1.0748 - accuracy: 0.4243 - auc: 0.5895 - f1_score: 0.4266 - val_loss: 1.0609 - val_accuracy: 0.4319 - val_auc: 0.6132 - val_f1_score: 0.4286
Epoch 2/50
188/188 [==============================] - 26s 140ms/step - loss: 1.0546 - accuracy: 0.4377 - auc: 0.6215 - f1_score: 0.4345 - val_loss: 1.0540 - val_accuracy: 0.4319 - val_auc: 0.6217 - val_f1_score: 0.4264
Epoch 3/50
188/188 [==============================] - 26s 140ms/step - loss: 1.0492 - accuracy: 0.4409 - auc: 0.6285 - f1_score: 0.4359 - val_loss: 1.0528 - val_accuracy: 0.4307 - val_auc: 0.6227 - val_f1_score: 0.4285
Epoch 4/50
188/188 [==============================] - 26s 141ms/step - loss: 1.0460 - accuracy: 0.4432 - auc: 0.6319 - f1_score: 0.4372 - val_loss: 1.0527 - val_accuracy: 0.4341 - val_auc: 0.6228 - val_f1_score: 0.4311
Epoch 5/50
188/188 [==============================] - 27s 141ms/step - loss: 1.0442 - accuracy: 0.4453 - auc: 0.6342 - f1_score: 0.4398 - val_loss: 1.0532 - val_accuracy: 0.4303 - val_auc: 0.6226 - val_f1_score: 0.4245
Epoch 6/50
188/188 [==============================] - 27s 142ms/step - loss: 1.0435 - accuracy: 0.4485 - auc: 0.6359 - f1_score: 0.4436 - val_loss: 1.0530 - val_accuracy: 0.4273 - val_auc: 0.6220 - val_f1_score: 0.4222
Epoch 7/50
188/188 [==============================] - 27s 144ms/step - loss: 1.0422 - accuracy: 0.4498 - auc: 0.6376 - f1_score: 0.4457 - val_loss: 1.0529 - val_accuracy: 0.4297 - val_auc: 0.6229 - val_f1_score: 0.4200
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1003000     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,203,543
Trainable params: 1,203,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 32s 152ms/step - loss: 1.0746 - accuracy: 0.4000 - auc: 0.5892 - f1_score: 0.4287 - val_loss: 1.0554 - val_accuracy: 0.4293 - val_auc: 0.6196 - val_f1_score: 0.4068
Epoch 2/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0501 - accuracy: 0.4367 - auc: 0.6252 - f1_score: 0.4250 - val_loss: 1.0524 - val_accuracy: 0.4297 - val_auc: 0.6225 - val_f1_score: 0.4023
Epoch 3/50
188/188 [==============================] - 30s 159ms/step - loss: 1.0462 - accuracy: 0.4416 - auc: 0.6308 - f1_score: 0.4328 - val_loss: 1.0520 - val_accuracy: 0.4357 - val_auc: 0.6239 - val_f1_score: 0.4215
Epoch 4/50
188/188 [==============================] - 29s 155ms/step - loss: 1.0446 - accuracy: 0.4460 - auc: 0.6336 - f1_score: 0.4391 - val_loss: 1.0518 - val_accuracy: 0.4351 - val_auc: 0.6237 - val_f1_score: 0.4253
Epoch 5/50
188/188 [==============================] - 30s 157ms/step - loss: 1.0440 - accuracy: 0.4455 - auc: 0.6348 - f1_score: 0.4405 - val_loss: 1.0525 - val_accuracy: 0.4324 - val_auc: 0.6231 - val_f1_score: 0.4169
Epoch 6/50
188/188 [==============================] - 29s 156ms/step - loss: 1.0415 - accuracy: 0.4522 - auc: 0.6384 - f1_score: 0.4476 - val_loss: 1.0530 - val_accuracy: 0.4332 - val_auc: 0.6223 - val_f1_score: 0.4264
Epoch 7/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0407 - accuracy: 0.4514 - auc: 0.6397 - f1_score: 0.4469 - val_loss: 1.0532 - val_accuracy: 0.4321 - val_auc: 0.6222 - val_f1_score: 0.4201
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 205)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          103000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 241,743
Trainable params: 241,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 7s 25ms/step - loss: 1.0756 - accuracy: 0.3931 - auc: 0.5823 - f1_score: 0.4263 - val_loss: 1.0568 - val_accuracy: 0.4297 - val_auc: 0.6196 - val_f1_score: 0.3977
Epoch 2/50
188/188 [==============================] - 4s 22ms/step - loss: 1.0577 - accuracy: 0.4264 - auc: 0.6139 - f1_score: 0.4154 - val_loss: 1.0560 - val_accuracy: 0.4293 - val_auc: 0.6188 - val_f1_score: 0.4027
Epoch 3/50
188/188 [==============================] - 4s 22ms/step - loss: 1.0550 - accuracy: 0.4299 - auc: 0.6180 - f1_score: 0.4177 - val_loss: 1.0556 - val_accuracy: 0.4281 - val_auc: 0.6205 - val_f1_score: 0.4188
Epoch 4/50
188/188 [==============================] - 4s 22ms/step - loss: 1.0542 - accuracy: 0.4301 - auc: 0.6186 - f1_score: 0.4213 - val_loss: 1.0551 - val_accuracy: 0.4335 - val_auc: 0.6217 - val_f1_score: 0.4158
Epoch 5/50
188/188 [==============================] - 4s 22ms/step - loss: 1.0538 - accuracy: 0.4305 - auc: 0.6193 - f1_score: 0.4206 - val_loss: 1.0554 - val_accuracy: 0.4326 - val_auc: 0.6218 - val_f1_score: 0.4200
Epoch 6/50
188/188 [==============================] - 4s 22ms/step - loss: 1.0531 - accuracy: 0.4315 - auc: 0.6207 - f1_score: 0.4196 - val_loss: 1.0555 - val_accuracy: 0.4335 - val_auc: 0.6212 - val_f1_score: 0.4123
Epoch 7/50
188/188 [==============================] - 4s 22ms/step - loss: 1.0529 - accuracy: 0.4320 - auc: 0.6212 - f1_score: 0.4216 - val_loss: 1.0553 - val_accuracy: 0.4345 - val_auc: 0.6216 - val_f1_score: 0.4150
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 405)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          203000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 341,743
Trainable params: 341,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 8s 32ms/step - loss: 1.0927 - accuracy: 0.3612 - auc: 0.5348 - f1_score: 0.4052 - val_loss: 1.0833 - val_accuracy: 0.4008 - val_auc: 0.5690 - val_f1_score: 0.3984
Epoch 2/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0718 - accuracy: 0.4125 - auc: 0.5925 - f1_score: 0.4133 - val_loss: 1.0608 - val_accuracy: 0.4267 - val_auc: 0.6121 - val_f1_score: 0.4211
Epoch 3/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0576 - accuracy: 0.4259 - auc: 0.6142 - f1_score: 0.4197 - val_loss: 1.0568 - val_accuracy: 0.4279 - val_auc: 0.6163 - val_f1_score: 0.4191
Epoch 4/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0543 - accuracy: 0.4313 - auc: 0.6189 - f1_score: 0.4239 - val_loss: 1.0574 - val_accuracy: 0.4264 - val_auc: 0.6153 - val_f1_score: 0.4138
Epoch 5/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0523 - accuracy: 0.4321 - auc: 0.6221 - f1_score: 0.4245 - val_loss: 1.0552 - val_accuracy: 0.4283 - val_auc: 0.6185 - val_f1_score: 0.4130
Epoch 6/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0513 - accuracy: 0.4324 - auc: 0.6227 - f1_score: 0.4227 - val_loss: 1.0566 - val_accuracy: 0.4279 - val_auc: 0.6164 - val_f1_score: 0.4178
Epoch 7/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0507 - accuracy: 0.4354 - auc: 0.6243 - f1_score: 0.4271 - val_loss: 1.0565 - val_accuracy: 0.4276 - val_auc: 0.6179 - val_f1_score: 0.4162
Epoch 8/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0503 - accuracy: 0.4371 - auc: 0.6252 - f1_score: 0.4271 - val_loss: 1.0554 - val_accuracy: 0.4302 - val_auc: 0.6187 - val_f1_score: 0.4207
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 605)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          303000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 441,743
Trainable params: 441,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 9s 39ms/step - loss: 1.0874 - accuracy: 0.3856 - auc: 0.5553 - f1_score: 0.3988 - val_loss: 1.0739 - val_accuracy: 0.4133 - val_auc: 0.5898 - val_f1_score: 0.3943
Epoch 2/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0684 - accuracy: 0.4209 - auc: 0.5986 - f1_score: 0.4143 - val_loss: 1.0618 - val_accuracy: 0.4183 - val_auc: 0.6108 - val_f1_score: 0.4189
Epoch 3/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0597 - accuracy: 0.4285 - auc: 0.6122 - f1_score: 0.4267 - val_loss: 1.0587 - val_accuracy: 0.4271 - val_auc: 0.6153 - val_f1_score: 0.4240
Epoch 4/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0553 - accuracy: 0.4313 - auc: 0.6187 - f1_score: 0.4294 - val_loss: 1.0568 - val_accuracy: 0.4297 - val_auc: 0.6165 - val_f1_score: 0.4292
Epoch 5/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0521 - accuracy: 0.4331 - auc: 0.6226 - f1_score: 0.4317 - val_loss: 1.0554 - val_accuracy: 0.4282 - val_auc: 0.6187 - val_f1_score: 0.4255
Epoch 6/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0513 - accuracy: 0.4348 - auc: 0.6239 - f1_score: 0.4317 - val_loss: 1.0552 - val_accuracy: 0.4277 - val_auc: 0.6191 - val_f1_score: 0.4253
Epoch 7/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0495 - accuracy: 0.4386 - auc: 0.6263 - f1_score: 0.4344 - val_loss: 1.0556 - val_accuracy: 0.4277 - val_auc: 0.6186 - val_f1_score: 0.4235
Epoch 8/50
188/188 [==============================] - 6s 35ms/step - loss: 1.0493 - accuracy: 0.4389 - auc: 0.6268 - f1_score: 0.4348 - val_loss: 1.0548 - val_accuracy: 0.4295 - val_auc: 0.6192 - val_f1_score: 0.4289
Epoch 9/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0478 - accuracy: 0.4415 - auc: 0.6292 - f1_score: 0.4383 - val_loss: 1.0542 - val_accuracy: 0.4327 - val_auc: 0.6209 - val_f1_score: 0.4278
Epoch 10/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0466 - accuracy: 0.4432 - auc: 0.6310 - f1_score: 0.4379 - val_loss: 1.0547 - val_accuracy: 0.4291 - val_auc: 0.6200 - val_f1_score: 0.4269
Epoch 11/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0464 - accuracy: 0.4410 - auc: 0.6307 - f1_score: 0.4368 - val_loss: 1.0553 - val_accuracy: 0.4313 - val_auc: 0.6189 - val_f1_score: 0.4261
Epoch 12/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0466 - accuracy: 0.4421 - auc: 0.6309 - f1_score: 0.4363 - val_loss: 1.0554 - val_accuracy: 0.4271 - val_auc: 0.6188 - val_f1_score: 0.4242
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 805)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          403000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 541,743
Trainable params: 541,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 51ms/step - loss: 1.0804 - accuracy: 0.3906 - auc: 0.5737 - f1_score: 0.4182 - val_loss: 1.0682 - val_accuracy: 0.4169 - val_auc: 0.5997 - val_f1_score: 0.3672
Epoch 2/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0622 - accuracy: 0.4203 - auc: 0.6066 - f1_score: 0.3861 - val_loss: 1.0582 - val_accuracy: 0.4225 - val_auc: 0.6128 - val_f1_score: 0.3629
Epoch 3/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0549 - accuracy: 0.4283 - auc: 0.6175 - f1_score: 0.3954 - val_loss: 1.0582 - val_accuracy: 0.4146 - val_auc: 0.6118 - val_f1_score: 0.3735
Epoch 4/50
188/188 [==============================] - 9s 45ms/step - loss: 1.0517 - accuracy: 0.4328 - auc: 0.6216 - f1_score: 0.4040 - val_loss: 1.0563 - val_accuracy: 0.4246 - val_auc: 0.6158 - val_f1_score: 0.3950
Epoch 5/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0495 - accuracy: 0.4346 - auc: 0.6251 - f1_score: 0.4118 - val_loss: 1.0554 - val_accuracy: 0.4203 - val_auc: 0.6167 - val_f1_score: 0.3821
Epoch 6/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0482 - accuracy: 0.4373 - auc: 0.6267 - f1_score: 0.4152 - val_loss: 1.0557 - val_accuracy: 0.4186 - val_auc: 0.6165 - val_f1_score: 0.3836
Epoch 7/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0478 - accuracy: 0.4393 - auc: 0.6277 - f1_score: 0.4187 - val_loss: 1.0553 - val_accuracy: 0.4189 - val_auc: 0.6169 - val_f1_score: 0.3842
Epoch 8/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0472 - accuracy: 0.4379 - auc: 0.6287 - f1_score: 0.4191 - val_loss: 1.0548 - val_accuracy: 0.4277 - val_auc: 0.6201 - val_f1_score: 0.4136
Epoch 9/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0462 - accuracy: 0.4389 - auc: 0.6297 - f1_score: 0.4251 - val_loss: 1.0562 - val_accuracy: 0.4239 - val_auc: 0.6169 - val_f1_score: 0.3965
Epoch 10/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0458 - accuracy: 0.4386 - auc: 0.6304 - f1_score: 0.4227 - val_loss: 1.0559 - val_accuracy: 0.4237 - val_auc: 0.6179 - val_f1_score: 0.4064
Epoch 11/50
188/188 [==============================] - 9s 45ms/step - loss: 1.0453 - accuracy: 0.4423 - auc: 0.6317 - f1_score: 0.4286 - val_loss: 1.0562 - val_accuracy: 0.4260 - val_auc: 0.6186 - val_f1_score: 0.4065
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          503000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 641,743
Trainable params: 641,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 54ms/step - loss: 1.0838 - accuracy: 0.3866 - auc: 0.5673 - f1_score: 0.4140 - val_loss: 1.0724 - val_accuracy: 0.4086 - val_auc: 0.5923 - val_f1_score: 0.4045
Epoch 2/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0666 - accuracy: 0.4144 - auc: 0.6007 - f1_score: 0.4108 - val_loss: 1.0638 - val_accuracy: 0.4180 - val_auc: 0.6053 - val_f1_score: 0.4133
Epoch 3/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0585 - accuracy: 0.4239 - auc: 0.6124 - f1_score: 0.4220 - val_loss: 1.0597 - val_accuracy: 0.4223 - val_auc: 0.6108 - val_f1_score: 0.4190
Epoch 4/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0538 - accuracy: 0.4310 - auc: 0.6200 - f1_score: 0.4290 - val_loss: 1.0581 - val_accuracy: 0.4249 - val_auc: 0.6132 - val_f1_score: 0.4148
Epoch 5/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0519 - accuracy: 0.4320 - auc: 0.6228 - f1_score: 0.4291 - val_loss: 1.0564 - val_accuracy: 0.4331 - val_auc: 0.6166 - val_f1_score: 0.4291
Epoch 6/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0493 - accuracy: 0.4370 - auc: 0.6265 - f1_score: 0.4335 - val_loss: 1.0565 - val_accuracy: 0.4327 - val_auc: 0.6170 - val_f1_score: 0.4278
Epoch 7/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0467 - accuracy: 0.4415 - auc: 0.6297 - f1_score: 0.4378 - val_loss: 1.0557 - val_accuracy: 0.4305 - val_auc: 0.6176 - val_f1_score: 0.4260
Epoch 8/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0460 - accuracy: 0.4422 - auc: 0.6307 - f1_score: 0.4371 - val_loss: 1.0558 - val_accuracy: 0.4296 - val_auc: 0.6169 - val_f1_score: 0.4245
Epoch 9/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0453 - accuracy: 0.4441 - auc: 0.6322 - f1_score: 0.4399 - val_loss: 1.0569 - val_accuracy: 0.4308 - val_auc: 0.6164 - val_f1_score: 0.4246
Epoch 10/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0445 - accuracy: 0.4434 - auc: 0.6326 - f1_score: 0.4407 - val_loss: 1.0559 - val_accuracy: 0.4317 - val_auc: 0.6174 - val_f1_score: 0.4274
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1205)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          603000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 741,743
Trainable params: 741,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 60ms/step - loss: 1.0825 - accuracy: 0.3941 - auc: 0.5711 - f1_score: 0.4161 - val_loss: 1.0705 - val_accuracy: 0.4135 - val_auc: 0.5943 - val_f1_score: 0.3947
Epoch 2/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0650 - accuracy: 0.4160 - auc: 0.6014 - f1_score: 0.4010 - val_loss: 1.0626 - val_accuracy: 0.4223 - val_auc: 0.6085 - val_f1_score: 0.4144
Epoch 3/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0582 - accuracy: 0.4278 - auc: 0.6130 - f1_score: 0.4175 - val_loss: 1.0586 - val_accuracy: 0.4270 - val_auc: 0.6134 - val_f1_score: 0.4204
Epoch 4/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0530 - accuracy: 0.4336 - auc: 0.6210 - f1_score: 0.4255 - val_loss: 1.0562 - val_accuracy: 0.4272 - val_auc: 0.6167 - val_f1_score: 0.4235
Epoch 5/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0495 - accuracy: 0.4358 - auc: 0.6253 - f1_score: 0.4312 - val_loss: 1.0564 - val_accuracy: 0.4237 - val_auc: 0.6166 - val_f1_score: 0.4191
Epoch 6/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0481 - accuracy: 0.4397 - auc: 0.6277 - f1_score: 0.4344 - val_loss: 1.0558 - val_accuracy: 0.4309 - val_auc: 0.6175 - val_f1_score: 0.4280
Epoch 7/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0463 - accuracy: 0.4405 - auc: 0.6301 - f1_score: 0.4365 - val_loss: 1.0551 - val_accuracy: 0.4293 - val_auc: 0.6186 - val_f1_score: 0.4262
Epoch 8/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0455 - accuracy: 0.4400 - auc: 0.6305 - f1_score: 0.4361 - val_loss: 1.0581 - val_accuracy: 0.4251 - val_auc: 0.6162 - val_f1_score: 0.4225
Epoch 9/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0442 - accuracy: 0.4426 - auc: 0.6323 - f1_score: 0.4387 - val_loss: 1.0572 - val_accuracy: 0.4281 - val_auc: 0.6161 - val_f1_score: 0.4254
Epoch 10/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0430 - accuracy: 0.4447 - auc: 0.6349 - f1_score: 0.4408 - val_loss: 1.0559 - val_accuracy: 0.4249 - val_auc: 0.6176 - val_f1_score: 0.4231
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1405)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          703000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 841,743
Trainable params: 841,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 72ms/step - loss: 1.0801 - accuracy: 0.3980 - auc: 0.5755 - f1_score: 0.4244 - val_loss: 1.0721 - val_accuracy: 0.4054 - val_auc: 0.5896 - val_f1_score: 0.3703
Epoch 2/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0634 - accuracy: 0.4183 - auc: 0.6051 - f1_score: 0.4085 - val_loss: 1.0639 - val_accuracy: 0.4157 - val_auc: 0.6037 - val_f1_score: 0.3984
Epoch 3/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0558 - accuracy: 0.4296 - auc: 0.6170 - f1_score: 0.4205 - val_loss: 1.0589 - val_accuracy: 0.4229 - val_auc: 0.6119 - val_f1_score: 0.4073
Epoch 4/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0519 - accuracy: 0.4352 - auc: 0.6231 - f1_score: 0.4273 - val_loss: 1.0586 - val_accuracy: 0.4255 - val_auc: 0.6130 - val_f1_score: 0.4037
Epoch 5/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0472 - accuracy: 0.4399 - auc: 0.6292 - f1_score: 0.4325 - val_loss: 1.0569 - val_accuracy: 0.4247 - val_auc: 0.6149 - val_f1_score: 0.4113
Epoch 6/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0457 - accuracy: 0.4420 - auc: 0.6312 - f1_score: 0.4357 - val_loss: 1.0578 - val_accuracy: 0.4253 - val_auc: 0.6158 - val_f1_score: 0.4121
Epoch 7/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0438 - accuracy: 0.4457 - auc: 0.6339 - f1_score: 0.4397 - val_loss: 1.0569 - val_accuracy: 0.4282 - val_auc: 0.6158 - val_f1_score: 0.4201
Epoch 8/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0422 - accuracy: 0.4463 - auc: 0.6361 - f1_score: 0.4400 - val_loss: 1.0565 - val_accuracy: 0.4227 - val_auc: 0.6167 - val_f1_score: 0.4058
Epoch 9/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0414 - accuracy: 0.4496 - auc: 0.6375 - f1_score: 0.4437 - val_loss: 1.0573 - val_accuracy: 0.4237 - val_auc: 0.6158 - val_f1_score: 0.4157
Epoch 10/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0394 - accuracy: 0.4509 - auc: 0.6402 - f1_score: 0.4454 - val_loss: 1.0558 - val_accuracy: 0.4288 - val_auc: 0.6176 - val_f1_score: 0.4204
Epoch 11/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0385 - accuracy: 0.4526 - auc: 0.6409 - f1_score: 0.4477 - val_loss: 1.0567 - val_accuracy: 0.4267 - val_auc: 0.6168 - val_f1_score: 0.4217
Epoch 12/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0374 - accuracy: 0.4560 - auc: 0.6428 - f1_score: 0.4508 - val_loss: 1.0579 - val_accuracy: 0.4290 - val_auc: 0.6166 - val_f1_score: 0.4144
Epoch 13/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0367 - accuracy: 0.4540 - auc: 0.6436 - f1_score: 0.4492 - val_loss: 1.0581 - val_accuracy: 0.4241 - val_auc: 0.6159 - val_f1_score: 0.4185
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1605)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          803000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 941,743
Trainable params: 941,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 15s 70ms/step - loss: 1.0843 - accuracy: 0.3914 - auc: 0.5661 - f1_score: 0.4294 - val_loss: 1.0750 - val_accuracy: 0.4090 - val_auc: 0.5876 - val_f1_score: 0.4044
Epoch 2/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0675 - accuracy: 0.4159 - auc: 0.6003 - f1_score: 0.4120 - val_loss: 1.0672 - val_accuracy: 0.4185 - val_auc: 0.6008 - val_f1_score: 0.4115
Epoch 3/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0587 - accuracy: 0.4252 - auc: 0.6130 - f1_score: 0.4209 - val_loss: 1.0636 - val_accuracy: 0.4219 - val_auc: 0.6046 - val_f1_score: 0.4101
Epoch 4/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0534 - accuracy: 0.4333 - auc: 0.6208 - f1_score: 0.4290 - val_loss: 1.0613 - val_accuracy: 0.4259 - val_auc: 0.6094 - val_f1_score: 0.4145
Epoch 5/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0492 - accuracy: 0.4368 - auc: 0.6262 - f1_score: 0.4313 - val_loss: 1.0582 - val_accuracy: 0.4289 - val_auc: 0.6141 - val_f1_score: 0.4227
Epoch 6/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0454 - accuracy: 0.4436 - auc: 0.6319 - f1_score: 0.4392 - val_loss: 1.0570 - val_accuracy: 0.4288 - val_auc: 0.6158 - val_f1_score: 0.4248
Epoch 7/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0435 - accuracy: 0.4450 - auc: 0.6343 - f1_score: 0.4413 - val_loss: 1.0581 - val_accuracy: 0.4277 - val_auc: 0.6141 - val_f1_score: 0.4196
Epoch 8/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0419 - accuracy: 0.4470 - auc: 0.6368 - f1_score: 0.4433 - val_loss: 1.0580 - val_accuracy: 0.4267 - val_auc: 0.6145 - val_f1_score: 0.4244
Epoch 9/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0401 - accuracy: 0.4516 - auc: 0.6386 - f1_score: 0.4484 - val_loss: 1.0582 - val_accuracy: 0.4255 - val_auc: 0.6152 - val_f1_score: 0.4154
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1805)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          903000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,041,743
Trainable params: 1,041,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 78ms/step - loss: 1.0791 - accuracy: 0.3970 - auc: 0.5777 - f1_score: 0.4293 - val_loss: 1.0735 - val_accuracy: 0.4034 - val_auc: 0.5882 - val_f1_score: 0.3773
Epoch 2/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0652 - accuracy: 0.4195 - auc: 0.6033 - f1_score: 0.4130 - val_loss: 1.0655 - val_accuracy: 0.4127 - val_auc: 0.6013 - val_f1_score: 0.3951
Epoch 3/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0575 - accuracy: 0.4292 - auc: 0.6150 - f1_score: 0.4247 - val_loss: 1.0612 - val_accuracy: 0.4251 - val_auc: 0.6092 - val_f1_score: 0.4145
Epoch 4/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0521 - accuracy: 0.4360 - auc: 0.6232 - f1_score: 0.4305 - val_loss: 1.0608 - val_accuracy: 0.4225 - val_auc: 0.6098 - val_f1_score: 0.4088
Epoch 5/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0479 - accuracy: 0.4414 - auc: 0.6285 - f1_score: 0.4358 - val_loss: 1.0586 - val_accuracy: 0.4257 - val_auc: 0.6130 - val_f1_score: 0.4162
Epoch 6/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0452 - accuracy: 0.4444 - auc: 0.6323 - f1_score: 0.4388 - val_loss: 1.0581 - val_accuracy: 0.4229 - val_auc: 0.6139 - val_f1_score: 0.4174
Epoch 7/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0432 - accuracy: 0.4461 - auc: 0.6356 - f1_score: 0.4412 - val_loss: 1.0580 - val_accuracy: 0.4225 - val_auc: 0.6155 - val_f1_score: 0.4159
Epoch 8/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0417 - accuracy: 0.4484 - auc: 0.6373 - f1_score: 0.4440 - val_loss: 1.0586 - val_accuracy: 0.4264 - val_auc: 0.6145 - val_f1_score: 0.4184
Epoch 9/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0391 - accuracy: 0.4516 - auc: 0.6406 - f1_score: 0.4471 - val_loss: 1.0594 - val_accuracy: 0.4285 - val_auc: 0.6142 - val_f1_score: 0.4207
Epoch 10/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0375 - accuracy: 0.4558 - auc: 0.6430 - f1_score: 0.4516 - val_loss: 1.0601 - val_accuracy: 0.4231 - val_auc: 0.6135 - val_f1_score: 0.4137
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1003000     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,141,743
Trainable params: 1,141,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 19s 89ms/step - loss: 1.0897 - accuracy: 0.3610 - auc: 0.5456 - f1_score: 0.4135 - val_loss: 1.0845 - val_accuracy: 0.3896 - val_auc: 0.5666 - val_f1_score: 0.3776
Epoch 2/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0737 - accuracy: 0.4140 - auc: 0.5910 - f1_score: 0.4125 - val_loss: 1.0748 - val_accuracy: 0.4070 - val_auc: 0.5869 - val_f1_score: 0.3895
Epoch 3/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0613 - accuracy: 0.4292 - auc: 0.6118 - f1_score: 0.4213 - val_loss: 1.0677 - val_accuracy: 0.4150 - val_auc: 0.5998 - val_f1_score: 0.4045
Epoch 4/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0527 - accuracy: 0.4373 - auc: 0.6237 - f1_score: 0.4288 - val_loss: 1.0625 - val_accuracy: 0.4191 - val_auc: 0.6067 - val_f1_score: 0.4101
Epoch 5/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0480 - accuracy: 0.4429 - auc: 0.6296 - f1_score: 0.4347 - val_loss: 1.0612 - val_accuracy: 0.4235 - val_auc: 0.6099 - val_f1_score: 0.4208
Epoch 6/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0441 - accuracy: 0.4470 - auc: 0.6355 - f1_score: 0.4400 - val_loss: 1.0602 - val_accuracy: 0.4211 - val_auc: 0.6116 - val_f1_score: 0.4049
Epoch 7/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0415 - accuracy: 0.4490 - auc: 0.6383 - f1_score: 0.4412 - val_loss: 1.0595 - val_accuracy: 0.4187 - val_auc: 0.6107 - val_f1_score: 0.4147
Epoch 8/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0374 - accuracy: 0.4575 - auc: 0.6439 - f1_score: 0.4517 - val_loss: 1.0595 - val_accuracy: 0.4207 - val_auc: 0.6118 - val_f1_score: 0.4134
Epoch 9/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0361 - accuracy: 0.4586 - auc: 0.6457 - f1_score: 0.4528 - val_loss: 1.0602 - val_accuracy: 0.4204 - val_auc: 0.6122 - val_f1_score: 0.4122
Epoch 10/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0334 - accuracy: 0.4615 - auc: 0.6488 - f1_score: 0.4563 - val_loss: 1.0613 - val_accuracy: 0.4181 - val_auc: 0.6106 - val_f1_score: 0.4122
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 208)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          104500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 305,043
Trainable params: 305,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 7s 31ms/step - loss: 1.0698 - accuracy: 0.4186 - auc: 0.5982 - f1_score: 0.4322 - val_loss: 1.0568 - val_accuracy: 0.4333 - val_auc: 0.6181 - val_f1_score: 0.3957
Epoch 2/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0546 - accuracy: 0.4298 - auc: 0.6187 - f1_score: 0.4003 - val_loss: 1.0552 - val_accuracy: 0.4312 - val_auc: 0.6202 - val_f1_score: 0.4059
Epoch 3/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0525 - accuracy: 0.4320 - auc: 0.6209 - f1_score: 0.4082 - val_loss: 1.0549 - val_accuracy: 0.4324 - val_auc: 0.6210 - val_f1_score: 0.4116
Epoch 4/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0521 - accuracy: 0.4322 - auc: 0.6225 - f1_score: 0.4136 - val_loss: 1.0547 - val_accuracy: 0.4335 - val_auc: 0.6217 - val_f1_score: 0.4169
Epoch 5/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0516 - accuracy: 0.4340 - auc: 0.6229 - f1_score: 0.4159 - val_loss: 1.0549 - val_accuracy: 0.4349 - val_auc: 0.6215 - val_f1_score: 0.4209
Epoch 6/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0514 - accuracy: 0.4332 - auc: 0.6231 - f1_score: 0.4184 - val_loss: 1.0549 - val_accuracy: 0.4353 - val_auc: 0.6217 - val_f1_score: 0.4218
Epoch 7/50
188/188 [==============================] - 6s 30ms/step - loss: 1.0512 - accuracy: 0.4339 - auc: 0.6235 - f1_score: 0.4220 - val_loss: 1.0551 - val_accuracy: 0.4332 - val_auc: 0.6217 - val_f1_score: 0.4237
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 408)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          204500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 405,043
Trainable params: 405,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 43ms/step - loss: 1.0725 - accuracy: 0.4043 - auc: 0.5918 - f1_score: 0.4174 - val_loss: 1.0564 - val_accuracy: 0.4278 - val_auc: 0.6185 - val_f1_score: 0.3719
Epoch 2/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0538 - accuracy: 0.4316 - auc: 0.6195 - f1_score: 0.4073 - val_loss: 1.0552 - val_accuracy: 0.4290 - val_auc: 0.6199 - val_f1_score: 0.3875
Epoch 3/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0507 - accuracy: 0.4351 - auc: 0.6243 - f1_score: 0.4178 - val_loss: 1.0547 - val_accuracy: 0.4323 - val_auc: 0.6206 - val_f1_score: 0.4129
Epoch 4/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0504 - accuracy: 0.4343 - auc: 0.6250 - f1_score: 0.4218 - val_loss: 1.0537 - val_accuracy: 0.4363 - val_auc: 0.6225 - val_f1_score: 0.4214
Epoch 5/50
188/188 [==============================] - 7s 40ms/step - loss: 1.0500 - accuracy: 0.4359 - auc: 0.6249 - f1_score: 0.4244 - val_loss: 1.0542 - val_accuracy: 0.4320 - val_auc: 0.6216 - val_f1_score: 0.4198
Epoch 6/50
188/188 [==============================] - 7s 40ms/step - loss: 1.0494 - accuracy: 0.4368 - auc: 0.6260 - f1_score: 0.4285 - val_loss: 1.0539 - val_accuracy: 0.4353 - val_auc: 0.6225 - val_f1_score: 0.4203
Epoch 7/50
188/188 [==============================] - 7s 40ms/step - loss: 1.0492 - accuracy: 0.4384 - auc: 0.6267 - f1_score: 0.4288 - val_loss: 1.0538 - val_accuracy: 0.4374 - val_auc: 0.6223 - val_f1_score: 0.4305
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 608)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          304500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 505,043
Trainable params: 505,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 59ms/step - loss: 1.0753 - accuracy: 0.4095 - auc: 0.5876 - f1_score: 0.4169 - val_loss: 1.0573 - val_accuracy: 0.4235 - val_auc: 0.6172 - val_f1_score: 0.3770
Epoch 2/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0548 - accuracy: 0.4287 - auc: 0.6184 - f1_score: 0.4117 - val_loss: 1.0539 - val_accuracy: 0.4269 - val_auc: 0.6219 - val_f1_score: 0.3930
Epoch 3/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0508 - accuracy: 0.4340 - auc: 0.6241 - f1_score: 0.4212 - val_loss: 1.0535 - val_accuracy: 0.4306 - val_auc: 0.6229 - val_f1_score: 0.4134
Epoch 4/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0502 - accuracy: 0.4355 - auc: 0.6254 - f1_score: 0.4250 - val_loss: 1.0527 - val_accuracy: 0.4318 - val_auc: 0.6239 - val_f1_score: 0.4123
Epoch 5/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0489 - accuracy: 0.4381 - auc: 0.6266 - f1_score: 0.4258 - val_loss: 1.0530 - val_accuracy: 0.4325 - val_auc: 0.6234 - val_f1_score: 0.4192
Epoch 6/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0479 - accuracy: 0.4393 - auc: 0.6291 - f1_score: 0.4326 - val_loss: 1.0531 - val_accuracy: 0.4327 - val_auc: 0.6230 - val_f1_score: 0.4192
Epoch 7/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0481 - accuracy: 0.4394 - auc: 0.6284 - f1_score: 0.4309 - val_loss: 1.0524 - val_accuracy: 0.4371 - val_auc: 0.6238 - val_f1_score: 0.4319
Epoch 8/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0469 - accuracy: 0.4420 - auc: 0.6302 - f1_score: 0.4340 - val_loss: 1.0526 - val_accuracy: 0.4342 - val_auc: 0.6238 - val_f1_score: 0.4237
Epoch 9/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0463 - accuracy: 0.4409 - auc: 0.6308 - f1_score: 0.4325 - val_loss: 1.0528 - val_accuracy: 0.4332 - val_auc: 0.6236 - val_f1_score: 0.4196
Epoch 10/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0460 - accuracy: 0.4421 - auc: 0.6310 - f1_score: 0.4325 - val_loss: 1.0525 - val_accuracy: 0.4335 - val_auc: 0.6242 - val_f1_score: 0.4247
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 808)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          404500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 605,043
Trainable params: 605,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 77ms/step - loss: 1.0742 - accuracy: 0.4143 - auc: 0.5884 - f1_score: 0.4061 - val_loss: 1.0605 - val_accuracy: 0.4260 - val_auc: 0.6131 - val_f1_score: 0.3740
Epoch 2/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0535 - accuracy: 0.4350 - auc: 0.6213 - f1_score: 0.4052 - val_loss: 1.0534 - val_accuracy: 0.4338 - val_auc: 0.6224 - val_f1_score: 0.4275
Epoch 3/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0494 - accuracy: 0.4383 - auc: 0.6262 - f1_score: 0.4278 - val_loss: 1.0526 - val_accuracy: 0.4323 - val_auc: 0.6233 - val_f1_score: 0.4239
Epoch 4/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0482 - accuracy: 0.4397 - auc: 0.6282 - f1_score: 0.4306 - val_loss: 1.0523 - val_accuracy: 0.4330 - val_auc: 0.6242 - val_f1_score: 0.4228
Epoch 5/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0468 - accuracy: 0.4419 - auc: 0.6300 - f1_score: 0.4325 - val_loss: 1.0527 - val_accuracy: 0.4367 - val_auc: 0.6239 - val_f1_score: 0.4286
Epoch 6/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0465 - accuracy: 0.4438 - auc: 0.6306 - f1_score: 0.4365 - val_loss: 1.0520 - val_accuracy: 0.4355 - val_auc: 0.6244 - val_f1_score: 0.4280
Epoch 7/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0457 - accuracy: 0.4416 - auc: 0.6314 - f1_score: 0.4337 - val_loss: 1.0528 - val_accuracy: 0.4362 - val_auc: 0.6242 - val_f1_score: 0.4283
Epoch 8/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0454 - accuracy: 0.4431 - auc: 0.6314 - f1_score: 0.4362 - val_loss: 1.0523 - val_accuracy: 0.4345 - val_auc: 0.6242 - val_f1_score: 0.4258
Epoch 9/50
188/188 [==============================] - 15s 80ms/step - loss: 1.0448 - accuracy: 0.4440 - auc: 0.6326 - f1_score: 0.4362 - val_loss: 1.0521 - val_accuracy: 0.4350 - val_auc: 0.6243 - val_f1_score: 0.4308
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.430015</td>
      <td>0.423959</td>
      <td>0.446179</td>
      <td>0.447668</td>
      <td>0.635884</td>
      <td>0.635908</td>
      <td>0.441845</td>
      <td>0.442506</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          504500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 705,043
Trainable params: 705,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 23s 103ms/step - loss: 1.0756 - accuracy: 0.4240 - auc: 0.5854 - f1_score: 0.4185 - val_loss: 1.0644 - val_accuracy: 0.4241 - val_auc: 0.6053 - val_f1_score: 0.3923
Epoch 2/50
188/188 [==============================] - 18s 93ms/step - loss: 1.0579 - accuracy: 0.4367 - auc: 0.6178 - f1_score: 0.4211 - val_loss: 1.0549 - val_accuracy: 0.4306 - val_auc: 0.6208 - val_f1_score: 0.4153
Epoch 3/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0497 - accuracy: 0.4372 - auc: 0.6263 - f1_score: 0.4245 - val_loss: 1.0518 - val_accuracy: 0.4350 - val_auc: 0.6233 - val_f1_score: 0.4197
Epoch 4/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0475 - accuracy: 0.4397 - auc: 0.6287 - f1_score: 0.4268 - val_loss: 1.0520 - val_accuracy: 0.4348 - val_auc: 0.6239 - val_f1_score: 0.4243
Epoch 5/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0461 - accuracy: 0.4416 - auc: 0.6305 - f1_score: 0.4322 - val_loss: 1.0513 - val_accuracy: 0.4312 - val_auc: 0.6245 - val_f1_score: 0.4228
Epoch 6/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0452 - accuracy: 0.4446 - auc: 0.6322 - f1_score: 0.4365 - val_loss: 1.0517 - val_accuracy: 0.4343 - val_auc: 0.6246 - val_f1_score: 0.4266
Epoch 7/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0446 - accuracy: 0.4444 - auc: 0.6327 - f1_score: 0.4353 - val_loss: 1.0516 - val_accuracy: 0.4337 - val_auc: 0.6241 - val_f1_score: 0.4282
Epoch 8/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0438 - accuracy: 0.4456 - auc: 0.6345 - f1_score: 0.4375 - val_loss: 1.0513 - val_accuracy: 0.4327 - val_auc: 0.6252 - val_f1_score: 0.4260
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.430015</td>
      <td>0.423959</td>
      <td>0.446179</td>
      <td>0.447668</td>
      <td>0.635884</td>
      <td>0.635908</td>
      <td>0.441845</td>
      <td>0.442506</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.432404</td>
      <td>0.426394</td>
      <td>0.446887</td>
      <td>0.448648</td>
      <td>0.637872</td>
      <td>0.637953</td>
      <td>0.439653</td>
      <td>0.440383</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1208)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          604500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 805,043
Trainable params: 805,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 25s 110ms/step - loss: 1.0715 - accuracy: 0.4131 - auc: 0.5951 - f1_score: 0.4254 - val_loss: 1.0549 - val_accuracy: 0.4320 - val_auc: 0.6208 - val_f1_score: 0.3806
Epoch 2/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0504 - accuracy: 0.4362 - auc: 0.6253 - f1_score: 0.4152 - val_loss: 1.0527 - val_accuracy: 0.4343 - val_auc: 0.6231 - val_f1_score: 0.4026
Epoch 3/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0475 - accuracy: 0.4411 - auc: 0.6291 - f1_score: 0.4290 - val_loss: 1.0519 - val_accuracy: 0.4300 - val_auc: 0.6243 - val_f1_score: 0.4207
Epoch 4/50
188/188 [==============================] - 21s 114ms/step - loss: 1.0459 - accuracy: 0.4431 - auc: 0.6317 - f1_score: 0.4352 - val_loss: 1.0516 - val_accuracy: 0.4294 - val_auc: 0.6248 - val_f1_score: 0.4162
Epoch 5/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0449 - accuracy: 0.4459 - auc: 0.6332 - f1_score: 0.4381 - val_loss: 1.0522 - val_accuracy: 0.4300 - val_auc: 0.6240 - val_f1_score: 0.4136
Epoch 6/50
188/188 [==============================] - 22s 116ms/step - loss: 1.0442 - accuracy: 0.4461 - auc: 0.6341 - f1_score: 0.4378 - val_loss: 1.0525 - val_accuracy: 0.4291 - val_auc: 0.6234 - val_f1_score: 0.4108
Epoch 7/50
188/188 [==============================] - 23s 120ms/step - loss: 1.0435 - accuracy: 0.4460 - auc: 0.6352 - f1_score: 0.4380 - val_loss: 1.0519 - val_accuracy: 0.4302 - val_auc: 0.6247 - val_f1_score: 0.4180
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.430015</td>
      <td>0.423959</td>
      <td>0.446179</td>
      <td>0.447668</td>
      <td>0.635884</td>
      <td>0.635908</td>
      <td>0.441845</td>
      <td>0.442506</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.432404</td>
      <td>0.426394</td>
      <td>0.446887</td>
      <td>0.448648</td>
      <td>0.637872</td>
      <td>0.637953</td>
      <td>0.439653</td>
      <td>0.440383</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.432681</td>
      <td>0.426723</td>
      <td>0.453308</td>
      <td>0.454646</td>
      <td>0.640547</td>
      <td>0.640675</td>
      <td>0.441138</td>
      <td>0.441219</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1408)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          704500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 905,043
Trainable params: 905,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 28s 127ms/step - loss: 1.0660 - accuracy: 0.4207 - auc: 0.6031 - f1_score: 0.4288 - val_loss: 1.0537 - val_accuracy: 0.4294 - val_auc: 0.6214 - val_f1_score: 0.3940
Epoch 2/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0489 - accuracy: 0.4393 - auc: 0.6274 - f1_score: 0.4213 - val_loss: 1.0523 - val_accuracy: 0.4306 - val_auc: 0.6230 - val_f1_score: 0.4107
Epoch 3/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0464 - accuracy: 0.4439 - auc: 0.6311 - f1_score: 0.4295 - val_loss: 1.0516 - val_accuracy: 0.4301 - val_auc: 0.6231 - val_f1_score: 0.4138
Epoch 4/50
188/188 [==============================] - 21s 112ms/step - loss: 1.0450 - accuracy: 0.4450 - auc: 0.6327 - f1_score: 0.4332 - val_loss: 1.0517 - val_accuracy: 0.4335 - val_auc: 0.6240 - val_f1_score: 0.4144
Epoch 5/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0435 - accuracy: 0.4460 - auc: 0.6352 - f1_score: 0.4360 - val_loss: 1.0522 - val_accuracy: 0.4287 - val_auc: 0.6233 - val_f1_score: 0.4126
Epoch 6/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0428 - accuracy: 0.4490 - auc: 0.6369 - f1_score: 0.4378 - val_loss: 1.0520 - val_accuracy: 0.4332 - val_auc: 0.6236 - val_f1_score: 0.4216
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.430015</td>
      <td>0.423959</td>
      <td>0.446179</td>
      <td>0.447668</td>
      <td>0.635884</td>
      <td>0.635908</td>
      <td>0.441845</td>
      <td>0.442506</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.432404</td>
      <td>0.426394</td>
      <td>0.446887</td>
      <td>0.448648</td>
      <td>0.637872</td>
      <td>0.637953</td>
      <td>0.439653</td>
      <td>0.440383</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.432681</td>
      <td>0.426723</td>
      <td>0.453308</td>
      <td>0.454646</td>
      <td>0.640547</td>
      <td>0.640675</td>
      <td>0.441138</td>
      <td>0.441219</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.433996</td>
      <td>0.428014</td>
      <td>0.451814</td>
      <td>0.453754</td>
      <td>0.642006</td>
      <td>0.642063</td>
      <td>0.440296</td>
      <td>0.441086</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1608)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          804500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,005,043
Trainable params: 1,005,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 32s 142ms/step - loss: 1.0656 - accuracy: 0.4225 - auc: 0.6037 - f1_score: 0.4233 - val_loss: 1.0530 - val_accuracy: 0.4308 - val_auc: 0.6216 - val_f1_score: 0.4169
Epoch 2/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0479 - accuracy: 0.4410 - auc: 0.6283 - f1_score: 0.4300 - val_loss: 1.0517 - val_accuracy: 0.4336 - val_auc: 0.6237 - val_f1_score: 0.4234
Epoch 3/50
188/188 [==============================] - 26s 137ms/step - loss: 1.0456 - accuracy: 0.4430 - auc: 0.6314 - f1_score: 0.4358 - val_loss: 1.0516 - val_accuracy: 0.4323 - val_auc: 0.6244 - val_f1_score: 0.4240
Epoch 4/50
188/188 [==============================] - 27s 145ms/step - loss: 1.0445 - accuracy: 0.4449 - auc: 0.6334 - f1_score: 0.4388 - val_loss: 1.0514 - val_accuracy: 0.4329 - val_auc: 0.6248 - val_f1_score: 0.4189
Epoch 5/50
188/188 [==============================] - 25s 132ms/step - loss: 1.0436 - accuracy: 0.4489 - auc: 0.6352 - f1_score: 0.4412 - val_loss: 1.0514 - val_accuracy: 0.4360 - val_auc: 0.6247 - val_f1_score: 0.4293
Epoch 6/50
188/188 [==============================] - 26s 136ms/step - loss: 1.0429 - accuracy: 0.4475 - auc: 0.6360 - f1_score: 0.4419 - val_loss: 1.0514 - val_accuracy: 0.4312 - val_auc: 0.6244 - val_f1_score: 0.4207
Epoch 7/50
188/188 [==============================] - 25s 135ms/step - loss: 1.0411 - accuracy: 0.4504 - auc: 0.6381 - f1_score: 0.4429 - val_loss: 1.0520 - val_accuracy: 0.4337 - val_auc: 0.6237 - val_f1_score: 0.4302
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.430015</td>
      <td>0.423959</td>
      <td>0.446179</td>
      <td>0.447668</td>
      <td>0.635884</td>
      <td>0.635908</td>
      <td>0.441845</td>
      <td>0.442506</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.432404</td>
      <td>0.426394</td>
      <td>0.446887</td>
      <td>0.448648</td>
      <td>0.637872</td>
      <td>0.637953</td>
      <td>0.439653</td>
      <td>0.440383</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.432681</td>
      <td>0.426723</td>
      <td>0.453308</td>
      <td>0.454646</td>
      <td>0.640547</td>
      <td>0.640675</td>
      <td>0.441138</td>
      <td>0.441219</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.433996</td>
      <td>0.428014</td>
      <td>0.451814</td>
      <td>0.453754</td>
      <td>0.642006</td>
      <td>0.642063</td>
      <td>0.440296</td>
      <td>0.441086</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.433360</td>
      <td>0.427190</td>
      <td>0.454387</td>
      <td>0.454970</td>
      <td>0.643039</td>
      <td>0.642296</td>
      <td>0.450757</td>
      <td>0.450540</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,105,043
Trainable params: 1,105,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 33s 152ms/step - loss: 1.0775 - accuracy: 0.3823 - auc: 0.5835 - f1_score: 0.4241 - val_loss: 1.0633 - val_accuracy: 0.4308 - val_auc: 0.6108 - val_f1_score: 0.4072
Epoch 2/50
188/188 [==============================] - 26s 137ms/step - loss: 1.0545 - accuracy: 0.4320 - auc: 0.6196 - f1_score: 0.4118 - val_loss: 1.0548 - val_accuracy: 0.4275 - val_auc: 0.6191 - val_f1_score: 0.3902
Epoch 3/50
188/188 [==============================] - 27s 142ms/step - loss: 1.0489 - accuracy: 0.4381 - auc: 0.6275 - f1_score: 0.4173 - val_loss: 1.0534 - val_accuracy: 0.4329 - val_auc: 0.6214 - val_f1_score: 0.4088
Epoch 4/50
188/188 [==============================] - 27s 145ms/step - loss: 1.0459 - accuracy: 0.4418 - auc: 0.6311 - f1_score: 0.4295 - val_loss: 1.0529 - val_accuracy: 0.4313 - val_auc: 0.6226 - val_f1_score: 0.4072
Epoch 5/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0459 - accuracy: 0.4438 - auc: 0.6318 - f1_score: 0.4309 - val_loss: 1.0532 - val_accuracy: 0.4301 - val_auc: 0.6222 - val_f1_score: 0.4127
Epoch 6/50
188/188 [==============================] - 31s 163ms/step - loss: 1.0433 - accuracy: 0.4477 - auc: 0.6354 - f1_score: 0.4372 - val_loss: 1.0530 - val_accuracy: 0.4303 - val_auc: 0.6225 - val_f1_score: 0.4121
Epoch 7/50
188/188 [==============================] - 31s 163ms/step - loss: 1.0421 - accuracy: 0.4490 - auc: 0.6369 - f1_score: 0.4397 - val_loss: 1.0528 - val_accuracy: 0.4326 - val_auc: 0.6228 - val_f1_score: 0.4147
Epoch 8/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0408 - accuracy: 0.4509 - auc: 0.6392 - f1_score: 0.4429 - val_loss: 1.0528 - val_accuracy: 0.4307 - val_auc: 0.6223 - val_f1_score: 0.4141
Epoch 9/50
188/188 [==============================] - 25s 134ms/step - loss: 1.0395 - accuracy: 0.4548 - auc: 0.6412 - f1_score: 0.4471 - val_loss: 1.0540 - val_accuracy: 0.4313 - val_auc: 0.6220 - val_f1_score: 0.4208
Epoch 10/50
188/188 [==============================] - 25s 135ms/step - loss: 1.0384 - accuracy: 0.4542 - auc: 0.6425 - f1_score: 0.4469 - val_loss: 1.0542 - val_accuracy: 0.4283 - val_auc: 0.6214 - val_f1_score: 0.4092
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.430015</td>
      <td>0.423959</td>
      <td>0.446179</td>
      <td>0.447668</td>
      <td>0.635884</td>
      <td>0.635908</td>
      <td>0.441845</td>
      <td>0.442506</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.432404</td>
      <td>0.426394</td>
      <td>0.446887</td>
      <td>0.448648</td>
      <td>0.637872</td>
      <td>0.637953</td>
      <td>0.439653</td>
      <td>0.440383</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.432681</td>
      <td>0.426723</td>
      <td>0.453308</td>
      <td>0.454646</td>
      <td>0.640547</td>
      <td>0.640675</td>
      <td>0.441138</td>
      <td>0.441219</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.433996</td>
      <td>0.428014</td>
      <td>0.451814</td>
      <td>0.453754</td>
      <td>0.642006</td>
      <td>0.642063</td>
      <td>0.440296</td>
      <td>0.441086</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.433360</td>
      <td>0.427190</td>
      <td>0.454387</td>
      <td>0.454970</td>
      <td>0.643039</td>
      <td>0.642296</td>
      <td>0.450757</td>
      <td>0.450540</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.436816</td>
      <td>0.430666</td>
      <td>0.460371</td>
      <td>0.461428</td>
      <td>0.648606</td>
      <td>0.647873</td>
      <td>0.442060</td>
      <td>0.441691</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      41200       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      41200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,205,043
Trainable params: 1,205,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 34s 154ms/step - loss: 1.0668 - accuracy: 0.4173 - auc: 0.6032 - f1_score: 0.4118 - val_loss: 1.0564 - val_accuracy: 0.4263 - val_auc: 0.6179 - val_f1_score: 0.3707
Epoch 2/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0493 - accuracy: 0.4331 - auc: 0.6255 - f1_score: 0.3934 - val_loss: 1.0529 - val_accuracy: 0.4245 - val_auc: 0.6207 - val_f1_score: 0.3833
Epoch 3/50
188/188 [==============================] - 32s 172ms/step - loss: 1.0462 - accuracy: 0.4402 - auc: 0.6308 - f1_score: 0.4166 - val_loss: 1.0522 - val_accuracy: 0.4307 - val_auc: 0.6227 - val_f1_score: 0.4079
Epoch 4/50
188/188 [==============================] - 33s 173ms/step - loss: 1.0446 - accuracy: 0.4451 - auc: 0.6334 - f1_score: 0.4283 - val_loss: 1.0528 - val_accuracy: 0.4308 - val_auc: 0.6228 - val_f1_score: 0.4132
Epoch 5/50
188/188 [==============================] - 34s 182ms/step - loss: 1.0428 - accuracy: 0.4464 - auc: 0.6359 - f1_score: 0.4338 - val_loss: 1.0529 - val_accuracy: 0.4311 - val_auc: 0.6225 - val_f1_score: 0.4199
Epoch 6/50
188/188 [==============================] - 34s 178ms/step - loss: 1.0421 - accuracy: 0.4491 - auc: 0.6374 - f1_score: 0.4391 - val_loss: 1.0528 - val_accuracy: 0.4324 - val_auc: 0.6216 - val_f1_score: 0.4223
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0.420874</td>
      <td>0.414307</td>
      <td>0.425969</td>
      <td>0.428311</td>
      <td>0.612525</td>
      <td>0.612039</td>
      <td>0.401707</td>
      <td>0.402642</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>7</th>
      <td>0.423529</td>
      <td>0.417128</td>
      <td>0.431899</td>
      <td>0.433721</td>
      <td>0.615959</td>
      <td>0.615889</td>
      <td>0.416635</td>
      <td>0.417222</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>8</th>
      <td>0.422975</td>
      <td>0.416422</td>
      <td>0.433534</td>
      <td>0.435172</td>
      <td>0.618341</td>
      <td>0.617855</td>
      <td>0.415715</td>
      <td>0.415966</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>9</th>
      <td>0.425592</td>
      <td>0.419126</td>
      <td>0.435594</td>
      <td>0.436612</td>
      <td>0.621961</td>
      <td>0.621458</td>
      <td>0.423059</td>
      <td>0.422832</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>10</th>
      <td>0.423286</td>
      <td>0.416875</td>
      <td>0.434853</td>
      <td>0.436573</td>
      <td>0.622327</td>
      <td>0.622197</td>
      <td>0.421499</td>
      <td>0.422001</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>0.409566</td>
      <td>0.402786</td>
      <td>0.391360</td>
      <td>0.394488</td>
      <td>0.571308</td>
      <td>0.571356</td>
      <td>0.361291</td>
      <td>0.362827</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>0.412860</td>
      <td>0.406185</td>
      <td>0.401803</td>
      <td>0.404848</td>
      <td>0.585563</td>
      <td>0.585721</td>
      <td>0.349418</td>
      <td>0.350517</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>0.414680</td>
      <td>0.407978</td>
      <td>0.412649</td>
      <td>0.413874</td>
      <td>0.595432</td>
      <td>0.594923</td>
      <td>0.404750</td>
      <td>0.404876</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>0.419501</td>
      <td>0.412791</td>
      <td>0.424476</td>
      <td>0.425577</td>
      <td>0.607537</td>
      <td>0.606712</td>
      <td>0.412870</td>
      <td>0.412646</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>0.423384</td>
      <td>0.416488</td>
      <td>0.437110</td>
      <td>0.436828</td>
      <td>0.622865</td>
      <td>0.620647</td>
      <td>0.422771</td>
      <td>0.421295</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>0.422466</td>
      <td>0.415599</td>
      <td>0.435093</td>
      <td>0.434103</td>
      <td>0.621498</td>
      <td>0.619183</td>
      <td>0.430489</td>
      <td>0.428656</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>0.425979</td>
      <td>0.419065</td>
      <td>0.443933</td>
      <td>0.442601</td>
      <td>0.629456</td>
      <td>0.626906</td>
      <td>0.434556</td>
      <td>0.432162</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>0.426935</td>
      <td>0.419987</td>
      <td>0.443704</td>
      <td>0.443581</td>
      <td>0.631292</td>
      <td>0.628751</td>
      <td>0.427411</td>
      <td>0.426077</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>0.432568</td>
      <td>0.425299</td>
      <td>0.457014</td>
      <td>0.454431</td>
      <td>0.643305</td>
      <td>0.639364</td>
      <td>0.450090</td>
      <td>0.446546</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>0.430086</td>
      <td>0.423145</td>
      <td>0.449296</td>
      <td>0.449638</td>
      <td>0.637246</td>
      <td>0.634833</td>
      <td>0.427514</td>
      <td>0.426469</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>0.428211</td>
      <td>0.422287</td>
      <td>0.435954</td>
      <td>0.437828</td>
      <td>0.625655</td>
      <td>0.626347</td>
      <td>0.423762</td>
      <td>0.424380</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>0.429134</td>
      <td>0.423162</td>
      <td>0.440249</td>
      <td>0.441866</td>
      <td>0.629152</td>
      <td>0.629532</td>
      <td>0.432957</td>
      <td>0.433552</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>0.431161</td>
      <td>0.425275</td>
      <td>0.443585</td>
      <td>0.446159</td>
      <td>0.632552</td>
      <td>0.633345</td>
      <td>0.429800</td>
      <td>0.431007</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>0.430215</td>
      <td>0.424288</td>
      <td>0.444860</td>
      <td>0.446600</td>
      <td>0.633238</td>
      <td>0.633691</td>
      <td>0.439542</td>
      <td>0.440396</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>0.431511</td>
      <td>0.425465</td>
      <td>0.447759</td>
      <td>0.449687</td>
      <td>0.637270</td>
      <td>0.637300</td>
      <td>0.438268</td>
      <td>0.439050</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>0.431356</td>
      <td>0.425425</td>
      <td>0.445732</td>
      <td>0.447746</td>
      <td>0.636244</td>
      <td>0.636658</td>
      <td>0.438326</td>
      <td>0.439369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>0.434613</td>
      <td>0.428611</td>
      <td>0.451324</td>
      <td>0.452921</td>
      <td>0.639785</td>
      <td>0.639764</td>
      <td>0.439711</td>
      <td>0.440177</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>0.434162</td>
      <td>0.428099</td>
      <td>0.452436</td>
      <td>0.453568</td>
      <td>0.641190</td>
      <td>0.640821</td>
      <td>0.442947</td>
      <td>0.443066</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>0.434577</td>
      <td>0.428493</td>
      <td>0.453809</td>
      <td>0.455117</td>
      <td>0.643228</td>
      <td>0.642921</td>
      <td>0.443746</td>
      <td>0.443928</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>0.435557</td>
      <td>0.429423</td>
      <td>0.457134</td>
      <td>0.458292</td>
      <td>0.646171</td>
      <td>0.645533</td>
      <td>0.444971</td>
      <td>0.444904</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>31</th>
      <td>0.428895</td>
      <td>0.422991</td>
      <td>0.436248</td>
      <td>0.439141</td>
      <td>0.626643</td>
      <td>0.627428</td>
      <td>0.416025</td>
      <td>0.417477</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>32</th>
      <td>0.428810</td>
      <td>0.422773</td>
      <td>0.440609</td>
      <td>0.441787</td>
      <td>0.630732</td>
      <td>0.630879</td>
      <td>0.431680</td>
      <td>0.431837</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>33</th>
      <td>0.431324</td>
      <td>0.425110</td>
      <td>0.450724</td>
      <td>0.450951</td>
      <td>0.639010</td>
      <td>0.638336</td>
      <td>0.447674</td>
      <td>0.447127</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>34</th>
      <td>0.434422</td>
      <td>0.428246</td>
      <td>0.448642</td>
      <td>0.450481</td>
      <td>0.639347</td>
      <td>0.638959</td>
      <td>0.428653</td>
      <td>0.429060</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>35</th>
      <td>0.432307</td>
      <td>0.425841</td>
      <td>0.457646</td>
      <td>0.457155</td>
      <td>0.645373</td>
      <td>0.643587</td>
      <td>0.453725</td>
      <td>0.452403</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>36</th>
      <td>0.434807</td>
      <td>0.428338</td>
      <td>0.453591</td>
      <td>0.452833</td>
      <td>0.644805</td>
      <td>0.642787</td>
      <td>0.451708</td>
      <td>0.450328</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>37</th>
      <td>0.438852</td>
      <td>0.432030</td>
      <td>0.470541</td>
      <td>0.468397</td>
      <td>0.659058</td>
      <td>0.655686</td>
      <td>0.465539</td>
      <td>0.462539</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>38</th>
      <td>0.438236</td>
      <td>0.431633</td>
      <td>0.464394</td>
      <td>0.463712</td>
      <td>0.654992</td>
      <td>0.652658</td>
      <td>0.455539</td>
      <td>0.453898</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>39</th>
      <td>0.440875</td>
      <td>0.434093</td>
      <td>0.468340</td>
      <td>0.467103</td>
      <td>0.658120</td>
      <td>0.655249</td>
      <td>0.459234</td>
      <td>0.456988</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>40</th>
      <td>0.441212</td>
      <td>0.434257</td>
      <td>0.476319</td>
      <td>0.473885</td>
      <td>0.664433</td>
      <td>0.660911</td>
      <td>0.471451</td>
      <td>0.468173</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>3</td>
    </tr>
    <tr>
      <th>41</th>
      <td>0.427431</td>
      <td>0.421428</td>
      <td>0.437273</td>
      <td>0.439268</td>
      <td>0.626936</td>
      <td>0.627383</td>
      <td>0.426847</td>
      <td>0.427606</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>42</th>
      <td>0.428426</td>
      <td>0.422414</td>
      <td>0.441274</td>
      <td>0.442826</td>
      <td>0.630513</td>
      <td>0.630709</td>
      <td>0.434591</td>
      <td>0.435120</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>43</th>
      <td>0.431404</td>
      <td>0.425487</td>
      <td>0.444315</td>
      <td>0.446443</td>
      <td>0.635047</td>
      <td>0.635598</td>
      <td>0.436086</td>
      <td>0.437079</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>44</th>
      <td>0.430015</td>
      <td>0.423959</td>
      <td>0.446179</td>
      <td>0.447668</td>
      <td>0.635884</td>
      <td>0.635908</td>
      <td>0.441845</td>
      <td>0.442506</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>45</th>
      <td>0.432404</td>
      <td>0.426394</td>
      <td>0.446887</td>
      <td>0.448648</td>
      <td>0.637872</td>
      <td>0.637953</td>
      <td>0.439653</td>
      <td>0.440383</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>46</th>
      <td>0.432681</td>
      <td>0.426723</td>
      <td>0.453308</td>
      <td>0.454646</td>
      <td>0.640547</td>
      <td>0.640675</td>
      <td>0.441138</td>
      <td>0.441219</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.433996</td>
      <td>0.428014</td>
      <td>0.451814</td>
      <td>0.453754</td>
      <td>0.642006</td>
      <td>0.642063</td>
      <td>0.440296</td>
      <td>0.441086</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.433360</td>
      <td>0.427190</td>
      <td>0.454387</td>
      <td>0.454970</td>
      <td>0.643039</td>
      <td>0.642296</td>
      <td>0.450757</td>
      <td>0.450540</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.436816</td>
      <td>0.430666</td>
      <td>0.460371</td>
      <td>0.461428</td>
      <td>0.648606</td>
      <td>0.647873</td>
      <td>0.442060</td>
      <td>0.441691</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.433134</td>
      <td>0.427003</td>
      <td>0.453428</td>
      <td>0.454529</td>
      <td>0.643891</td>
      <td>0.643565</td>
      <td>0.442517</td>
      <td>0.442524</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 1, 3)
Test shape: (10292, 1, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 208)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          104500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 243,243
Trainable params: 243,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 31ms/step - loss: 1.0761 - accuracy: 0.4047 - auc: 0.5844 - f1_score: 0.4129 - val_loss: 1.0594 - val_accuracy: 0.4265 - val_auc: 0.6135 - val_f1_score: 0.3780
Epoch 2/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0575 - accuracy: 0.4268 - auc: 0.6143 - f1_score: 0.3795 - val_loss: 1.0562 - val_accuracy: 0.4285 - val_auc: 0.6176 - val_f1_score: 0.3804
Epoch 3/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0546 - accuracy: 0.4282 - auc: 0.6184 - f1_score: 0.3838 - val_loss: 1.0564 - val_accuracy: 0.4251 - val_auc: 0.6177 - val_f1_score: 0.3841
Epoch 4/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0536 - accuracy: 0.4299 - auc: 0.6195 - f1_score: 0.4000 - val_loss: 1.0561 - val_accuracy: 0.4277 - val_auc: 0.6198 - val_f1_score: 0.3887
Epoch 5/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0525 - accuracy: 0.4321 - auc: 0.6215 - f1_score: 0.4034 - val_loss: 1.0558 - val_accuracy: 0.4347 - val_auc: 0.6201 - val_f1_score: 0.4072
Epoch 6/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0521 - accuracy: 0.4337 - auc: 0.6220 - f1_score: 0.4123 - val_loss: 1.0559 - val_accuracy: 0.4325 - val_auc: 0.6201 - val_f1_score: 0.4128
Epoch 7/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0516 - accuracy: 0.4334 - auc: 0.6228 - f1_score: 0.4132 - val_loss: 1.0560 - val_accuracy: 0.4325 - val_auc: 0.6200 - val_f1_score: 0.4161
Epoch 8/50
188/188 [==============================] - 6s 30ms/step - loss: 1.0509 - accuracy: 0.4355 - auc: 0.6241 - f1_score: 0.4191 - val_loss: 1.0558 - val_accuracy: 0.4312 - val_auc: 0.6203 - val_f1_score: 0.4125
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>47</th>
      <td>0.433996</td>
      <td>0.428014</td>
      <td>0.451814</td>
      <td>0.453754</td>
      <td>0.642006</td>
      <td>0.642063</td>
      <td>0.440296</td>
      <td>0.441086</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.433360</td>
      <td>0.427190</td>
      <td>0.454387</td>
      <td>0.454970</td>
      <td>0.643039</td>
      <td>0.642296</td>
      <td>0.450757</td>
      <td>0.450540</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.436816</td>
      <td>0.430666</td>
      <td>0.460371</td>
      <td>0.461428</td>
      <td>0.648606</td>
      <td>0.647873</td>
      <td>0.442060</td>
      <td>0.441691</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.433134</td>
      <td>0.427003</td>
      <td>0.453428</td>
      <td>0.454529</td>
      <td>0.643891</td>
      <td>0.643565</td>
      <td>0.442517</td>
      <td>0.442524</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>51</th>
      <td>0.429799</td>
      <td>0.423908</td>
      <td>0.437546</td>
      <td>0.439925</td>
      <td>0.626992</td>
      <td>0.627791</td>
      <td>0.418663</td>
      <td>0.419517</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>51 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 2, 3)
Test shape: (10292, 2, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 408)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          204500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 343,243
Trainable params: 343,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 11s 39ms/step - loss: 1.0883 - accuracy: 0.3751 - auc: 0.5509 - f1_score: 0.3803 - val_loss: 1.0777 - val_accuracy: 0.4113 - val_auc: 0.5864 - val_f1_score: 0.4042
Epoch 2/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0655 - accuracy: 0.4202 - auc: 0.6050 - f1_score: 0.4162 - val_loss: 1.0609 - val_accuracy: 0.4234 - val_auc: 0.6112 - val_f1_score: 0.4202
Epoch 3/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0563 - accuracy: 0.4273 - auc: 0.6166 - f1_score: 0.4210 - val_loss: 1.0566 - val_accuracy: 0.4302 - val_auc: 0.6173 - val_f1_score: 0.4238
Epoch 4/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0527 - accuracy: 0.4335 - auc: 0.6215 - f1_score: 0.4239 - val_loss: 1.0571 - val_accuracy: 0.4264 - val_auc: 0.6160 - val_f1_score: 0.4130
Epoch 5/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0516 - accuracy: 0.4331 - auc: 0.6232 - f1_score: 0.4223 - val_loss: 1.0566 - val_accuracy: 0.4287 - val_auc: 0.6164 - val_f1_score: 0.4188
Epoch 6/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0507 - accuracy: 0.4338 - auc: 0.6241 - f1_score: 0.4252 - val_loss: 1.0558 - val_accuracy: 0.4257 - val_auc: 0.6181 - val_f1_score: 0.4156
Epoch 7/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0496 - accuracy: 0.4386 - auc: 0.6260 - f1_score: 0.4303 - val_loss: 1.0560 - val_accuracy: 0.4271 - val_auc: 0.6189 - val_f1_score: 0.4130
Epoch 8/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0490 - accuracy: 0.4379 - auc: 0.6268 - f1_score: 0.4279 - val_loss: 1.0559 - val_accuracy: 0.4299 - val_auc: 0.6179 - val_f1_score: 0.4232
Epoch 9/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0484 - accuracy: 0.4381 - auc: 0.6277 - f1_score: 0.4298 - val_loss: 1.0555 - val_accuracy: 0.4282 - val_auc: 0.6191 - val_f1_score: 0.4215
Epoch 10/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0486 - accuracy: 0.4390 - auc: 0.6281 - f1_score: 0.4325 - val_loss: 1.0552 - val_accuracy: 0.4324 - val_auc: 0.6202 - val_f1_score: 0.4207
Epoch 11/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0479 - accuracy: 0.4389 - auc: 0.6283 - f1_score: 0.4302 - val_loss: 1.0563 - val_accuracy: 0.4260 - val_auc: 0.6179 - val_f1_score: 0.4158
Epoch 12/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0471 - accuracy: 0.4395 - auc: 0.6297 - f1_score: 0.4307 - val_loss: 1.0555 - val_accuracy: 0.4293 - val_auc: 0.6190 - val_f1_score: 0.4224
Epoch 13/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0477 - accuracy: 0.4390 - auc: 0.6287 - f1_score: 0.4334 - val_loss: 1.0564 - val_accuracy: 0.4266 - val_auc: 0.6181 - val_f1_score: 0.4168
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>48</th>
      <td>0.433360</td>
      <td>0.427190</td>
      <td>0.454387</td>
      <td>0.454970</td>
      <td>0.643039</td>
      <td>0.642296</td>
      <td>0.450757</td>
      <td>0.450540</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.436816</td>
      <td>0.430666</td>
      <td>0.460371</td>
      <td>0.461428</td>
      <td>0.648606</td>
      <td>0.647873</td>
      <td>0.442060</td>
      <td>0.441691</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.433134</td>
      <td>0.427003</td>
      <td>0.453428</td>
      <td>0.454529</td>
      <td>0.643891</td>
      <td>0.643565</td>
      <td>0.442517</td>
      <td>0.442524</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>51</th>
      <td>0.429799</td>
      <td>0.423908</td>
      <td>0.437546</td>
      <td>0.439925</td>
      <td>0.626992</td>
      <td>0.627791</td>
      <td>0.418663</td>
      <td>0.419517</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0.431442</td>
      <td>0.425406</td>
      <td>0.443366</td>
      <td>0.444610</td>
      <td>0.633184</td>
      <td>0.633280</td>
      <td>0.434007</td>
      <td>0.434173</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>52 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 3, 3)
Test shape: (10292, 3, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 608)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          304500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 443,243
Trainable params: 443,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 58ms/step - loss: 1.0919 - accuracy: 0.3578 - auc: 0.5435 - f1_score: 0.4058 - val_loss: 1.0808 - val_accuracy: 0.3993 - val_auc: 0.5777 - val_f1_score: 0.3718
Epoch 2/50
188/188 [==============================] - 9s 48ms/step - loss: 1.0705 - accuracy: 0.4075 - auc: 0.5944 - f1_score: 0.3909 - val_loss: 1.0626 - val_accuracy: 0.4201 - val_auc: 0.6091 - val_f1_score: 0.3667
Epoch 3/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0586 - accuracy: 0.4255 - auc: 0.6130 - f1_score: 0.4019 - val_loss: 1.0567 - val_accuracy: 0.4254 - val_auc: 0.6173 - val_f1_score: 0.3742
Epoch 4/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0532 - accuracy: 0.4322 - auc: 0.6206 - f1_score: 0.4000 - val_loss: 1.0576 - val_accuracy: 0.4246 - val_auc: 0.6169 - val_f1_score: 0.3786
Epoch 5/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0518 - accuracy: 0.4343 - auc: 0.6229 - f1_score: 0.4064 - val_loss: 1.0551 - val_accuracy: 0.4265 - val_auc: 0.6199 - val_f1_score: 0.3897
Epoch 6/50
188/188 [==============================] - 9s 48ms/step - loss: 1.0507 - accuracy: 0.4347 - auc: 0.6248 - f1_score: 0.4111 - val_loss: 1.0547 - val_accuracy: 0.4297 - val_auc: 0.6205 - val_f1_score: 0.4027
Epoch 7/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0497 - accuracy: 0.4350 - auc: 0.6259 - f1_score: 0.4145 - val_loss: 1.0549 - val_accuracy: 0.4297 - val_auc: 0.6194 - val_f1_score: 0.4010
Epoch 8/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0478 - accuracy: 0.4394 - auc: 0.6289 - f1_score: 0.4244 - val_loss: 1.0536 - val_accuracy: 0.4317 - val_auc: 0.6218 - val_f1_score: 0.4060
Epoch 9/50
188/188 [==============================] - 8s 41ms/step - loss: 1.0477 - accuracy: 0.4390 - auc: 0.6289 - f1_score: 0.4239 - val_loss: 1.0540 - val_accuracy: 0.4315 - val_auc: 0.6215 - val_f1_score: 0.4160
Epoch 10/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0480 - accuracy: 0.4398 - auc: 0.6287 - f1_score: 0.4272 - val_loss: 1.0539 - val_accuracy: 0.4303 - val_auc: 0.6213 - val_f1_score: 0.4144
Epoch 11/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0468 - accuracy: 0.4392 - auc: 0.6296 - f1_score: 0.4278 - val_loss: 1.0536 - val_accuracy: 0.4296 - val_auc: 0.6214 - val_f1_score: 0.4147
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>49</th>
      <td>0.436816</td>
      <td>0.430666</td>
      <td>0.460371</td>
      <td>0.461428</td>
      <td>0.648606</td>
      <td>0.647873</td>
      <td>0.442060</td>
      <td>0.441691</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.433134</td>
      <td>0.427003</td>
      <td>0.453428</td>
      <td>0.454529</td>
      <td>0.643891</td>
      <td>0.643565</td>
      <td>0.442517</td>
      <td>0.442524</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>51</th>
      <td>0.429799</td>
      <td>0.423908</td>
      <td>0.437546</td>
      <td>0.439925</td>
      <td>0.626992</td>
      <td>0.627791</td>
      <td>0.418663</td>
      <td>0.419517</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0.431442</td>
      <td>0.425406</td>
      <td>0.443366</td>
      <td>0.444610</td>
      <td>0.633184</td>
      <td>0.633280</td>
      <td>0.434007</td>
      <td>0.434173</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>53</th>
      <td>0.431460</td>
      <td>0.425422</td>
      <td>0.445394</td>
      <td>0.447158</td>
      <td>0.637919</td>
      <td>0.637933</td>
      <td>0.431140</td>
      <td>0.431641</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>53 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 4, 3)
Test shape: (10292, 4, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 808)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          404500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 543,243
Trainable params: 543,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 53ms/step - loss: 1.0838 - accuracy: 0.3907 - auc: 0.5677 - f1_score: 0.4106 - val_loss: 1.0684 - val_accuracy: 0.4121 - val_auc: 0.6018 - val_f1_score: 0.4095
Epoch 2/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0621 - accuracy: 0.4224 - auc: 0.6079 - f1_score: 0.4161 - val_loss: 1.0562 - val_accuracy: 0.4264 - val_auc: 0.6181 - val_f1_score: 0.4246
Epoch 3/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0543 - accuracy: 0.4302 - auc: 0.6191 - f1_score: 0.4253 - val_loss: 1.0557 - val_accuracy: 0.4309 - val_auc: 0.6175 - val_f1_score: 0.4272
Epoch 4/50
188/188 [==============================] - 9s 45ms/step - loss: 1.0518 - accuracy: 0.4334 - auc: 0.6219 - f1_score: 0.4291 - val_loss: 1.0551 - val_accuracy: 0.4293 - val_auc: 0.6198 - val_f1_score: 0.4261
Epoch 5/50
188/188 [==============================] - 9s 45ms/step - loss: 1.0497 - accuracy: 0.4355 - auc: 0.6254 - f1_score: 0.4303 - val_loss: 1.0544 - val_accuracy: 0.4239 - val_auc: 0.6200 - val_f1_score: 0.4205
Epoch 6/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0479 - accuracy: 0.4391 - auc: 0.6286 - f1_score: 0.4334 - val_loss: 1.0540 - val_accuracy: 0.4263 - val_auc: 0.6203 - val_f1_score: 0.4166
Epoch 7/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0463 - accuracy: 0.4423 - auc: 0.6304 - f1_score: 0.4356 - val_loss: 1.0538 - val_accuracy: 0.4288 - val_auc: 0.6215 - val_f1_score: 0.4241
Epoch 8/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0457 - accuracy: 0.4415 - auc: 0.6310 - f1_score: 0.4359 - val_loss: 1.0537 - val_accuracy: 0.4282 - val_auc: 0.6215 - val_f1_score: 0.4269
Epoch 9/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0451 - accuracy: 0.4438 - auc: 0.6323 - f1_score: 0.4381 - val_loss: 1.0538 - val_accuracy: 0.4288 - val_auc: 0.6218 - val_f1_score: 0.4259
Epoch 10/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0446 - accuracy: 0.4442 - auc: 0.6331 - f1_score: 0.4381 - val_loss: 1.0534 - val_accuracy: 0.4283 - val_auc: 0.6221 - val_f1_score: 0.4206
Epoch 11/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0434 - accuracy: 0.4464 - auc: 0.6349 - f1_score: 0.4407 - val_loss: 1.0538 - val_accuracy: 0.4288 - val_auc: 0.6212 - val_f1_score: 0.4241
Epoch 12/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0433 - accuracy: 0.4447 - auc: 0.6346 - f1_score: 0.4382 - val_loss: 1.0535 - val_accuracy: 0.4291 - val_auc: 0.6217 - val_f1_score: 0.4267
Epoch 13/50
188/188 [==============================] - 9s 45ms/step - loss: 1.0424 - accuracy: 0.4459 - auc: 0.6356 - f1_score: 0.4406 - val_loss: 1.0544 - val_accuracy: 0.4300 - val_auc: 0.6204 - val_f1_score: 0.4286
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>50</th>
      <td>0.433134</td>
      <td>0.427003</td>
      <td>0.453428</td>
      <td>0.454529</td>
      <td>0.643891</td>
      <td>0.643565</td>
      <td>0.442517</td>
      <td>0.442524</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>51</th>
      <td>0.429799</td>
      <td>0.423908</td>
      <td>0.437546</td>
      <td>0.439925</td>
      <td>0.626992</td>
      <td>0.627791</td>
      <td>0.418663</td>
      <td>0.419517</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0.431442</td>
      <td>0.425406</td>
      <td>0.443366</td>
      <td>0.444610</td>
      <td>0.633184</td>
      <td>0.633280</td>
      <td>0.434007</td>
      <td>0.434173</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>53</th>
      <td>0.431460</td>
      <td>0.425422</td>
      <td>0.445394</td>
      <td>0.447158</td>
      <td>0.637919</td>
      <td>0.637933</td>
      <td>0.431140</td>
      <td>0.431641</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>54</th>
      <td>0.432009</td>
      <td>0.425618</td>
      <td>0.455466</td>
      <td>0.455244</td>
      <td>0.644543</td>
      <td>0.643218</td>
      <td>0.453959</td>
      <td>0.453127</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>54 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 5, 3)
Test shape: (10292, 5, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          504500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 643,243
Trainable params: 643,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 63ms/step - loss: 1.0839 - accuracy: 0.3912 - auc: 0.5688 - f1_score: 0.4244 - val_loss: 1.0725 - val_accuracy: 0.4074 - val_auc: 0.5919 - val_f1_score: 0.3735
Epoch 2/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0656 - accuracy: 0.4149 - auc: 0.6017 - f1_score: 0.3952 - val_loss: 1.0601 - val_accuracy: 0.4225 - val_auc: 0.6094 - val_f1_score: 0.3899
Epoch 3/50
188/188 [==============================] - 10s 56ms/step - loss: 1.0577 - accuracy: 0.4249 - auc: 0.6125 - f1_score: 0.4046 - val_loss: 1.0575 - val_accuracy: 0.4255 - val_auc: 0.6147 - val_f1_score: 0.3885
Epoch 4/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0528 - accuracy: 0.4327 - auc: 0.6209 - f1_score: 0.4114 - val_loss: 1.0548 - val_accuracy: 0.4293 - val_auc: 0.6191 - val_f1_score: 0.4100
Epoch 5/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0500 - accuracy: 0.4359 - auc: 0.6245 - f1_score: 0.4209 - val_loss: 1.0550 - val_accuracy: 0.4315 - val_auc: 0.6202 - val_f1_score: 0.4124
Epoch 6/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0484 - accuracy: 0.4377 - auc: 0.6268 - f1_score: 0.4221 - val_loss: 1.0541 - val_accuracy: 0.4293 - val_auc: 0.6205 - val_f1_score: 0.4150
Epoch 7/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0468 - accuracy: 0.4373 - auc: 0.6283 - f1_score: 0.4237 - val_loss: 1.0557 - val_accuracy: 0.4249 - val_auc: 0.6173 - val_f1_score: 0.4044
Epoch 8/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0462 - accuracy: 0.4392 - auc: 0.6292 - f1_score: 0.4266 - val_loss: 1.0549 - val_accuracy: 0.4276 - val_auc: 0.6198 - val_f1_score: 0.4111
Epoch 9/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0448 - accuracy: 0.4428 - auc: 0.6318 - f1_score: 0.4311 - val_loss: 1.0547 - val_accuracy: 0.4225 - val_auc: 0.6190 - val_f1_score: 0.4041
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>51</th>
      <td>0.429799</td>
      <td>0.423908</td>
      <td>0.437546</td>
      <td>0.439925</td>
      <td>0.626992</td>
      <td>0.627791</td>
      <td>0.418663</td>
      <td>0.419517</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0.431442</td>
      <td>0.425406</td>
      <td>0.443366</td>
      <td>0.444610</td>
      <td>0.633184</td>
      <td>0.633280</td>
      <td>0.434007</td>
      <td>0.434173</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>53</th>
      <td>0.431460</td>
      <td>0.425422</td>
      <td>0.445394</td>
      <td>0.447158</td>
      <td>0.637919</td>
      <td>0.637933</td>
      <td>0.431140</td>
      <td>0.431641</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>54</th>
      <td>0.432009</td>
      <td>0.425618</td>
      <td>0.455466</td>
      <td>0.455244</td>
      <td>0.644543</td>
      <td>0.643218</td>
      <td>0.453959</td>
      <td>0.453127</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>55</th>
      <td>0.433546</td>
      <td>0.427289</td>
      <td>0.449503</td>
      <td>0.451235</td>
      <td>0.642273</td>
      <td>0.641356</td>
      <td>0.431301</td>
      <td>0.431733</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>55 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 6, 3)
Test shape: (10292, 6, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1208)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          604500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 743,243
Trainable params: 743,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 61ms/step - loss: 1.0877 - accuracy: 0.3638 - auc: 0.5557 - f1_score: 0.4136 - val_loss: 1.0787 - val_accuracy: 0.3793 - val_auc: 0.5797 - val_f1_score: 0.3294
Epoch 2/50
188/188 [==============================] - 10s 56ms/step - loss: 1.0708 - accuracy: 0.3936 - auc: 0.5925 - f1_score: 0.3854 - val_loss: 1.0637 - val_accuracy: 0.4194 - val_auc: 0.6054 - val_f1_score: 0.3806
Epoch 3/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0581 - accuracy: 0.4267 - auc: 0.6129 - f1_score: 0.4108 - val_loss: 1.0593 - val_accuracy: 0.4200 - val_auc: 0.6124 - val_f1_score: 0.3944
Epoch 4/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0519 - accuracy: 0.4349 - auc: 0.6218 - f1_score: 0.4221 - val_loss: 1.0551 - val_accuracy: 0.4235 - val_auc: 0.6185 - val_f1_score: 0.3987
Epoch 5/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0496 - accuracy: 0.4343 - auc: 0.6253 - f1_score: 0.4208 - val_loss: 1.0543 - val_accuracy: 0.4289 - val_auc: 0.6205 - val_f1_score: 0.4106
Epoch 6/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0475 - accuracy: 0.4412 - auc: 0.6289 - f1_score: 0.4312 - val_loss: 1.0541 - val_accuracy: 0.4285 - val_auc: 0.6211 - val_f1_score: 0.4043
Epoch 7/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0445 - accuracy: 0.4424 - auc: 0.6328 - f1_score: 0.4318 - val_loss: 1.0528 - val_accuracy: 0.4330 - val_auc: 0.6217 - val_f1_score: 0.4203
Epoch 8/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0436 - accuracy: 0.4457 - auc: 0.6343 - f1_score: 0.4363 - val_loss: 1.0540 - val_accuracy: 0.4329 - val_auc: 0.6211 - val_f1_score: 0.4169
Epoch 9/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0427 - accuracy: 0.4451 - auc: 0.6351 - f1_score: 0.4378 - val_loss: 1.0532 - val_accuracy: 0.4333 - val_auc: 0.6231 - val_f1_score: 0.4207
Epoch 10/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0416 - accuracy: 0.4498 - auc: 0.6375 - f1_score: 0.4404 - val_loss: 1.0535 - val_accuracy: 0.4341 - val_auc: 0.6233 - val_f1_score: 0.4235
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>52</th>
      <td>0.431442</td>
      <td>0.425406</td>
      <td>0.443366</td>
      <td>0.444610</td>
      <td>0.633184</td>
      <td>0.633280</td>
      <td>0.434007</td>
      <td>0.434173</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>53</th>
      <td>0.431460</td>
      <td>0.425422</td>
      <td>0.445394</td>
      <td>0.447158</td>
      <td>0.637919</td>
      <td>0.637933</td>
      <td>0.431140</td>
      <td>0.431641</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>54</th>
      <td>0.432009</td>
      <td>0.425618</td>
      <td>0.455466</td>
      <td>0.455244</td>
      <td>0.644543</td>
      <td>0.643218</td>
      <td>0.453959</td>
      <td>0.453127</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>55</th>
      <td>0.433546</td>
      <td>0.427289</td>
      <td>0.449503</td>
      <td>0.451235</td>
      <td>0.642273</td>
      <td>0.641356</td>
      <td>0.431301</td>
      <td>0.431733</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>56</th>
      <td>0.435751</td>
      <td>0.429486</td>
      <td>0.457395</td>
      <td>0.458204</td>
      <td>0.648364</td>
      <td>0.647179</td>
      <td>0.446376</td>
      <td>0.446102</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>56 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 7, 3)
Test shape: (10292, 7, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1408)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          704500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 843,243
Trainable params: 843,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 68ms/step - loss: 1.0830 - accuracy: 0.3927 - auc: 0.5719 - f1_score: 0.4211 - val_loss: 1.0712 - val_accuracy: 0.4096 - val_auc: 0.5938 - val_f1_score: 0.3903
Epoch 2/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0653 - accuracy: 0.4179 - auc: 0.6021 - f1_score: 0.4060 - val_loss: 1.0632 - val_accuracy: 0.4195 - val_auc: 0.6062 - val_f1_score: 0.4059
Epoch 3/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0559 - accuracy: 0.4296 - auc: 0.6174 - f1_score: 0.4183 - val_loss: 1.0581 - val_accuracy: 0.4276 - val_auc: 0.6145 - val_f1_score: 0.4182
Epoch 4/50
188/188 [==============================] - 15s 82ms/step - loss: 1.0504 - accuracy: 0.4336 - auc: 0.6243 - f1_score: 0.4229 - val_loss: 1.0587 - val_accuracy: 0.4269 - val_auc: 0.6137 - val_f1_score: 0.4130
Epoch 5/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0466 - accuracy: 0.4441 - auc: 0.6308 - f1_score: 0.4352 - val_loss: 1.0566 - val_accuracy: 0.4293 - val_auc: 0.6163 - val_f1_score: 0.4224
Epoch 6/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0438 - accuracy: 0.4449 - auc: 0.6339 - f1_score: 0.4381 - val_loss: 1.0553 - val_accuracy: 0.4320 - val_auc: 0.6185 - val_f1_score: 0.4198
Epoch 7/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0428 - accuracy: 0.4474 - auc: 0.6357 - f1_score: 0.4379 - val_loss: 1.0579 - val_accuracy: 0.4289 - val_auc: 0.6151 - val_f1_score: 0.4146
Epoch 8/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0415 - accuracy: 0.4492 - auc: 0.6376 - f1_score: 0.4411 - val_loss: 1.0590 - val_accuracy: 0.4252 - val_auc: 0.6153 - val_f1_score: 0.4121
Epoch 9/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0392 - accuracy: 0.4497 - auc: 0.6402 - f1_score: 0.4440 - val_loss: 1.0575 - val_accuracy: 0.4264 - val_auc: 0.6168 - val_f1_score: 0.4105
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>53</th>
      <td>0.431460</td>
      <td>0.425422</td>
      <td>0.445394</td>
      <td>0.447158</td>
      <td>0.637919</td>
      <td>0.637933</td>
      <td>0.431140</td>
      <td>0.431641</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>54</th>
      <td>0.432009</td>
      <td>0.425618</td>
      <td>0.455466</td>
      <td>0.455244</td>
      <td>0.644543</td>
      <td>0.643218</td>
      <td>0.453959</td>
      <td>0.453127</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>55</th>
      <td>0.433546</td>
      <td>0.427289</td>
      <td>0.449503</td>
      <td>0.451235</td>
      <td>0.642273</td>
      <td>0.641356</td>
      <td>0.431301</td>
      <td>0.431733</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>56</th>
      <td>0.435751</td>
      <td>0.429486</td>
      <td>0.457395</td>
      <td>0.458204</td>
      <td>0.648364</td>
      <td>0.647179</td>
      <td>0.446376</td>
      <td>0.446102</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>57</th>
      <td>0.438593</td>
      <td>0.432070</td>
      <td>0.462431</td>
      <td>0.462212</td>
      <td>0.652948</td>
      <td>0.650993</td>
      <td>0.447286</td>
      <td>0.445852</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>57 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8, 3)
Test shape: (10292, 8, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1608)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          804500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 943,243
Trainable params: 943,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 19s 85ms/step - loss: 1.0813 - accuracy: 0.3921 - auc: 0.5717 - f1_score: 0.4199 - val_loss: 1.0719 - val_accuracy: 0.4103 - val_auc: 0.5916 - val_f1_score: 0.3907
Epoch 2/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0643 - accuracy: 0.4223 - auc: 0.6053 - f1_score: 0.4103 - val_loss: 1.0650 - val_accuracy: 0.4199 - val_auc: 0.6029 - val_f1_score: 0.4050
Epoch 3/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0568 - accuracy: 0.4289 - auc: 0.6161 - f1_score: 0.4212 - val_loss: 1.0612 - val_accuracy: 0.4189 - val_auc: 0.6082 - val_f1_score: 0.3976
Epoch 4/50
188/188 [==============================] - 15s 79ms/step - loss: 1.0511 - accuracy: 0.4364 - auc: 0.6237 - f1_score: 0.4276 - val_loss: 1.0575 - val_accuracy: 0.4234 - val_auc: 0.6141 - val_f1_score: 0.4056
Epoch 5/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0479 - accuracy: 0.4389 - auc: 0.6277 - f1_score: 0.4307 - val_loss: 1.0566 - val_accuracy: 0.4342 - val_auc: 0.6169 - val_f1_score: 0.4239
Epoch 6/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0453 - accuracy: 0.4423 - auc: 0.6316 - f1_score: 0.4355 - val_loss: 1.0566 - val_accuracy: 0.4309 - val_auc: 0.6168 - val_f1_score: 0.4205
Epoch 7/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0433 - accuracy: 0.4436 - auc: 0.6341 - f1_score: 0.4377 - val_loss: 1.0571 - val_accuracy: 0.4297 - val_auc: 0.6171 - val_f1_score: 0.4176
Epoch 8/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0421 - accuracy: 0.4458 - auc: 0.6357 - f1_score: 0.4396 - val_loss: 1.0588 - val_accuracy: 0.4223 - val_auc: 0.6137 - val_f1_score: 0.4144
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>54</th>
      <td>0.432009</td>
      <td>0.425618</td>
      <td>0.455466</td>
      <td>0.455244</td>
      <td>0.644543</td>
      <td>0.643218</td>
      <td>0.453959</td>
      <td>0.453127</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>55</th>
      <td>0.433546</td>
      <td>0.427289</td>
      <td>0.449503</td>
      <td>0.451235</td>
      <td>0.642273</td>
      <td>0.641356</td>
      <td>0.431301</td>
      <td>0.431733</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>56</th>
      <td>0.435751</td>
      <td>0.429486</td>
      <td>0.457395</td>
      <td>0.458204</td>
      <td>0.648364</td>
      <td>0.647179</td>
      <td>0.446376</td>
      <td>0.446102</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>57</th>
      <td>0.438593</td>
      <td>0.432070</td>
      <td>0.462431</td>
      <td>0.462212</td>
      <td>0.652948</td>
      <td>0.650993</td>
      <td>0.447286</td>
      <td>0.445852</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.436751</td>
      <td>0.430123</td>
      <td>0.459150</td>
      <td>0.458802</td>
      <td>0.650716</td>
      <td>0.648437</td>
      <td>0.452011</td>
      <td>0.450682</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>58 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 9, 3)
Test shape: (10292, 9, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 3)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,043,243
Trainable params: 1,043,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 18s 81ms/step - loss: 1.0883 - accuracy: 0.3853 - auc: 0.5617 - f1_score: 0.4199 - val_loss: 1.0747 - val_accuracy: 0.3950 - val_auc: 0.5862 - val_f1_score: 0.3887
Epoch 2/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0661 - accuracy: 0.4161 - auc: 0.6012 - f1_score: 0.4061 - val_loss: 1.0681 - val_accuracy: 0.4131 - val_auc: 0.5982 - val_f1_score: 0.4045
Epoch 3/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0588 - accuracy: 0.4254 - auc: 0.6123 - f1_score: 0.4149 - val_loss: 1.0621 - val_accuracy: 0.4170 - val_auc: 0.6076 - val_f1_score: 0.4118
Epoch 4/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0525 - accuracy: 0.4330 - auc: 0.6210 - f1_score: 0.4220 - val_loss: 1.0609 - val_accuracy: 0.4192 - val_auc: 0.6085 - val_f1_score: 0.4067
Epoch 5/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0496 - accuracy: 0.4389 - auc: 0.6256 - f1_score: 0.4288 - val_loss: 1.0563 - val_accuracy: 0.4233 - val_auc: 0.6148 - val_f1_score: 0.4163
Epoch 6/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0459 - accuracy: 0.4409 - auc: 0.6301 - f1_score: 0.4337 - val_loss: 1.0582 - val_accuracy: 0.4264 - val_auc: 0.6133 - val_f1_score: 0.4183
Epoch 7/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0440 - accuracy: 0.4439 - auc: 0.6328 - f1_score: 0.4336 - val_loss: 1.0580 - val_accuracy: 0.4239 - val_auc: 0.6152 - val_f1_score: 0.4129
Epoch 8/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0419 - accuracy: 0.4459 - auc: 0.6364 - f1_score: 0.4380 - val_loss: 1.0566 - val_accuracy: 0.4260 - val_auc: 0.6163 - val_f1_score: 0.4228
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>55</th>
      <td>0.433546</td>
      <td>0.427289</td>
      <td>0.449503</td>
      <td>0.451235</td>
      <td>0.642273</td>
      <td>0.641356</td>
      <td>0.431301</td>
      <td>0.431733</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>56</th>
      <td>0.435751</td>
      <td>0.429486</td>
      <td>0.457395</td>
      <td>0.458204</td>
      <td>0.648364</td>
      <td>0.647179</td>
      <td>0.446376</td>
      <td>0.446102</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>57</th>
      <td>0.438593</td>
      <td>0.432070</td>
      <td>0.462431</td>
      <td>0.462212</td>
      <td>0.652948</td>
      <td>0.650993</td>
      <td>0.447286</td>
      <td>0.445852</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.436751</td>
      <td>0.430123</td>
      <td>0.459150</td>
      <td>0.458802</td>
      <td>0.650716</td>
      <td>0.648437</td>
      <td>0.452011</td>
      <td>0.450682</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>59</th>
      <td>0.434331</td>
      <td>0.427698</td>
      <td>0.458671</td>
      <td>0.458018</td>
      <td>0.651064</td>
      <td>0.648594</td>
      <td>0.454947</td>
      <td>0.453487</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>59 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 10, 3)
Test shape: (10292, 10, 3)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 3)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10300       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10300       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,143,243
Trainable params: 1,143,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 19s 86ms/step - loss: 1.0789 - accuracy: 0.3945 - auc: 0.5781 - f1_score: 0.4231 - val_loss: 1.0710 - val_accuracy: 0.4077 - val_auc: 0.5934 - val_f1_score: 0.3590
Epoch 2/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0632 - accuracy: 0.4184 - auc: 0.6052 - f1_score: 0.3839 - val_loss: 1.0643 - val_accuracy: 0.4180 - val_auc: 0.6051 - val_f1_score: 0.3691
Epoch 3/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0551 - accuracy: 0.4316 - auc: 0.6181 - f1_score: 0.4030 - val_loss: 1.0607 - val_accuracy: 0.4209 - val_auc: 0.6089 - val_f1_score: 0.3996
Epoch 4/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0504 - accuracy: 0.4368 - auc: 0.6250 - f1_score: 0.4084 - val_loss: 1.0588 - val_accuracy: 0.4254 - val_auc: 0.6127 - val_f1_score: 0.3923
Epoch 5/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0463 - accuracy: 0.4399 - auc: 0.6302 - f1_score: 0.4178 - val_loss: 1.0592 - val_accuracy: 0.4248 - val_auc: 0.6126 - val_f1_score: 0.4001
Epoch 6/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0431 - accuracy: 0.4450 - auc: 0.6345 - f1_score: 0.4237 - val_loss: 1.0588 - val_accuracy: 0.4206 - val_auc: 0.6140 - val_f1_score: 0.3976
Epoch 7/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0415 - accuracy: 0.4456 - auc: 0.6364 - f1_score: 0.4288 - val_loss: 1.0584 - val_accuracy: 0.4204 - val_auc: 0.6139 - val_f1_score: 0.4009
Epoch 8/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0390 - accuracy: 0.4476 - auc: 0.6397 - f1_score: 0.4300 - val_loss: 1.0578 - val_accuracy: 0.4230 - val_auc: 0.6150 - val_f1_score: 0.4108
Epoch 9/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0375 - accuracy: 0.4518 - auc: 0.6420 - f1_score: 0.4375 - val_loss: 1.0589 - val_accuracy: 0.4224 - val_auc: 0.6130 - val_f1_score: 0.4135
Epoch 10/50
188/188 [==============================] - 18s 93ms/step - loss: 1.0353 - accuracy: 0.4562 - auc: 0.6451 - f1_score: 0.4434 - val_loss: 1.0584 - val_accuracy: 0.4279 - val_auc: 0.6153 - val_f1_score: 0.4216
Epoch 11/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0339 - accuracy: 0.4568 - auc: 0.6466 - f1_score: 0.4443 - val_loss: 1.0593 - val_accuracy: 0.4260 - val_auc: 0.6146 - val_f1_score: 0.4168
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>56</th>
      <td>0.435751</td>
      <td>0.429486</td>
      <td>0.457395</td>
      <td>0.458204</td>
      <td>0.648364</td>
      <td>0.647179</td>
      <td>0.446376</td>
      <td>0.446102</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>57</th>
      <td>0.438593</td>
      <td>0.432070</td>
      <td>0.462431</td>
      <td>0.462212</td>
      <td>0.652948</td>
      <td>0.650993</td>
      <td>0.447286</td>
      <td>0.445852</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.436751</td>
      <td>0.430123</td>
      <td>0.459150</td>
      <td>0.458802</td>
      <td>0.650716</td>
      <td>0.648437</td>
      <td>0.452011</td>
      <td>0.450682</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>59</th>
      <td>0.434331</td>
      <td>0.427698</td>
      <td>0.458671</td>
      <td>0.458018</td>
      <td>0.651064</td>
      <td>0.648594</td>
      <td>0.454947</td>
      <td>0.453487</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>60</th>
      <td>0.441610</td>
      <td>0.434677</td>
      <td>0.471751</td>
      <td>0.470249</td>
      <td>0.663292</td>
      <td>0.659855</td>
      <td>0.461930</td>
      <td>0.459346</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
<p>60 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 202)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          101500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 301,243
Trainable params: 301,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 8s 33ms/step - loss: 1.0915 - accuracy: 0.3633 - auc: 0.5493 - f1_score: 0.4231 - val_loss: 1.0855 - val_accuracy: 0.3885 - val_auc: 0.5675 - val_f1_score: 0.3141
Epoch 2/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0850 - accuracy: 0.3911 - auc: 0.5682 - f1_score: 0.3458 - val_loss: 1.0832 - val_accuracy: 0.3879 - val_auc: 0.5723 - val_f1_score: 0.3362
Epoch 3/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0843 - accuracy: 0.3915 - auc: 0.5700 - f1_score: 0.3479 - val_loss: 1.0829 - val_accuracy: 0.3868 - val_auc: 0.5733 - val_f1_score: 0.3316
Epoch 4/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0843 - accuracy: 0.3911 - auc: 0.5698 - f1_score: 0.3401 - val_loss: 1.0828 - val_accuracy: 0.3904 - val_auc: 0.5744 - val_f1_score: 0.3457
Epoch 5/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0834 - accuracy: 0.3931 - auc: 0.5719 - f1_score: 0.3573 - val_loss: 1.0829 - val_accuracy: 0.3893 - val_auc: 0.5735 - val_f1_score: 0.3392
Epoch 6/50
188/188 [==============================] - 5s 25ms/step - loss: 1.0835 - accuracy: 0.3913 - auc: 0.5714 - f1_score: 0.3531 - val_loss: 1.0829 - val_accuracy: 0.3915 - val_auc: 0.5741 - val_f1_score: 0.3499
Epoch 7/50
188/188 [==============================] - 5s 25ms/step - loss: 1.0838 - accuracy: 0.3919 - auc: 0.5716 - f1_score: 0.3463 - val_loss: 1.0827 - val_accuracy: 0.3914 - val_auc: 0.5744 - val_f1_score: 0.3414
Epoch 8/50
188/188 [==============================] - 5s 25ms/step - loss: 1.0834 - accuracy: 0.3914 - auc: 0.5720 - f1_score: 0.3501 - val_loss: 1.0828 - val_accuracy: 0.3920 - val_auc: 0.5745 - val_f1_score: 0.3490
Epoch 9/50
188/188 [==============================] - 5s 24ms/step - loss: 1.0833 - accuracy: 0.3929 - auc: 0.5725 - f1_score: 0.3500 - val_loss: 1.0828 - val_accuracy: 0.3918 - val_auc: 0.5747 - val_f1_score: 0.3410
Epoch 10/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0829 - accuracy: 0.3934 - auc: 0.5732 - f1_score: 0.3549 - val_loss: 1.0829 - val_accuracy: 0.3911 - val_auc: 0.5744 - val_f1_score: 0.3456
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>57</th>
      <td>0.438593</td>
      <td>0.432070</td>
      <td>0.462431</td>
      <td>0.462212</td>
      <td>0.652948</td>
      <td>0.650993</td>
      <td>0.447286</td>
      <td>0.445852</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.436751</td>
      <td>0.430123</td>
      <td>0.459150</td>
      <td>0.458802</td>
      <td>0.650716</td>
      <td>0.648437</td>
      <td>0.452011</td>
      <td>0.450682</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>59</th>
      <td>0.434331</td>
      <td>0.427698</td>
      <td>0.458671</td>
      <td>0.458018</td>
      <td>0.651064</td>
      <td>0.648594</td>
      <td>0.454947</td>
      <td>0.453487</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>60</th>
      <td>0.441610</td>
      <td>0.434677</td>
      <td>0.471751</td>
      <td>0.470249</td>
      <td>0.663292</td>
      <td>0.659855</td>
      <td>0.461930</td>
      <td>0.459346</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>61</th>
      <td>0.410930</td>
      <td>0.404286</td>
      <td>0.393693</td>
      <td>0.397595</td>
      <td>0.576263</td>
      <td>0.576945</td>
      <td>0.347596</td>
      <td>0.349436</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>61 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 402)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          201500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 401,243
Trainable params: 401,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 49ms/step - loss: 1.0843 - accuracy: 0.3947 - auc: 0.5705 - f1_score: 0.3422 - val_loss: 1.0793 - val_accuracy: 0.3966 - val_auc: 0.5820 - val_f1_score: 0.3403
Epoch 2/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0787 - accuracy: 0.4016 - auc: 0.5814 - f1_score: 0.3664 - val_loss: 1.0782 - val_accuracy: 0.4013 - val_auc: 0.5836 - val_f1_score: 0.3811
Epoch 3/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0779 - accuracy: 0.4015 - auc: 0.5831 - f1_score: 0.3819 - val_loss: 1.0776 - val_accuracy: 0.4000 - val_auc: 0.5844 - val_f1_score: 0.3831
Epoch 4/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0777 - accuracy: 0.4010 - auc: 0.5835 - f1_score: 0.3854 - val_loss: 1.0777 - val_accuracy: 0.4025 - val_auc: 0.5848 - val_f1_score: 0.3906
Epoch 5/50
188/188 [==============================] - 11s 56ms/step - loss: 1.0773 - accuracy: 0.4024 - auc: 0.5846 - f1_score: 0.3884 - val_loss: 1.0771 - val_accuracy: 0.4024 - val_auc: 0.5859 - val_f1_score: 0.3929
Epoch 6/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0772 - accuracy: 0.4025 - auc: 0.5849 - f1_score: 0.3907 - val_loss: 1.0781 - val_accuracy: 0.4014 - val_auc: 0.5845 - val_f1_score: 0.3969
Epoch 7/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0773 - accuracy: 0.4043 - auc: 0.5845 - f1_score: 0.3949 - val_loss: 1.0778 - val_accuracy: 0.3999 - val_auc: 0.5852 - val_f1_score: 0.3938
Epoch 8/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0769 - accuracy: 0.4052 - auc: 0.5860 - f1_score: 0.3933 - val_loss: 1.0773 - val_accuracy: 0.4065 - val_auc: 0.5862 - val_f1_score: 0.4029
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>58</th>
      <td>0.436751</td>
      <td>0.430123</td>
      <td>0.459150</td>
      <td>0.458802</td>
      <td>0.650716</td>
      <td>0.648437</td>
      <td>0.452011</td>
      <td>0.450682</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>59</th>
      <td>0.434331</td>
      <td>0.427698</td>
      <td>0.458671</td>
      <td>0.458018</td>
      <td>0.651064</td>
      <td>0.648594</td>
      <td>0.454947</td>
      <td>0.453487</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>60</th>
      <td>0.441610</td>
      <td>0.434677</td>
      <td>0.471751</td>
      <td>0.470249</td>
      <td>0.663292</td>
      <td>0.659855</td>
      <td>0.461930</td>
      <td>0.459346</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>61</th>
      <td>0.410930</td>
      <td>0.404286</td>
      <td>0.393693</td>
      <td>0.397595</td>
      <td>0.576263</td>
      <td>0.576945</td>
      <td>0.347596</td>
      <td>0.349436</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>62</th>
      <td>0.413318</td>
      <td>0.406691</td>
      <td>0.408496</td>
      <td>0.409454</td>
      <td>0.590161</td>
      <td>0.589968</td>
      <td>0.404763</td>
      <td>0.405096</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>62 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 602)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          301500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 501,243
Trainable params: 501,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 69ms/step - loss: 1.0859 - accuracy: 0.3920 - auc: 0.5697 - f1_score: 0.3991 - val_loss: 1.0746 - val_accuracy: 0.4066 - val_auc: 0.5900 - val_f1_score: 0.3880
Epoch 2/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0744 - accuracy: 0.4076 - auc: 0.5897 - f1_score: 0.3976 - val_loss: 1.0730 - val_accuracy: 0.4058 - val_auc: 0.5920 - val_f1_score: 0.3994
Epoch 3/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0736 - accuracy: 0.4084 - auc: 0.5915 - f1_score: 0.4005 - val_loss: 1.0718 - val_accuracy: 0.4077 - val_auc: 0.5943 - val_f1_score: 0.3935
Epoch 4/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0728 - accuracy: 0.4068 - auc: 0.5916 - f1_score: 0.3996 - val_loss: 1.0721 - val_accuracy: 0.4090 - val_auc: 0.5937 - val_f1_score: 0.3954
Epoch 5/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0729 - accuracy: 0.4081 - auc: 0.5921 - f1_score: 0.4002 - val_loss: 1.0717 - val_accuracy: 0.4083 - val_auc: 0.5939 - val_f1_score: 0.4001
Epoch 6/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0727 - accuracy: 0.4087 - auc: 0.5930 - f1_score: 0.4008 - val_loss: 1.0721 - val_accuracy: 0.4058 - val_auc: 0.5934 - val_f1_score: 0.3898
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>59</th>
      <td>0.434331</td>
      <td>0.427698</td>
      <td>0.458671</td>
      <td>0.458018</td>
      <td>0.651064</td>
      <td>0.648594</td>
      <td>0.454947</td>
      <td>0.453487</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>60</th>
      <td>0.441610</td>
      <td>0.434677</td>
      <td>0.471751</td>
      <td>0.470249</td>
      <td>0.663292</td>
      <td>0.659855</td>
      <td>0.461930</td>
      <td>0.459346</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>61</th>
      <td>0.410930</td>
      <td>0.404286</td>
      <td>0.393693</td>
      <td>0.397595</td>
      <td>0.576263</td>
      <td>0.576945</td>
      <td>0.347596</td>
      <td>0.349436</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>62</th>
      <td>0.413318</td>
      <td>0.406691</td>
      <td>0.408496</td>
      <td>0.409454</td>
      <td>0.590161</td>
      <td>0.589968</td>
      <td>0.404763</td>
      <td>0.405096</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.417086</td>
      <td>0.410738</td>
      <td>0.413859</td>
      <td>0.416638</td>
      <td>0.600008</td>
      <td>0.601063</td>
      <td>0.397879</td>
      <td>0.399353</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>63 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 802)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          401500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 601,243
Trainable params: 601,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 19s 77ms/step - loss: 1.0807 - accuracy: 0.4000 - auc: 0.5777 - f1_score: 0.3950 - val_loss: 1.0690 - val_accuracy: 0.4082 - val_auc: 0.5998 - val_f1_score: 0.3434
Epoch 2/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0705 - accuracy: 0.4119 - auc: 0.5951 - f1_score: 0.3698 - val_loss: 1.0668 - val_accuracy: 0.4134 - val_auc: 0.6017 - val_f1_score: 0.3840
Epoch 3/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0686 - accuracy: 0.4138 - auc: 0.5987 - f1_score: 0.3901 - val_loss: 1.0669 - val_accuracy: 0.4127 - val_auc: 0.6025 - val_f1_score: 0.3948
Epoch 4/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0687 - accuracy: 0.4143 - auc: 0.5989 - f1_score: 0.3994 - val_loss: 1.0665 - val_accuracy: 0.4170 - val_auc: 0.6035 - val_f1_score: 0.4041
Epoch 5/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0682 - accuracy: 0.4156 - auc: 0.5996 - f1_score: 0.4020 - val_loss: 1.0666 - val_accuracy: 0.4168 - val_auc: 0.6026 - val_f1_score: 0.4006
Epoch 6/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0674 - accuracy: 0.4160 - auc: 0.6010 - f1_score: 0.4026 - val_loss: 1.0670 - val_accuracy: 0.4164 - val_auc: 0.6027 - val_f1_score: 0.4064
Epoch 7/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0672 - accuracy: 0.4163 - auc: 0.6012 - f1_score: 0.4036 - val_loss: 1.0670 - val_accuracy: 0.4103 - val_auc: 0.6009 - val_f1_score: 0.3841
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>60</th>
      <td>0.441610</td>
      <td>0.434677</td>
      <td>0.471751</td>
      <td>0.470249</td>
      <td>0.663292</td>
      <td>0.659855</td>
      <td>0.461930</td>
      <td>0.459346</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <th>61</th>
      <td>0.410930</td>
      <td>0.404286</td>
      <td>0.393693</td>
      <td>0.397595</td>
      <td>0.576263</td>
      <td>0.576945</td>
      <td>0.347596</td>
      <td>0.349436</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>62</th>
      <td>0.413318</td>
      <td>0.406691</td>
      <td>0.408496</td>
      <td>0.409454</td>
      <td>0.590161</td>
      <td>0.589968</td>
      <td>0.404763</td>
      <td>0.405096</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.417086</td>
      <td>0.410738</td>
      <td>0.413859</td>
      <td>0.416638</td>
      <td>0.600008</td>
      <td>0.601063</td>
      <td>0.397879</td>
      <td>0.399353</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>64</th>
      <td>0.419841</td>
      <td>0.413588</td>
      <td>0.420715</td>
      <td>0.423881</td>
      <td>0.608033</td>
      <td>0.609166</td>
      <td>0.395096</td>
      <td>0.396718</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>64 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          501500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 701,243
Trainable params: 701,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 22s 100ms/step - loss: 1.0908 - accuracy: 0.3797 - auc: 0.5499 - f1_score: 0.4002 - val_loss: 1.0800 - val_accuracy: 0.4080 - val_auc: 0.5846 - val_f1_score: 0.3755
Epoch 2/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0745 - accuracy: 0.4130 - auc: 0.5910 - f1_score: 0.3972 - val_loss: 1.0656 - val_accuracy: 0.4175 - val_auc: 0.6047 - val_f1_score: 0.4051
Epoch 3/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0675 - accuracy: 0.4159 - auc: 0.6009 - f1_score: 0.4070 - val_loss: 1.0652 - val_accuracy: 0.4169 - val_auc: 0.6047 - val_f1_score: 0.3992
Epoch 4/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0668 - accuracy: 0.4175 - auc: 0.6023 - f1_score: 0.4078 - val_loss: 1.0651 - val_accuracy: 0.4157 - val_auc: 0.6049 - val_f1_score: 0.4057
Epoch 5/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0660 - accuracy: 0.4197 - auc: 0.6040 - f1_score: 0.4120 - val_loss: 1.0647 - val_accuracy: 0.4177 - val_auc: 0.6048 - val_f1_score: 0.4105
Epoch 6/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0653 - accuracy: 0.4223 - auc: 0.6049 - f1_score: 0.4155 - val_loss: 1.0637 - val_accuracy: 0.4188 - val_auc: 0.6063 - val_f1_score: 0.4085
Epoch 7/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0639 - accuracy: 0.4219 - auc: 0.6067 - f1_score: 0.4166 - val_loss: 1.0639 - val_accuracy: 0.4171 - val_auc: 0.6053 - val_f1_score: 0.4069
Epoch 8/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0638 - accuracy: 0.4219 - auc: 0.6072 - f1_score: 0.4168 - val_loss: 1.0642 - val_accuracy: 0.4124 - val_auc: 0.6047 - val_f1_score: 0.3957
Epoch 9/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0637 - accuracy: 0.4241 - auc: 0.6079 - f1_score: 0.4172 - val_loss: 1.0644 - val_accuracy: 0.4174 - val_auc: 0.6047 - val_f1_score: 0.4103
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>61</th>
      <td>0.410930</td>
      <td>0.404286</td>
      <td>0.393693</td>
      <td>0.397595</td>
      <td>0.576263</td>
      <td>0.576945</td>
      <td>0.347596</td>
      <td>0.349436</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>62</th>
      <td>0.413318</td>
      <td>0.406691</td>
      <td>0.408496</td>
      <td>0.409454</td>
      <td>0.590161</td>
      <td>0.589968</td>
      <td>0.404763</td>
      <td>0.405096</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.417086</td>
      <td>0.410738</td>
      <td>0.413859</td>
      <td>0.416638</td>
      <td>0.600008</td>
      <td>0.601063</td>
      <td>0.397879</td>
      <td>0.399353</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>64</th>
      <td>0.419841</td>
      <td>0.413588</td>
      <td>0.420715</td>
      <td>0.423881</td>
      <td>0.608033</td>
      <td>0.609166</td>
      <td>0.395096</td>
      <td>0.396718</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>65</th>
      <td>0.422192</td>
      <td>0.415801</td>
      <td>0.429109</td>
      <td>0.430761</td>
      <td>0.615495</td>
      <td>0.615413</td>
      <td>0.422091</td>
      <td>0.422747</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>65 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1202)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          601500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 801,243
Trainable params: 801,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 25s 113ms/step - loss: 1.0778 - accuracy: 0.4059 - auc: 0.5854 - f1_score: 0.4121 - val_loss: 1.0642 - val_accuracy: 0.4181 - val_auc: 0.6060 - val_f1_score: 0.3961
Epoch 2/50
188/188 [==============================] - 20s 108ms/step - loss: 1.0648 - accuracy: 0.4194 - auc: 0.6055 - f1_score: 0.4060 - val_loss: 1.0633 - val_accuracy: 0.4168 - val_auc: 0.6065 - val_f1_score: 0.3927
Epoch 3/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0636 - accuracy: 0.4226 - auc: 0.6074 - f1_score: 0.4090 - val_loss: 1.0623 - val_accuracy: 0.4194 - val_auc: 0.6080 - val_f1_score: 0.4070
Epoch 4/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0634 - accuracy: 0.4223 - auc: 0.6078 - f1_score: 0.4114 - val_loss: 1.0625 - val_accuracy: 0.4156 - val_auc: 0.6080 - val_f1_score: 0.4058
Epoch 5/50
188/188 [==============================] - 20s 104ms/step - loss: 1.0626 - accuracy: 0.4235 - auc: 0.6087 - f1_score: 0.4147 - val_loss: 1.0623 - val_accuracy: 0.4157 - val_auc: 0.6077 - val_f1_score: 0.4070
Epoch 6/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0619 - accuracy: 0.4249 - auc: 0.6099 - f1_score: 0.4170 - val_loss: 1.0625 - val_accuracy: 0.4197 - val_auc: 0.6081 - val_f1_score: 0.4183
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>62</th>
      <td>0.413318</td>
      <td>0.406691</td>
      <td>0.408496</td>
      <td>0.409454</td>
      <td>0.590161</td>
      <td>0.589968</td>
      <td>0.404763</td>
      <td>0.405096</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.417086</td>
      <td>0.410738</td>
      <td>0.413859</td>
      <td>0.416638</td>
      <td>0.600008</td>
      <td>0.601063</td>
      <td>0.397879</td>
      <td>0.399353</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>64</th>
      <td>0.419841</td>
      <td>0.413588</td>
      <td>0.420715</td>
      <td>0.423881</td>
      <td>0.608033</td>
      <td>0.609166</td>
      <td>0.395096</td>
      <td>0.396718</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>65</th>
      <td>0.422192</td>
      <td>0.415801</td>
      <td>0.429109</td>
      <td>0.430761</td>
      <td>0.615495</td>
      <td>0.615413</td>
      <td>0.422091</td>
      <td>0.422747</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>66</th>
      <td>0.421293</td>
      <td>0.414861</td>
      <td>0.432106</td>
      <td>0.432653</td>
      <td>0.618415</td>
      <td>0.617832</td>
      <td>0.430306</td>
      <td>0.430218</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>66 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1402)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          701500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 901,243
Trainable params: 901,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 28s 123ms/step - loss: 1.0780 - accuracy: 0.4116 - auc: 0.5793 - f1_score: 0.4048 - val_loss: 1.0663 - val_accuracy: 0.4152 - val_auc: 0.6024 - val_f1_score: 0.3389
Epoch 2/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0642 - accuracy: 0.4215 - auc: 0.6059 - f1_score: 0.3639 - val_loss: 1.0628 - val_accuracy: 0.4169 - val_auc: 0.6063 - val_f1_score: 0.3688
Epoch 3/50
188/188 [==============================] - 23s 123ms/step - loss: 1.0617 - accuracy: 0.4234 - auc: 0.6096 - f1_score: 0.3853 - val_loss: 1.0620 - val_accuracy: 0.4175 - val_auc: 0.6074 - val_f1_score: 0.3880
Epoch 4/50
188/188 [==============================] - 24s 127ms/step - loss: 1.0613 - accuracy: 0.4258 - auc: 0.6113 - f1_score: 0.4009 - val_loss: 1.0618 - val_accuracy: 0.4219 - val_auc: 0.6092 - val_f1_score: 0.4028
Epoch 5/50
188/188 [==============================] - 23s 122ms/step - loss: 1.0605 - accuracy: 0.4262 - auc: 0.6123 - f1_score: 0.4060 - val_loss: 1.0621 - val_accuracy: 0.4179 - val_auc: 0.6086 - val_f1_score: 0.4021
Epoch 6/50
188/188 [==============================] - 23s 124ms/step - loss: 1.0584 - accuracy: 0.4277 - auc: 0.6145 - f1_score: 0.4117 - val_loss: 1.0613 - val_accuracy: 0.4186 - val_auc: 0.6087 - val_f1_score: 0.4051
Epoch 7/50
188/188 [==============================] - 24s 126ms/step - loss: 1.0587 - accuracy: 0.4288 - auc: 0.6148 - f1_score: 0.4155 - val_loss: 1.0616 - val_accuracy: 0.4189 - val_auc: 0.6085 - val_f1_score: 0.4119
Epoch 8/50
188/188 [==============================] - 23s 124ms/step - loss: 1.0583 - accuracy: 0.4299 - auc: 0.6149 - f1_score: 0.4197 - val_loss: 1.0617 - val_accuracy: 0.4195 - val_auc: 0.6090 - val_f1_score: 0.4133
Epoch 9/50
188/188 [==============================] - 23s 120ms/step - loss: 1.0590 - accuracy: 0.4281 - auc: 0.6143 - f1_score: 0.4197 - val_loss: 1.0614 - val_accuracy: 0.4197 - val_auc: 0.6089 - val_f1_score: 0.4139
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>63</th>
      <td>0.417086</td>
      <td>0.410738</td>
      <td>0.413859</td>
      <td>0.416638</td>
      <td>0.600008</td>
      <td>0.601063</td>
      <td>0.397879</td>
      <td>0.399353</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>64</th>
      <td>0.419841</td>
      <td>0.413588</td>
      <td>0.420715</td>
      <td>0.423881</td>
      <td>0.608033</td>
      <td>0.609166</td>
      <td>0.395096</td>
      <td>0.396718</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>65</th>
      <td>0.422192</td>
      <td>0.415801</td>
      <td>0.429109</td>
      <td>0.430761</td>
      <td>0.615495</td>
      <td>0.615413</td>
      <td>0.422091</td>
      <td>0.422747</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>66</th>
      <td>0.421293</td>
      <td>0.414861</td>
      <td>0.432106</td>
      <td>0.432653</td>
      <td>0.618415</td>
      <td>0.617832</td>
      <td>0.430306</td>
      <td>0.430218</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>67</th>
      <td>0.423153</td>
      <td>0.416833</td>
      <td>0.434657</td>
      <td>0.436367</td>
      <td>0.622302</td>
      <td>0.622161</td>
      <td>0.427952</td>
      <td>0.428796</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>67 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1602)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          801500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,001,243
Trainable params: 1,001,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 30s 137ms/step - loss: 1.0826 - accuracy: 0.3736 - auc: 0.5681 - f1_score: 0.4073 - val_loss: 1.0751 - val_accuracy: 0.3854 - val_auc: 0.5835 - val_f1_score: 0.3278
Epoch 2/50
188/188 [==============================] - 24s 128ms/step - loss: 1.0672 - accuracy: 0.4140 - auc: 0.6013 - f1_score: 0.4113 - val_loss: 1.0632 - val_accuracy: 0.4125 - val_auc: 0.6054 - val_f1_score: 0.3877
Epoch 3/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0608 - accuracy: 0.4240 - auc: 0.6111 - f1_score: 0.4128 - val_loss: 1.0621 - val_accuracy: 0.4147 - val_auc: 0.6067 - val_f1_score: 0.4031
Epoch 4/50
188/188 [==============================] - 29s 155ms/step - loss: 1.0585 - accuracy: 0.4285 - auc: 0.6144 - f1_score: 0.4223 - val_loss: 1.0616 - val_accuracy: 0.4169 - val_auc: 0.6085 - val_f1_score: 0.4042
Epoch 5/50
188/188 [==============================] - 24s 129ms/step - loss: 1.0589 - accuracy: 0.4284 - auc: 0.6147 - f1_score: 0.4205 - val_loss: 1.0615 - val_accuracy: 0.4152 - val_auc: 0.6081 - val_f1_score: 0.4113
Epoch 6/50
188/188 [==============================] - 23s 125ms/step - loss: 1.0582 - accuracy: 0.4284 - auc: 0.6155 - f1_score: 0.4221 - val_loss: 1.0610 - val_accuracy: 0.4219 - val_auc: 0.6091 - val_f1_score: 0.4202
Epoch 7/50
188/188 [==============================] - 24s 127ms/step - loss: 1.0570 - accuracy: 0.4322 - auc: 0.6175 - f1_score: 0.4277 - val_loss: 1.0614 - val_accuracy: 0.4206 - val_auc: 0.6088 - val_f1_score: 0.4110
Epoch 8/50
188/188 [==============================] - 23s 124ms/step - loss: 1.0568 - accuracy: 0.4342 - auc: 0.6186 - f1_score: 0.4278 - val_loss: 1.0613 - val_accuracy: 0.4169 - val_auc: 0.6085 - val_f1_score: 0.4096
Epoch 9/50
188/188 [==============================] - 23s 123ms/step - loss: 1.0561 - accuracy: 0.4329 - auc: 0.6195 - f1_score: 0.4274 - val_loss: 1.0618 - val_accuracy: 0.4152 - val_auc: 0.6073 - val_f1_score: 0.4056
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>64</th>
      <td>0.419841</td>
      <td>0.413588</td>
      <td>0.420715</td>
      <td>0.423881</td>
      <td>0.608033</td>
      <td>0.609166</td>
      <td>0.395096</td>
      <td>0.396718</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>65</th>
      <td>0.422192</td>
      <td>0.415801</td>
      <td>0.429109</td>
      <td>0.430761</td>
      <td>0.615495</td>
      <td>0.615413</td>
      <td>0.422091</td>
      <td>0.422747</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>66</th>
      <td>0.421293</td>
      <td>0.414861</td>
      <td>0.432106</td>
      <td>0.432653</td>
      <td>0.618415</td>
      <td>0.617832</td>
      <td>0.430306</td>
      <td>0.430218</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>67</th>
      <td>0.423153</td>
      <td>0.416833</td>
      <td>0.434657</td>
      <td>0.436367</td>
      <td>0.622302</td>
      <td>0.622161</td>
      <td>0.427952</td>
      <td>0.428796</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>68</th>
      <td>0.425347</td>
      <td>0.419001</td>
      <td>0.441132</td>
      <td>0.442856</td>
      <td>0.628616</td>
      <td>0.628440</td>
      <td>0.431302</td>
      <td>0.432004</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>68 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1802)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          901500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,101,243
Trainable params: 1,101,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 36s 156ms/step - loss: 1.0750 - accuracy: 0.4165 - auc: 0.5893 - f1_score: 0.4117 - val_loss: 1.0637 - val_accuracy: 0.4175 - val_auc: 0.6052 - val_f1_score: 0.4038
Epoch 2/50
188/188 [==============================] - 27s 146ms/step - loss: 1.0612 - accuracy: 0.4272 - auc: 0.6112 - f1_score: 0.4180 - val_loss: 1.0613 - val_accuracy: 0.4223 - val_auc: 0.6091 - val_f1_score: 0.4170
Epoch 3/50
188/188 [==============================] - 27s 144ms/step - loss: 1.0592 - accuracy: 0.4292 - auc: 0.6140 - f1_score: 0.4233 - val_loss: 1.0613 - val_accuracy: 0.4224 - val_auc: 0.6092 - val_f1_score: 0.4176
Epoch 4/50
188/188 [==============================] - 27s 144ms/step - loss: 1.0583 - accuracy: 0.4288 - auc: 0.6151 - f1_score: 0.4231 - val_loss: 1.0609 - val_accuracy: 0.4193 - val_auc: 0.6097 - val_f1_score: 0.4142
Epoch 5/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0572 - accuracy: 0.4317 - auc: 0.6170 - f1_score: 0.4276 - val_loss: 1.0609 - val_accuracy: 0.4181 - val_auc: 0.6101 - val_f1_score: 0.4126
Epoch 6/50
188/188 [==============================] - 27s 145ms/step - loss: 1.0568 - accuracy: 0.4329 - auc: 0.6184 - f1_score: 0.4284 - val_loss: 1.0606 - val_accuracy: 0.4192 - val_auc: 0.6101 - val_f1_score: 0.4146
Epoch 7/50
188/188 [==============================] - 28s 149ms/step - loss: 1.0560 - accuracy: 0.4330 - auc: 0.6192 - f1_score: 0.4291 - val_loss: 1.0613 - val_accuracy: 0.4177 - val_auc: 0.6088 - val_f1_score: 0.4139
Epoch 8/50
188/188 [==============================] - 27s 146ms/step - loss: 1.0552 - accuracy: 0.4367 - auc: 0.6213 - f1_score: 0.4322 - val_loss: 1.0605 - val_accuracy: 0.4222 - val_auc: 0.6109 - val_f1_score: 0.4170
Epoch 9/50
188/188 [==============================] - 28s 148ms/step - loss: 1.0557 - accuracy: 0.4368 - auc: 0.6206 - f1_score: 0.4334 - val_loss: 1.0607 - val_accuracy: 0.4180 - val_auc: 0.6098 - val_f1_score: 0.4126
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>65</th>
      <td>0.422192</td>
      <td>0.415801</td>
      <td>0.429109</td>
      <td>0.430761</td>
      <td>0.615495</td>
      <td>0.615413</td>
      <td>0.422091</td>
      <td>0.422747</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>66</th>
      <td>0.421293</td>
      <td>0.414861</td>
      <td>0.432106</td>
      <td>0.432653</td>
      <td>0.618415</td>
      <td>0.617832</td>
      <td>0.430306</td>
      <td>0.430218</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>67</th>
      <td>0.423153</td>
      <td>0.416833</td>
      <td>0.434657</td>
      <td>0.436367</td>
      <td>0.622302</td>
      <td>0.622161</td>
      <td>0.427952</td>
      <td>0.428796</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>68</th>
      <td>0.425347</td>
      <td>0.419001</td>
      <td>0.441132</td>
      <td>0.442856</td>
      <td>0.628616</td>
      <td>0.628440</td>
      <td>0.431302</td>
      <td>0.432004</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>69</th>
      <td>0.426285</td>
      <td>0.419939</td>
      <td>0.444707</td>
      <td>0.445365</td>
      <td>0.630568</td>
      <td>0.629962</td>
      <td>0.438532</td>
      <td>0.438226</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>69 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1001500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,201,243
Trainable params: 1,201,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 36s 160ms/step - loss: 1.0794 - accuracy: 0.3804 - auc: 0.5771 - f1_score: 0.4146 - val_loss: 1.0712 - val_accuracy: 0.4149 - val_auc: 0.5993 - val_f1_score: 0.4134
Epoch 2/50
188/188 [==============================] - 29s 156ms/step - loss: 1.0625 - accuracy: 0.4237 - auc: 0.6100 - f1_score: 0.4145 - val_loss: 1.0612 - val_accuracy: 0.4210 - val_auc: 0.6111 - val_f1_score: 0.3991
Epoch 3/50
188/188 [==============================] - 31s 163ms/step - loss: 1.0581 - accuracy: 0.4320 - auc: 0.6156 - f1_score: 0.4171 - val_loss: 1.0596 - val_accuracy: 0.4222 - val_auc: 0.6132 - val_f1_score: 0.4050
Epoch 4/50
188/188 [==============================] - 29s 154ms/step - loss: 1.0566 - accuracy: 0.4322 - auc: 0.6180 - f1_score: 0.4195 - val_loss: 1.0597 - val_accuracy: 0.4223 - val_auc: 0.6128 - val_f1_score: 0.4114
Epoch 5/50
188/188 [==============================] - 31s 165ms/step - loss: 1.0554 - accuracy: 0.4337 - auc: 0.6200 - f1_score: 0.4264 - val_loss: 1.0597 - val_accuracy: 0.4248 - val_auc: 0.6130 - val_f1_score: 0.4212
Epoch 6/50
188/188 [==============================] - 30s 157ms/step - loss: 1.0540 - accuracy: 0.4370 - auc: 0.6227 - f1_score: 0.4319 - val_loss: 1.0601 - val_accuracy: 0.4230 - val_auc: 0.6125 - val_f1_score: 0.4159
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>66</th>
      <td>0.421293</td>
      <td>0.414861</td>
      <td>0.432106</td>
      <td>0.432653</td>
      <td>0.618415</td>
      <td>0.617832</td>
      <td>0.430306</td>
      <td>0.430218</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>67</th>
      <td>0.423153</td>
      <td>0.416833</td>
      <td>0.434657</td>
      <td>0.436367</td>
      <td>0.622302</td>
      <td>0.622161</td>
      <td>0.427952</td>
      <td>0.428796</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>68</th>
      <td>0.425347</td>
      <td>0.419001</td>
      <td>0.441132</td>
      <td>0.442856</td>
      <td>0.628616</td>
      <td>0.628440</td>
      <td>0.431302</td>
      <td>0.432004</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>69</th>
      <td>0.426285</td>
      <td>0.419939</td>
      <td>0.444707</td>
      <td>0.445365</td>
      <td>0.630568</td>
      <td>0.629962</td>
      <td>0.438532</td>
      <td>0.438226</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>70</th>
      <td>0.427374</td>
      <td>0.421171</td>
      <td>0.443661</td>
      <td>0.444747</td>
      <td>0.630587</td>
      <td>0.630568</td>
      <td>0.436326</td>
      <td>0.436371</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>70 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 202)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          101500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 240,043
Trainable params: 240,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 8s 28ms/step - loss: 1.0907 - accuracy: 0.3669 - auc: 0.5507 - f1_score: 0.4050 - val_loss: 1.0865 - val_accuracy: 0.3871 - val_auc: 0.5672 - val_f1_score: 0.3532
Epoch 2/50
188/188 [==============================] - 5s 24ms/step - loss: 1.0857 - accuracy: 0.3880 - auc: 0.5666 - f1_score: 0.3678 - val_loss: 1.0833 - val_accuracy: 0.3966 - val_auc: 0.5738 - val_f1_score: 0.3408
Epoch 3/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0845 - accuracy: 0.3894 - auc: 0.5694 - f1_score: 0.3616 - val_loss: 1.0827 - val_accuracy: 0.3958 - val_auc: 0.5751 - val_f1_score: 0.3445
Epoch 4/50
188/188 [==============================] - 5s 24ms/step - loss: 1.0837 - accuracy: 0.3913 - auc: 0.5712 - f1_score: 0.3531 - val_loss: 1.0826 - val_accuracy: 0.3924 - val_auc: 0.5752 - val_f1_score: 0.3222
Epoch 5/50
188/188 [==============================] - 5s 24ms/step - loss: 1.0838 - accuracy: 0.3899 - auc: 0.5708 - f1_score: 0.3514 - val_loss: 1.0823 - val_accuracy: 0.3920 - val_auc: 0.5752 - val_f1_score: 0.3354
Epoch 6/50
188/188 [==============================] - 5s 25ms/step - loss: 1.0836 - accuracy: 0.3915 - auc: 0.5717 - f1_score: 0.3487 - val_loss: 1.0819 - val_accuracy: 0.3902 - val_auc: 0.5759 - val_f1_score: 0.3537
Epoch 7/50
188/188 [==============================] - 5s 25ms/step - loss: 1.0837 - accuracy: 0.3926 - auc: 0.5716 - f1_score: 0.3602 - val_loss: 1.0824 - val_accuracy: 0.3869 - val_auc: 0.5743 - val_f1_score: 0.3504
Epoch 8/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0834 - accuracy: 0.3919 - auc: 0.5722 - f1_score: 0.3575 - val_loss: 1.0827 - val_accuracy: 0.3888 - val_auc: 0.5731 - val_f1_score: 0.3557
Epoch 9/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0836 - accuracy: 0.3911 - auc: 0.5724 - f1_score: 0.3517 - val_loss: 1.0823 - val_accuracy: 0.3910 - val_auc: 0.5750 - val_f1_score: 0.3481
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>67</th>
      <td>0.423153</td>
      <td>0.416833</td>
      <td>0.434657</td>
      <td>0.436367</td>
      <td>0.622302</td>
      <td>0.622161</td>
      <td>0.427952</td>
      <td>0.428796</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>68</th>
      <td>0.425347</td>
      <td>0.419001</td>
      <td>0.441132</td>
      <td>0.442856</td>
      <td>0.628616</td>
      <td>0.628440</td>
      <td>0.431302</td>
      <td>0.432004</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>69</th>
      <td>0.426285</td>
      <td>0.419939</td>
      <td>0.444707</td>
      <td>0.445365</td>
      <td>0.630568</td>
      <td>0.629962</td>
      <td>0.438532</td>
      <td>0.438226</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>70</th>
      <td>0.427374</td>
      <td>0.421171</td>
      <td>0.443661</td>
      <td>0.444747</td>
      <td>0.630587</td>
      <td>0.630568</td>
      <td>0.436326</td>
      <td>0.436371</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>71</th>
      <td>0.410917</td>
      <td>0.404180</td>
      <td>0.394925</td>
      <td>0.398624</td>
      <td>0.577812</td>
      <td>0.578069</td>
      <td>0.352692</td>
      <td>0.354411</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>71 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 402)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          201500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 340,043
Trainable params: 340,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 42ms/step - loss: 1.0874 - accuracy: 0.3804 - auc: 0.5596 - f1_score: 0.3557 - val_loss: 1.0821 - val_accuracy: 0.3957 - val_auc: 0.5754 - val_f1_score: 0.3504
Epoch 2/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0808 - accuracy: 0.3992 - auc: 0.5782 - f1_score: 0.3684 - val_loss: 1.0790 - val_accuracy: 0.4011 - val_auc: 0.5822 - val_f1_score: 0.3796
Epoch 3/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0791 - accuracy: 0.3990 - auc: 0.5804 - f1_score: 0.3755 - val_loss: 1.0783 - val_accuracy: 0.4004 - val_auc: 0.5838 - val_f1_score: 0.3825
Epoch 4/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0781 - accuracy: 0.4017 - auc: 0.5829 - f1_score: 0.3823 - val_loss: 1.0778 - val_accuracy: 0.4007 - val_auc: 0.5848 - val_f1_score: 0.3705
Epoch 5/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0776 - accuracy: 0.4029 - auc: 0.5839 - f1_score: 0.3849 - val_loss: 1.0780 - val_accuracy: 0.4062 - val_auc: 0.5863 - val_f1_score: 0.3815
Epoch 6/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0775 - accuracy: 0.4014 - auc: 0.5842 - f1_score: 0.3839 - val_loss: 1.0782 - val_accuracy: 0.4023 - val_auc: 0.5857 - val_f1_score: 0.3915
Epoch 7/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0775 - accuracy: 0.4012 - auc: 0.5838 - f1_score: 0.3860 - val_loss: 1.0776 - val_accuracy: 0.4028 - val_auc: 0.5869 - val_f1_score: 0.3869
Epoch 8/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0775 - accuracy: 0.4033 - auc: 0.5843 - f1_score: 0.3881 - val_loss: 1.0775 - val_accuracy: 0.4026 - val_auc: 0.5866 - val_f1_score: 0.3894
Epoch 9/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0775 - accuracy: 0.4025 - auc: 0.5848 - f1_score: 0.3860 - val_loss: 1.0772 - val_accuracy: 0.4042 - val_auc: 0.5877 - val_f1_score: 0.3802
Epoch 10/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0770 - accuracy: 0.4027 - auc: 0.5854 - f1_score: 0.3868 - val_loss: 1.0772 - val_accuracy: 0.4026 - val_auc: 0.5871 - val_f1_score: 0.3849
Epoch 11/50
188/188 [==============================] - 7s 40ms/step - loss: 1.0760 - accuracy: 0.4047 - auc: 0.5871 - f1_score: 0.3913 - val_loss: 1.0774 - val_accuracy: 0.3998 - val_auc: 0.5878 - val_f1_score: 0.3854
Epoch 12/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0768 - accuracy: 0.4034 - auc: 0.5857 - f1_score: 0.3924 - val_loss: 1.0766 - val_accuracy: 0.4024 - val_auc: 0.5886 - val_f1_score: 0.3857
Epoch 13/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0767 - accuracy: 0.4045 - auc: 0.5864 - f1_score: 0.3910 - val_loss: 1.0769 - val_accuracy: 0.4047 - val_auc: 0.5884 - val_f1_score: 0.3869
Epoch 14/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0763 - accuracy: 0.4049 - auc: 0.5871 - f1_score: 0.3893 - val_loss: 1.0766 - val_accuracy: 0.4030 - val_auc: 0.5887 - val_f1_score: 0.3839
Epoch 15/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0763 - accuracy: 0.4056 - auc: 0.5872 - f1_score: 0.3938 - val_loss: 1.0769 - val_accuracy: 0.4025 - val_auc: 0.5879 - val_f1_score: 0.3847
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>68</th>
      <td>0.425347</td>
      <td>0.419001</td>
      <td>0.441132</td>
      <td>0.442856</td>
      <td>0.628616</td>
      <td>0.628440</td>
      <td>0.431302</td>
      <td>0.432004</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>69</th>
      <td>0.426285</td>
      <td>0.419939</td>
      <td>0.444707</td>
      <td>0.445365</td>
      <td>0.630568</td>
      <td>0.629962</td>
      <td>0.438532</td>
      <td>0.438226</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>70</th>
      <td>0.427374</td>
      <td>0.421171</td>
      <td>0.443661</td>
      <td>0.444747</td>
      <td>0.630587</td>
      <td>0.630568</td>
      <td>0.436326</td>
      <td>0.436371</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>71</th>
      <td>0.410917</td>
      <td>0.404180</td>
      <td>0.394925</td>
      <td>0.398624</td>
      <td>0.577812</td>
      <td>0.578069</td>
      <td>0.352692</td>
      <td>0.354411</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>72</th>
      <td>0.414599</td>
      <td>0.407993</td>
      <td>0.410305</td>
      <td>0.412649</td>
      <td>0.594682</td>
      <td>0.594554</td>
      <td>0.393465</td>
      <td>0.394592</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>72 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 602)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          301500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 440,043
Trainable params: 440,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 50ms/step - loss: 1.0863 - accuracy: 0.3847 - auc: 0.5621 - f1_score: 0.3822 - val_loss: 1.0769 - val_accuracy: 0.4026 - val_auc: 0.5854 - val_f1_score: 0.3896
Epoch 2/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0772 - accuracy: 0.4012 - auc: 0.5838 - f1_score: 0.3879 - val_loss: 1.0753 - val_accuracy: 0.4068 - val_auc: 0.5890 - val_f1_score: 0.3883
Epoch 3/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0741 - accuracy: 0.4083 - auc: 0.5902 - f1_score: 0.3945 - val_loss: 1.0732 - val_accuracy: 0.4110 - val_auc: 0.5927 - val_f1_score: 0.3982
Epoch 4/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0733 - accuracy: 0.4092 - auc: 0.5917 - f1_score: 0.3943 - val_loss: 1.0737 - val_accuracy: 0.4061 - val_auc: 0.5907 - val_f1_score: 0.3854
Epoch 5/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0720 - accuracy: 0.4118 - auc: 0.5943 - f1_score: 0.3938 - val_loss: 1.0719 - val_accuracy: 0.4088 - val_auc: 0.5947 - val_f1_score: 0.3911
Epoch 6/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0717 - accuracy: 0.4097 - auc: 0.5943 - f1_score: 0.3932 - val_loss: 1.0719 - val_accuracy: 0.4065 - val_auc: 0.5945 - val_f1_score: 0.3905
Epoch 7/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0722 - accuracy: 0.4102 - auc: 0.5935 - f1_score: 0.3952 - val_loss: 1.0721 - val_accuracy: 0.4090 - val_auc: 0.5936 - val_f1_score: 0.3990
Epoch 8/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0712 - accuracy: 0.4119 - auc: 0.5963 - f1_score: 0.3978 - val_loss: 1.0726 - val_accuracy: 0.4118 - val_auc: 0.5932 - val_f1_score: 0.3981
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>69</th>
      <td>0.426285</td>
      <td>0.419939</td>
      <td>0.444707</td>
      <td>0.445365</td>
      <td>0.630568</td>
      <td>0.629962</td>
      <td>0.438532</td>
      <td>0.438226</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>70</th>
      <td>0.427374</td>
      <td>0.421171</td>
      <td>0.443661</td>
      <td>0.444747</td>
      <td>0.630587</td>
      <td>0.630568</td>
      <td>0.436326</td>
      <td>0.436371</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>71</th>
      <td>0.410917</td>
      <td>0.404180</td>
      <td>0.394925</td>
      <td>0.398624</td>
      <td>0.577812</td>
      <td>0.578069</td>
      <td>0.352692</td>
      <td>0.354411</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>72</th>
      <td>0.414599</td>
      <td>0.407993</td>
      <td>0.410305</td>
      <td>0.412649</td>
      <td>0.594682</td>
      <td>0.594554</td>
      <td>0.393465</td>
      <td>0.394592</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>73</th>
      <td>0.418077</td>
      <td>0.411569</td>
      <td>0.419266</td>
      <td>0.421490</td>
      <td>0.603743</td>
      <td>0.603930</td>
      <td>0.405959</td>
      <td>0.406901</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>73 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 802)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          401500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 540,043
Trainable params: 540,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 48ms/step - loss: 1.0833 - accuracy: 0.3897 - auc: 0.5677 - f1_score: 0.3970 - val_loss: 1.0752 - val_accuracy: 0.4042 - val_auc: 0.5839 - val_f1_score: 0.3510
Epoch 2/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0741 - accuracy: 0.4062 - auc: 0.5876 - f1_score: 0.3723 - val_loss: 1.0718 - val_accuracy: 0.4064 - val_auc: 0.5904 - val_f1_score: 0.3630
Epoch 3/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0713 - accuracy: 0.4095 - auc: 0.5928 - f1_score: 0.3741 - val_loss: 1.0699 - val_accuracy: 0.4064 - val_auc: 0.5937 - val_f1_score: 0.3628
Epoch 4/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0689 - accuracy: 0.4131 - auc: 0.5972 - f1_score: 0.3884 - val_loss: 1.0701 - val_accuracy: 0.4106 - val_auc: 0.5948 - val_f1_score: 0.3726
Epoch 5/50
188/188 [==============================] - 10s 56ms/step - loss: 1.0683 - accuracy: 0.4166 - auc: 0.5995 - f1_score: 0.3980 - val_loss: 1.0697 - val_accuracy: 0.4113 - val_auc: 0.5969 - val_f1_score: 0.3860
Epoch 6/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0682 - accuracy: 0.4154 - auc: 0.5994 - f1_score: 0.3979 - val_loss: 1.0692 - val_accuracy: 0.4104 - val_auc: 0.5975 - val_f1_score: 0.3823
Epoch 7/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0680 - accuracy: 0.4164 - auc: 0.6007 - f1_score: 0.3984 - val_loss: 1.0693 - val_accuracy: 0.4119 - val_auc: 0.5966 - val_f1_score: 0.3938
Epoch 8/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0666 - accuracy: 0.4183 - auc: 0.6027 - f1_score: 0.4020 - val_loss: 1.0681 - val_accuracy: 0.4095 - val_auc: 0.5990 - val_f1_score: 0.3908
Epoch 9/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0669 - accuracy: 0.4177 - auc: 0.6017 - f1_score: 0.4048 - val_loss: 1.0682 - val_accuracy: 0.4066 - val_auc: 0.5982 - val_f1_score: 0.3779
Epoch 10/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0658 - accuracy: 0.4192 - auc: 0.6043 - f1_score: 0.4076 - val_loss: 1.0682 - val_accuracy: 0.4145 - val_auc: 0.5992 - val_f1_score: 0.4031
Epoch 11/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0650 - accuracy: 0.4192 - auc: 0.6048 - f1_score: 0.4074 - val_loss: 1.0688 - val_accuracy: 0.4095 - val_auc: 0.5981 - val_f1_score: 0.3954
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>70</th>
      <td>0.427374</td>
      <td>0.421171</td>
      <td>0.443661</td>
      <td>0.444747</td>
      <td>0.630587</td>
      <td>0.630568</td>
      <td>0.436326</td>
      <td>0.436371</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>71</th>
      <td>0.410917</td>
      <td>0.404180</td>
      <td>0.394925</td>
      <td>0.398624</td>
      <td>0.577812</td>
      <td>0.578069</td>
      <td>0.352692</td>
      <td>0.354411</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>72</th>
      <td>0.414599</td>
      <td>0.407993</td>
      <td>0.410305</td>
      <td>0.412649</td>
      <td>0.594682</td>
      <td>0.594554</td>
      <td>0.393465</td>
      <td>0.394592</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>73</th>
      <td>0.418077</td>
      <td>0.411569</td>
      <td>0.419266</td>
      <td>0.421490</td>
      <td>0.603743</td>
      <td>0.603930</td>
      <td>0.405959</td>
      <td>0.406901</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>74</th>
      <td>0.421872</td>
      <td>0.415397</td>
      <td>0.427670</td>
      <td>0.429654</td>
      <td>0.614013</td>
      <td>0.613847</td>
      <td>0.413532</td>
      <td>0.414293</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>74 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          501500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 640,043
Trainable params: 640,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 55ms/step - loss: 1.0926 - accuracy: 0.3615 - auc: 0.5385 - f1_score: 0.3960 - val_loss: 1.0839 - val_accuracy: 0.3769 - val_auc: 0.5647 - val_f1_score: 0.3059
Epoch 2/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0839 - accuracy: 0.3719 - auc: 0.5655 - f1_score: 0.3127 - val_loss: 1.0801 - val_accuracy: 0.3778 - val_auc: 0.5719 - val_f1_score: 0.3471
Epoch 3/50
188/188 [==============================] - 11s 56ms/step - loss: 1.0807 - accuracy: 0.3795 - auc: 0.5747 - f1_score: 0.3489 - val_loss: 1.0782 - val_accuracy: 0.3783 - val_auc: 0.5781 - val_f1_score: 0.3691
Epoch 4/50
188/188 [==============================] - 13s 72ms/step - loss: 1.0776 - accuracy: 0.3843 - auc: 0.5824 - f1_score: 0.3663 - val_loss: 1.0761 - val_accuracy: 0.3811 - val_auc: 0.5831 - val_f1_score: 0.3764
Epoch 5/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0754 - accuracy: 0.3873 - auc: 0.5867 - f1_score: 0.3796 - val_loss: 1.0748 - val_accuracy: 0.3830 - val_auc: 0.5863 - val_f1_score: 0.3758
Epoch 6/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0728 - accuracy: 0.3979 - auc: 0.5927 - f1_score: 0.3967 - val_loss: 1.0728 - val_accuracy: 0.4024 - val_auc: 0.5922 - val_f1_score: 0.3755
Epoch 7/50
188/188 [==============================] - 12s 61ms/step - loss: 1.0701 - accuracy: 0.4090 - auc: 0.5969 - f1_score: 0.4009 - val_loss: 1.0708 - val_accuracy: 0.4165 - val_auc: 0.5943 - val_f1_score: 0.3777
Epoch 8/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0683 - accuracy: 0.4165 - auc: 0.5996 - f1_score: 0.4005 - val_loss: 1.0710 - val_accuracy: 0.4088 - val_auc: 0.5925 - val_f1_score: 0.3626
Epoch 9/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0673 - accuracy: 0.4189 - auc: 0.6013 - f1_score: 0.3993 - val_loss: 1.0702 - val_accuracy: 0.4156 - val_auc: 0.5948 - val_f1_score: 0.3903
Epoch 10/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0660 - accuracy: 0.4204 - auc: 0.6041 - f1_score: 0.4016 - val_loss: 1.0698 - val_accuracy: 0.4144 - val_auc: 0.5953 - val_f1_score: 0.3904
Epoch 11/50
188/188 [==============================] - 11s 56ms/step - loss: 1.0659 - accuracy: 0.4188 - auc: 0.6037 - f1_score: 0.4022 - val_loss: 1.0687 - val_accuracy: 0.4112 - val_auc: 0.5972 - val_f1_score: 0.3956
Epoch 12/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0643 - accuracy: 0.4213 - auc: 0.6058 - f1_score: 0.4058 - val_loss: 1.0680 - val_accuracy: 0.4108 - val_auc: 0.5979 - val_f1_score: 0.3816
Epoch 13/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0632 - accuracy: 0.4232 - auc: 0.6076 - f1_score: 0.4111 - val_loss: 1.0672 - val_accuracy: 0.4162 - val_auc: 0.6000 - val_f1_score: 0.3944
Epoch 14/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0623 - accuracy: 0.4248 - auc: 0.6095 - f1_score: 0.4130 - val_loss: 1.0673 - val_accuracy: 0.4181 - val_auc: 0.6006 - val_f1_score: 0.4016
Epoch 15/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0618 - accuracy: 0.4234 - auc: 0.6096 - f1_score: 0.4135 - val_loss: 1.0666 - val_accuracy: 0.4145 - val_auc: 0.6015 - val_f1_score: 0.4017
Epoch 16/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0613 - accuracy: 0.4261 - auc: 0.6113 - f1_score: 0.4167 - val_loss: 1.0665 - val_accuracy: 0.4180 - val_auc: 0.6021 - val_f1_score: 0.4065
Epoch 17/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0608 - accuracy: 0.4259 - auc: 0.6119 - f1_score: 0.4176 - val_loss: 1.0662 - val_accuracy: 0.4203 - val_auc: 0.6026 - val_f1_score: 0.4109
Epoch 18/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0601 - accuracy: 0.4264 - auc: 0.6128 - f1_score: 0.4192 - val_loss: 1.0683 - val_accuracy: 0.4179 - val_auc: 0.5983 - val_f1_score: 0.4107
Epoch 19/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0602 - accuracy: 0.4269 - auc: 0.6135 - f1_score: 0.4200 - val_loss: 1.0665 - val_accuracy: 0.4173 - val_auc: 0.6020 - val_f1_score: 0.4028
Epoch 20/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0594 - accuracy: 0.4286 - auc: 0.6142 - f1_score: 0.4208 - val_loss: 1.0669 - val_accuracy: 0.4216 - val_auc: 0.6010 - val_f1_score: 0.4104
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>71</th>
      <td>0.410917</td>
      <td>0.404180</td>
      <td>0.394925</td>
      <td>0.398624</td>
      <td>0.577812</td>
      <td>0.578069</td>
      <td>0.352692</td>
      <td>0.354411</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>72</th>
      <td>0.414599</td>
      <td>0.407993</td>
      <td>0.410305</td>
      <td>0.412649</td>
      <td>0.594682</td>
      <td>0.594554</td>
      <td>0.393465</td>
      <td>0.394592</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>73</th>
      <td>0.418077</td>
      <td>0.411569</td>
      <td>0.419266</td>
      <td>0.421490</td>
      <td>0.603743</td>
      <td>0.603930</td>
      <td>0.405959</td>
      <td>0.406901</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>74</th>
      <td>0.421872</td>
      <td>0.415397</td>
      <td>0.427670</td>
      <td>0.429654</td>
      <td>0.614013</td>
      <td>0.613847</td>
      <td>0.413532</td>
      <td>0.414293</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.425240</td>
      <td>0.418601</td>
      <td>0.440968</td>
      <td>0.441454</td>
      <td>0.627441</td>
      <td>0.626246</td>
      <td>0.429513</td>
      <td>0.428766</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>75 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1202)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          601500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 740,043
Trainable params: 740,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 65ms/step - loss: 1.0823 - accuracy: 0.3891 - auc: 0.5689 - f1_score: 0.4119 - val_loss: 1.0713 - val_accuracy: 0.4061 - val_auc: 0.5912 - val_f1_score: 0.3655
Epoch 2/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0684 - accuracy: 0.4142 - auc: 0.5981 - f1_score: 0.3960 - val_loss: 1.0677 - val_accuracy: 0.4189 - val_auc: 0.5999 - val_f1_score: 0.4063
Epoch 3/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0652 - accuracy: 0.4182 - auc: 0.6035 - f1_score: 0.4051 - val_loss: 1.0674 - val_accuracy: 0.4068 - val_auc: 0.5991 - val_f1_score: 0.3864
Epoch 4/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0641 - accuracy: 0.4194 - auc: 0.6061 - f1_score: 0.4081 - val_loss: 1.0665 - val_accuracy: 0.4129 - val_auc: 0.6019 - val_f1_score: 0.4009
Epoch 5/50
188/188 [==============================] - 12s 61ms/step - loss: 1.0629 - accuracy: 0.4209 - auc: 0.6073 - f1_score: 0.4123 - val_loss: 1.0665 - val_accuracy: 0.4139 - val_auc: 0.6015 - val_f1_score: 0.3997
Epoch 6/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0619 - accuracy: 0.4211 - auc: 0.6087 - f1_score: 0.4090 - val_loss: 1.0655 - val_accuracy: 0.4139 - val_auc: 0.6017 - val_f1_score: 0.3933
Epoch 7/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0612 - accuracy: 0.4256 - auc: 0.6110 - f1_score: 0.4163 - val_loss: 1.0653 - val_accuracy: 0.4113 - val_auc: 0.6029 - val_f1_score: 0.3988
Epoch 8/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0609 - accuracy: 0.4243 - auc: 0.6115 - f1_score: 0.4143 - val_loss: 1.0654 - val_accuracy: 0.4128 - val_auc: 0.6030 - val_f1_score: 0.3946
Epoch 9/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0595 - accuracy: 0.4261 - auc: 0.6132 - f1_score: 0.4147 - val_loss: 1.0653 - val_accuracy: 0.4145 - val_auc: 0.6024 - val_f1_score: 0.3987
Epoch 10/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0591 - accuracy: 0.4274 - auc: 0.6138 - f1_score: 0.4167 - val_loss: 1.0659 - val_accuracy: 0.4150 - val_auc: 0.6031 - val_f1_score: 0.3974
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>72</th>
      <td>0.414599</td>
      <td>0.407993</td>
      <td>0.410305</td>
      <td>0.412649</td>
      <td>0.594682</td>
      <td>0.594554</td>
      <td>0.393465</td>
      <td>0.394592</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>73</th>
      <td>0.418077</td>
      <td>0.411569</td>
      <td>0.419266</td>
      <td>0.421490</td>
      <td>0.603743</td>
      <td>0.603930</td>
      <td>0.405959</td>
      <td>0.406901</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>74</th>
      <td>0.421872</td>
      <td>0.415397</td>
      <td>0.427670</td>
      <td>0.429654</td>
      <td>0.614013</td>
      <td>0.613847</td>
      <td>0.413532</td>
      <td>0.414293</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.425240</td>
      <td>0.418601</td>
      <td>0.440968</td>
      <td>0.441454</td>
      <td>0.627441</td>
      <td>0.626246</td>
      <td>0.429513</td>
      <td>0.428766</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.424918</td>
      <td>0.418472</td>
      <td>0.434712</td>
      <td>0.436691</td>
      <td>0.624399</td>
      <td>0.623855</td>
      <td>0.414225</td>
      <td>0.414793</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>76 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1402)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          701500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 840,043
Trainable params: 840,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 75ms/step - loss: 1.0848 - accuracy: 0.3743 - auc: 0.5618 - f1_score: 0.3942 - val_loss: 1.0762 - val_accuracy: 0.4029 - val_auc: 0.5835 - val_f1_score: 0.3579
Epoch 2/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0694 - accuracy: 0.4108 - auc: 0.5956 - f1_score: 0.3817 - val_loss: 1.0698 - val_accuracy: 0.4083 - val_auc: 0.5952 - val_f1_score: 0.3618
Epoch 3/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0642 - accuracy: 0.4189 - auc: 0.6048 - f1_score: 0.3847 - val_loss: 1.0663 - val_accuracy: 0.4140 - val_auc: 0.6014 - val_f1_score: 0.3798
Epoch 4/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0618 - accuracy: 0.4229 - auc: 0.6093 - f1_score: 0.3961 - val_loss: 1.0661 - val_accuracy: 0.4107 - val_auc: 0.5994 - val_f1_score: 0.3850
Epoch 5/50
188/188 [==============================] - 13s 70ms/step - loss: 1.0606 - accuracy: 0.4230 - auc: 0.6112 - f1_score: 0.4017 - val_loss: 1.0649 - val_accuracy: 0.4125 - val_auc: 0.6030 - val_f1_score: 0.3843
Epoch 6/50
188/188 [==============================] - 13s 70ms/step - loss: 1.0594 - accuracy: 0.4267 - auc: 0.6130 - f1_score: 0.4078 - val_loss: 1.0654 - val_accuracy: 0.4128 - val_auc: 0.6022 - val_f1_score: 0.3849
Epoch 7/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0577 - accuracy: 0.4273 - auc: 0.6155 - f1_score: 0.4086 - val_loss: 1.0656 - val_accuracy: 0.4108 - val_auc: 0.6014 - val_f1_score: 0.3877
Epoch 8/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0570 - accuracy: 0.4290 - auc: 0.6167 - f1_score: 0.4120 - val_loss: 1.0661 - val_accuracy: 0.4096 - val_auc: 0.6012 - val_f1_score: 0.3904
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>73</th>
      <td>0.418077</td>
      <td>0.411569</td>
      <td>0.419266</td>
      <td>0.421490</td>
      <td>0.603743</td>
      <td>0.603930</td>
      <td>0.405959</td>
      <td>0.406901</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>74</th>
      <td>0.421872</td>
      <td>0.415397</td>
      <td>0.427670</td>
      <td>0.429654</td>
      <td>0.614013</td>
      <td>0.613847</td>
      <td>0.413532</td>
      <td>0.414293</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.425240</td>
      <td>0.418601</td>
      <td>0.440968</td>
      <td>0.441454</td>
      <td>0.627441</td>
      <td>0.626246</td>
      <td>0.429513</td>
      <td>0.428766</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.424918</td>
      <td>0.418472</td>
      <td>0.434712</td>
      <td>0.436691</td>
      <td>0.624399</td>
      <td>0.623855</td>
      <td>0.414225</td>
      <td>0.414793</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.427357</td>
      <td>0.420931</td>
      <td>0.436521</td>
      <td>0.438406</td>
      <td>0.627299</td>
      <td>0.626764</td>
      <td>0.415720</td>
      <td>0.416135</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>77 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1602)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          801500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 940,043
Trainable params: 940,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 77ms/step - loss: 1.0767 - accuracy: 0.4045 - auc: 0.5836 - f1_score: 0.3875 - val_loss: 1.0700 - val_accuracy: 0.4080 - val_auc: 0.5933 - val_f1_score: 0.3644
Epoch 2/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0656 - accuracy: 0.4183 - auc: 0.6032 - f1_score: 0.3916 - val_loss: 1.0658 - val_accuracy: 0.4176 - val_auc: 0.6017 - val_f1_score: 0.4014
Epoch 3/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0625 - accuracy: 0.4229 - auc: 0.6083 - f1_score: 0.4100 - val_loss: 1.0648 - val_accuracy: 0.4200 - val_auc: 0.6036 - val_f1_score: 0.4107
Epoch 4/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0599 - accuracy: 0.4264 - auc: 0.6118 - f1_score: 0.4164 - val_loss: 1.0641 - val_accuracy: 0.4179 - val_auc: 0.6045 - val_f1_score: 0.4087
Epoch 5/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0583 - accuracy: 0.4274 - auc: 0.6151 - f1_score: 0.4179 - val_loss: 1.0629 - val_accuracy: 0.4150 - val_auc: 0.6057 - val_f1_score: 0.4075
Epoch 6/50
188/188 [==============================] - 15s 82ms/step - loss: 1.0572 - accuracy: 0.4319 - auc: 0.6169 - f1_score: 0.4240 - val_loss: 1.0631 - val_accuracy: 0.4169 - val_auc: 0.6056 - val_f1_score: 0.4118
Epoch 7/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0563 - accuracy: 0.4298 - auc: 0.6177 - f1_score: 0.4226 - val_loss: 1.0647 - val_accuracy: 0.4146 - val_auc: 0.6033 - val_f1_score: 0.4096
Epoch 8/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0551 - accuracy: 0.4324 - auc: 0.6196 - f1_score: 0.4257 - val_loss: 1.0640 - val_accuracy: 0.4175 - val_auc: 0.6036 - val_f1_score: 0.4114
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>74</th>
      <td>0.421872</td>
      <td>0.415397</td>
      <td>0.427670</td>
      <td>0.429654</td>
      <td>0.614013</td>
      <td>0.613847</td>
      <td>0.413532</td>
      <td>0.414293</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.425240</td>
      <td>0.418601</td>
      <td>0.440968</td>
      <td>0.441454</td>
      <td>0.627441</td>
      <td>0.626246</td>
      <td>0.429513</td>
      <td>0.428766</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.424918</td>
      <td>0.418472</td>
      <td>0.434712</td>
      <td>0.436691</td>
      <td>0.624399</td>
      <td>0.623855</td>
      <td>0.414225</td>
      <td>0.414793</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.427357</td>
      <td>0.420931</td>
      <td>0.436521</td>
      <td>0.438406</td>
      <td>0.627299</td>
      <td>0.626764</td>
      <td>0.415720</td>
      <td>0.416135</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>78</th>
      <td>0.427606</td>
      <td>0.421062</td>
      <td>0.439987</td>
      <td>0.440474</td>
      <td>0.629281</td>
      <td>0.628125</td>
      <td>0.432923</td>
      <td>0.432532</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>78 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1802)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          901500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,040,043
Trainable params: 1,040,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 21s 97ms/step - loss: 1.0784 - accuracy: 0.3985 - auc: 0.5778 - f1_score: 0.3976 - val_loss: 1.0707 - val_accuracy: 0.4018 - val_auc: 0.5915 - val_f1_score: 0.3291
Epoch 2/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0654 - accuracy: 0.4163 - auc: 0.6029 - f1_score: 0.3547 - val_loss: 1.0663 - val_accuracy: 0.4167 - val_auc: 0.6007 - val_f1_score: 0.3822
Epoch 3/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0620 - accuracy: 0.4191 - auc: 0.6076 - f1_score: 0.3803 - val_loss: 1.0657 - val_accuracy: 0.4134 - val_auc: 0.6012 - val_f1_score: 0.3950
Epoch 4/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0598 - accuracy: 0.4274 - auc: 0.6114 - f1_score: 0.4018 - val_loss: 1.0653 - val_accuracy: 0.4171 - val_auc: 0.6024 - val_f1_score: 0.4062
Epoch 5/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0579 - accuracy: 0.4293 - auc: 0.6148 - f1_score: 0.4119 - val_loss: 1.0655 - val_accuracy: 0.4162 - val_auc: 0.6026 - val_f1_score: 0.4078
Epoch 6/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0563 - accuracy: 0.4331 - auc: 0.6180 - f1_score: 0.4180 - val_loss: 1.0658 - val_accuracy: 0.4161 - val_auc: 0.6022 - val_f1_score: 0.4073
Epoch 7/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0551 - accuracy: 0.4337 - auc: 0.6196 - f1_score: 0.4198 - val_loss: 1.0650 - val_accuracy: 0.4189 - val_auc: 0.6033 - val_f1_score: 0.4128
Epoch 8/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0531 - accuracy: 0.4342 - auc: 0.6219 - f1_score: 0.4212 - val_loss: 1.0649 - val_accuracy: 0.4145 - val_auc: 0.6035 - val_f1_score: 0.4104
Epoch 9/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0520 - accuracy: 0.4354 - auc: 0.6235 - f1_score: 0.4242 - val_loss: 1.0648 - val_accuracy: 0.4168 - val_auc: 0.6039 - val_f1_score: 0.4133
Epoch 10/50
188/188 [==============================] - 15s 79ms/step - loss: 1.0518 - accuracy: 0.4368 - auc: 0.6239 - f1_score: 0.4277 - val_loss: 1.0649 - val_accuracy: 0.4143 - val_auc: 0.6035 - val_f1_score: 0.4095
Epoch 11/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0510 - accuracy: 0.4366 - auc: 0.6245 - f1_score: 0.4280 - val_loss: 1.0663 - val_accuracy: 0.4127 - val_auc: 0.6010 - val_f1_score: 0.4099
Epoch 12/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0509 - accuracy: 0.4374 - auc: 0.6248 - f1_score: 0.4306 - val_loss: 1.0660 - val_accuracy: 0.4163 - val_auc: 0.6038 - val_f1_score: 0.4119
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>75</th>
      <td>0.425240</td>
      <td>0.418601</td>
      <td>0.440968</td>
      <td>0.441454</td>
      <td>0.627441</td>
      <td>0.626246</td>
      <td>0.429513</td>
      <td>0.428766</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.424918</td>
      <td>0.418472</td>
      <td>0.434712</td>
      <td>0.436691</td>
      <td>0.624399</td>
      <td>0.623855</td>
      <td>0.414225</td>
      <td>0.414793</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.427357</td>
      <td>0.420931</td>
      <td>0.436521</td>
      <td>0.438406</td>
      <td>0.627299</td>
      <td>0.626764</td>
      <td>0.415720</td>
      <td>0.416135</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>78</th>
      <td>0.427606</td>
      <td>0.421062</td>
      <td>0.439987</td>
      <td>0.440474</td>
      <td>0.629281</td>
      <td>0.628125</td>
      <td>0.432923</td>
      <td>0.432532</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.431254</td>
      <td>0.424516</td>
      <td>0.446757</td>
      <td>0.446649</td>
      <td>0.636787</td>
      <td>0.634955</td>
      <td>0.441967</td>
      <td>0.441049</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>79 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1001500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,140,043
Trainable params: 1,140,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 20s 91ms/step - loss: 1.0820 - accuracy: 0.3934 - auc: 0.5697 - f1_score: 0.4091 - val_loss: 1.0741 - val_accuracy: 0.4096 - val_auc: 0.5885 - val_f1_score: 0.3898
Epoch 2/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0687 - accuracy: 0.4170 - auc: 0.5985 - f1_score: 0.3966 - val_loss: 1.0685 - val_accuracy: 0.4161 - val_auc: 0.5977 - val_f1_score: 0.4074
Epoch 3/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0622 - accuracy: 0.4227 - auc: 0.6089 - f1_score: 0.4164 - val_loss: 1.0658 - val_accuracy: 0.4221 - val_auc: 0.6025 - val_f1_score: 0.4198
Epoch 4/50
188/188 [==============================] - 18s 95ms/step - loss: 1.0587 - accuracy: 0.4271 - auc: 0.6145 - f1_score: 0.4222 - val_loss: 1.0648 - val_accuracy: 0.4198 - val_auc: 0.6033 - val_f1_score: 0.4161
Epoch 5/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0560 - accuracy: 0.4320 - auc: 0.6174 - f1_score: 0.4274 - val_loss: 1.0634 - val_accuracy: 0.4207 - val_auc: 0.6062 - val_f1_score: 0.4145
Epoch 6/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0542 - accuracy: 0.4371 - auc: 0.6216 - f1_score: 0.4338 - val_loss: 1.0631 - val_accuracy: 0.4231 - val_auc: 0.6068 - val_f1_score: 0.4223
Epoch 7/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0529 - accuracy: 0.4356 - auc: 0.6228 - f1_score: 0.4318 - val_loss: 1.0635 - val_accuracy: 0.4243 - val_auc: 0.6071 - val_f1_score: 0.4225
Epoch 8/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0516 - accuracy: 0.4367 - auc: 0.6247 - f1_score: 0.4340 - val_loss: 1.0634 - val_accuracy: 0.4193 - val_auc: 0.6060 - val_f1_score: 0.4178
Epoch 9/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0502 - accuracy: 0.4376 - auc: 0.6263 - f1_score: 0.4335 - val_loss: 1.0631 - val_accuracy: 0.4203 - val_auc: 0.6067 - val_f1_score: 0.4174
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>76</th>
      <td>0.424918</td>
      <td>0.418472</td>
      <td>0.434712</td>
      <td>0.436691</td>
      <td>0.624399</td>
      <td>0.623855</td>
      <td>0.414225</td>
      <td>0.414793</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.427357</td>
      <td>0.420931</td>
      <td>0.436521</td>
      <td>0.438406</td>
      <td>0.627299</td>
      <td>0.626764</td>
      <td>0.415720</td>
      <td>0.416135</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>78</th>
      <td>0.427606</td>
      <td>0.421062</td>
      <td>0.439987</td>
      <td>0.440474</td>
      <td>0.629281</td>
      <td>0.628125</td>
      <td>0.432923</td>
      <td>0.432532</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.431254</td>
      <td>0.424516</td>
      <td>0.446757</td>
      <td>0.446649</td>
      <td>0.636787</td>
      <td>0.634955</td>
      <td>0.441967</td>
      <td>0.441049</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.430037</td>
      <td>0.423400</td>
      <td>0.453646</td>
      <td>0.453460</td>
      <td>0.641572</td>
      <td>0.639772</td>
      <td>0.450515</td>
      <td>0.449607</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>80 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 205)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          103000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 302,743
Trainable params: 302,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 8s 32ms/step - loss: 1.0675 - accuracy: 0.4197 - auc: 0.6022 - f1_score: 0.4262 - val_loss: 1.0556 - val_accuracy: 0.4291 - val_auc: 0.6193 - val_f1_score: 0.4052
Epoch 2/50
188/188 [==============================] - 6s 30ms/step - loss: 1.0536 - accuracy: 0.4328 - auc: 0.6201 - f1_score: 0.4122 - val_loss: 1.0547 - val_accuracy: 0.4314 - val_auc: 0.6214 - val_f1_score: 0.4196
Epoch 3/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0525 - accuracy: 0.4335 - auc: 0.6215 - f1_score: 0.4199 - val_loss: 1.0545 - val_accuracy: 0.4343 - val_auc: 0.6220 - val_f1_score: 0.4266
Epoch 4/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0521 - accuracy: 0.4331 - auc: 0.6221 - f1_score: 0.4221 - val_loss: 1.0548 - val_accuracy: 0.4330 - val_auc: 0.6216 - val_f1_score: 0.4268
Epoch 5/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0512 - accuracy: 0.4338 - auc: 0.6234 - f1_score: 0.4229 - val_loss: 1.0547 - val_accuracy: 0.4348 - val_auc: 0.6221 - val_f1_score: 0.4244
Epoch 6/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0510 - accuracy: 0.4338 - auc: 0.6232 - f1_score: 0.4242 - val_loss: 1.0545 - val_accuracy: 0.4351 - val_auc: 0.6226 - val_f1_score: 0.4289
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>77</th>
      <td>0.427357</td>
      <td>0.420931</td>
      <td>0.436521</td>
      <td>0.438406</td>
      <td>0.627299</td>
      <td>0.626764</td>
      <td>0.415720</td>
      <td>0.416135</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>78</th>
      <td>0.427606</td>
      <td>0.421062</td>
      <td>0.439987</td>
      <td>0.440474</td>
      <td>0.629281</td>
      <td>0.628125</td>
      <td>0.432923</td>
      <td>0.432532</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.431254</td>
      <td>0.424516</td>
      <td>0.446757</td>
      <td>0.446649</td>
      <td>0.636787</td>
      <td>0.634955</td>
      <td>0.441967</td>
      <td>0.441049</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.430037</td>
      <td>0.423400</td>
      <td>0.453646</td>
      <td>0.453460</td>
      <td>0.641572</td>
      <td>0.639772</td>
      <td>0.450515</td>
      <td>0.449607</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.427529</td>
      <td>0.421569</td>
      <td>0.436303</td>
      <td>0.437534</td>
      <td>0.626557</td>
      <td>0.627034</td>
      <td>0.430298</td>
      <td>0.430585</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>81 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 405)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          203000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 402,743
Trainable params: 402,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 52ms/step - loss: 1.0676 - accuracy: 0.4220 - auc: 0.6007 - f1_score: 0.4070 - val_loss: 1.0561 - val_accuracy: 0.4279 - val_auc: 0.6180 - val_f1_score: 0.3765
Epoch 2/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0528 - accuracy: 0.4338 - auc: 0.6218 - f1_score: 0.3967 - val_loss: 1.0542 - val_accuracy: 0.4318 - val_auc: 0.6208 - val_f1_score: 0.4007
Epoch 3/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0503 - accuracy: 0.4358 - auc: 0.6244 - f1_score: 0.4138 - val_loss: 1.0531 - val_accuracy: 0.4365 - val_auc: 0.6236 - val_f1_score: 0.4171
Epoch 4/50
188/188 [==============================] - 8s 41ms/step - loss: 1.0500 - accuracy: 0.4365 - auc: 0.6252 - f1_score: 0.4243 - val_loss: 1.0535 - val_accuracy: 0.4353 - val_auc: 0.6233 - val_f1_score: 0.4197
Epoch 5/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0499 - accuracy: 0.4359 - auc: 0.6253 - f1_score: 0.4247 - val_loss: 1.0534 - val_accuracy: 0.4341 - val_auc: 0.6239 - val_f1_score: 0.4169
Epoch 6/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0485 - accuracy: 0.4380 - auc: 0.6273 - f1_score: 0.4259 - val_loss: 1.0530 - val_accuracy: 0.4405 - val_auc: 0.6249 - val_f1_score: 0.4289
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>78</th>
      <td>0.427606</td>
      <td>0.421062</td>
      <td>0.439987</td>
      <td>0.440474</td>
      <td>0.629281</td>
      <td>0.628125</td>
      <td>0.432923</td>
      <td>0.432532</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.431254</td>
      <td>0.424516</td>
      <td>0.446757</td>
      <td>0.446649</td>
      <td>0.636787</td>
      <td>0.634955</td>
      <td>0.441967</td>
      <td>0.441049</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.430037</td>
      <td>0.423400</td>
      <td>0.453646</td>
      <td>0.453460</td>
      <td>0.641572</td>
      <td>0.639772</td>
      <td>0.450515</td>
      <td>0.449607</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.427529</td>
      <td>0.421569</td>
      <td>0.436303</td>
      <td>0.437534</td>
      <td>0.626557</td>
      <td>0.627034</td>
      <td>0.430298</td>
      <td>0.430585</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>82</th>
      <td>0.429649</td>
      <td>0.423740</td>
      <td>0.441633</td>
      <td>0.443346</td>
      <td>0.630471</td>
      <td>0.631014</td>
      <td>0.429898</td>
      <td>0.430345</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>82 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 605)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          303000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 502,743
Trainable params: 502,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 15s 65ms/step - loss: 1.0683 - accuracy: 0.4191 - auc: 0.6007 - f1_score: 0.4212 - val_loss: 1.0548 - val_accuracy: 0.4306 - val_auc: 0.6220 - val_f1_score: 0.4110
Epoch 2/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0514 - accuracy: 0.4341 - auc: 0.6239 - f1_score: 0.4225 - val_loss: 1.0519 - val_accuracy: 0.4357 - val_auc: 0.6252 - val_f1_score: 0.4185
Epoch 3/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0487 - accuracy: 0.4363 - auc: 0.6269 - f1_score: 0.4242 - val_loss: 1.0513 - val_accuracy: 0.4336 - val_auc: 0.6263 - val_f1_score: 0.4231
Epoch 4/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0477 - accuracy: 0.4355 - auc: 0.6278 - f1_score: 0.4267 - val_loss: 1.0512 - val_accuracy: 0.4324 - val_auc: 0.6266 - val_f1_score: 0.4188
Epoch 5/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0471 - accuracy: 0.4385 - auc: 0.6284 - f1_score: 0.4292 - val_loss: 1.0522 - val_accuracy: 0.4347 - val_auc: 0.6256 - val_f1_score: 0.4233
Epoch 6/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0468 - accuracy: 0.4398 - auc: 0.6297 - f1_score: 0.4313 - val_loss: 1.0517 - val_accuracy: 0.4351 - val_auc: 0.6265 - val_f1_score: 0.4219
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>79</th>
      <td>0.431254</td>
      <td>0.424516</td>
      <td>0.446757</td>
      <td>0.446649</td>
      <td>0.636787</td>
      <td>0.634955</td>
      <td>0.441967</td>
      <td>0.441049</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.430037</td>
      <td>0.423400</td>
      <td>0.453646</td>
      <td>0.453460</td>
      <td>0.641572</td>
      <td>0.639772</td>
      <td>0.450515</td>
      <td>0.449607</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.427529</td>
      <td>0.421569</td>
      <td>0.436303</td>
      <td>0.437534</td>
      <td>0.626557</td>
      <td>0.627034</td>
      <td>0.430298</td>
      <td>0.430585</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>82</th>
      <td>0.429649</td>
      <td>0.423740</td>
      <td>0.441633</td>
      <td>0.443346</td>
      <td>0.630471</td>
      <td>0.631014</td>
      <td>0.429898</td>
      <td>0.430345</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>83</th>
      <td>0.430894</td>
      <td>0.425086</td>
      <td>0.441786</td>
      <td>0.444061</td>
      <td>0.633122</td>
      <td>0.634155</td>
      <td>0.428058</td>
      <td>0.429072</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>83 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 805)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          403000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 602,743
Trainable params: 602,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 73ms/step - loss: 1.0716 - accuracy: 0.4153 - auc: 0.5941 - f1_score: 0.4050 - val_loss: 1.0540 - val_accuracy: 0.4320 - val_auc: 0.6230 - val_f1_score: 0.3654
Epoch 2/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0512 - accuracy: 0.4337 - auc: 0.6233 - f1_score: 0.3993 - val_loss: 1.0503 - val_accuracy: 0.4361 - val_auc: 0.6277 - val_f1_score: 0.4064
Epoch 3/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0480 - accuracy: 0.4378 - auc: 0.6276 - f1_score: 0.4120 - val_loss: 1.0498 - val_accuracy: 0.4388 - val_auc: 0.6282 - val_f1_score: 0.4126
Epoch 4/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0466 - accuracy: 0.4389 - auc: 0.6293 - f1_score: 0.4213 - val_loss: 1.0495 - val_accuracy: 0.4367 - val_auc: 0.6291 - val_f1_score: 0.4243
Epoch 5/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0453 - accuracy: 0.4391 - auc: 0.6308 - f1_score: 0.4283 - val_loss: 1.0498 - val_accuracy: 0.4362 - val_auc: 0.6292 - val_f1_score: 0.4269
Epoch 6/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0452 - accuracy: 0.4430 - auc: 0.6317 - f1_score: 0.4321 - val_loss: 1.0497 - val_accuracy: 0.4385 - val_auc: 0.6286 - val_f1_score: 0.4276
Epoch 7/50
188/188 [==============================] - 13s 70ms/step - loss: 1.0454 - accuracy: 0.4410 - auc: 0.6315 - f1_score: 0.4328 - val_loss: 1.0498 - val_accuracy: 0.4375 - val_auc: 0.6286 - val_f1_score: 0.4269
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>80</th>
      <td>0.430037</td>
      <td>0.423400</td>
      <td>0.453646</td>
      <td>0.453460</td>
      <td>0.641572</td>
      <td>0.639772</td>
      <td>0.450515</td>
      <td>0.449607</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>2</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.427529</td>
      <td>0.421569</td>
      <td>0.436303</td>
      <td>0.437534</td>
      <td>0.626557</td>
      <td>0.627034</td>
      <td>0.430298</td>
      <td>0.430585</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>82</th>
      <td>0.429649</td>
      <td>0.423740</td>
      <td>0.441633</td>
      <td>0.443346</td>
      <td>0.630471</td>
      <td>0.631014</td>
      <td>0.429898</td>
      <td>0.430345</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>83</th>
      <td>0.430894</td>
      <td>0.425086</td>
      <td>0.441786</td>
      <td>0.444061</td>
      <td>0.633122</td>
      <td>0.634155</td>
      <td>0.428058</td>
      <td>0.429072</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.431652</td>
      <td>0.425786</td>
      <td>0.445318</td>
      <td>0.447178</td>
      <td>0.635669</td>
      <td>0.636310</td>
      <td>0.434498</td>
      <td>0.435192</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>84 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          503000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 702,743
Trainable params: 702,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 21s 92ms/step - loss: 1.0718 - accuracy: 0.4199 - auc: 0.5984 - f1_score: 0.4185 - val_loss: 1.0563 - val_accuracy: 0.4293 - val_auc: 0.6211 - val_f1_score: 0.4033
Epoch 2/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0500 - accuracy: 0.4371 - auc: 0.6258 - f1_score: 0.4144 - val_loss: 1.0505 - val_accuracy: 0.4301 - val_auc: 0.6252 - val_f1_score: 0.4147
Epoch 3/50
188/188 [==============================] - 15s 80ms/step - loss: 1.0472 - accuracy: 0.4386 - auc: 0.6284 - f1_score: 0.4248 - val_loss: 1.0494 - val_accuracy: 0.4318 - val_auc: 0.6265 - val_f1_score: 0.4164
Epoch 4/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0460 - accuracy: 0.4413 - auc: 0.6304 - f1_score: 0.4285 - val_loss: 1.0486 - val_accuracy: 0.4325 - val_auc: 0.6277 - val_f1_score: 0.4180
Epoch 5/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0445 - accuracy: 0.4423 - auc: 0.6319 - f1_score: 0.4344 - val_loss: 1.0496 - val_accuracy: 0.4319 - val_auc: 0.6266 - val_f1_score: 0.4199
Epoch 6/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0447 - accuracy: 0.4430 - auc: 0.6320 - f1_score: 0.4332 - val_loss: 1.0491 - val_accuracy: 0.4369 - val_auc: 0.6280 - val_f1_score: 0.4276
Epoch 7/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0443 - accuracy: 0.4414 - auc: 0.6324 - f1_score: 0.4338 - val_loss: 1.0494 - val_accuracy: 0.4374 - val_auc: 0.6274 - val_f1_score: 0.4301
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>81</th>
      <td>0.427529</td>
      <td>0.421569</td>
      <td>0.436303</td>
      <td>0.437534</td>
      <td>0.626557</td>
      <td>0.627034</td>
      <td>0.430298</td>
      <td>0.430585</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>82</th>
      <td>0.429649</td>
      <td>0.423740</td>
      <td>0.441633</td>
      <td>0.443346</td>
      <td>0.630471</td>
      <td>0.631014</td>
      <td>0.429898</td>
      <td>0.430345</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>83</th>
      <td>0.430894</td>
      <td>0.425086</td>
      <td>0.441786</td>
      <td>0.444061</td>
      <td>0.633122</td>
      <td>0.634155</td>
      <td>0.428058</td>
      <td>0.429072</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.431652</td>
      <td>0.425786</td>
      <td>0.445318</td>
      <td>0.447178</td>
      <td>0.635669</td>
      <td>0.636310</td>
      <td>0.434498</td>
      <td>0.435192</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>85</th>
      <td>0.431731</td>
      <td>0.425747</td>
      <td>0.445394</td>
      <td>0.447393</td>
      <td>0.637063</td>
      <td>0.637364</td>
      <td>0.437174</td>
      <td>0.438126</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>85 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1205)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          603000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 802,743
Trainable params: 802,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 23s 103ms/step - loss: 1.0898 - accuracy: 0.3872 - auc: 0.5518 - f1_score: 0.4124 - val_loss: 1.0796 - val_accuracy: 0.4187 - val_auc: 0.5952 - val_f1_score: 0.3849
Epoch 2/50
188/188 [==============================] - 20s 108ms/step - loss: 1.0682 - accuracy: 0.4273 - auc: 0.6053 - f1_score: 0.4260 - val_loss: 1.0526 - val_accuracy: 0.4400 - val_auc: 0.6267 - val_f1_score: 0.4373
Epoch 3/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0507 - accuracy: 0.4404 - auc: 0.6257 - f1_score: 0.4352 - val_loss: 1.0499 - val_accuracy: 0.4372 - val_auc: 0.6275 - val_f1_score: 0.4330
Epoch 4/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0475 - accuracy: 0.4424 - auc: 0.6291 - f1_score: 0.4366 - val_loss: 1.0498 - val_accuracy: 0.4379 - val_auc: 0.6277 - val_f1_score: 0.4345
Epoch 5/50
188/188 [==============================] - 21s 111ms/step - loss: 1.0451 - accuracy: 0.4452 - auc: 0.6326 - f1_score: 0.4392 - val_loss: 1.0495 - val_accuracy: 0.4348 - val_auc: 0.6278 - val_f1_score: 0.4294
Epoch 6/50
188/188 [==============================] - 21s 111ms/step - loss: 1.0441 - accuracy: 0.4450 - auc: 0.6332 - f1_score: 0.4393 - val_loss: 1.0492 - val_accuracy: 0.4312 - val_auc: 0.6280 - val_f1_score: 0.4215
Epoch 7/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0434 - accuracy: 0.4460 - auc: 0.6343 - f1_score: 0.4399 - val_loss: 1.0494 - val_accuracy: 0.4336 - val_auc: 0.6278 - val_f1_score: 0.4288
Epoch 8/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0433 - accuracy: 0.4457 - auc: 0.6345 - f1_score: 0.4402 - val_loss: 1.0492 - val_accuracy: 0.4347 - val_auc: 0.6281 - val_f1_score: 0.4316
Epoch 9/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0420 - accuracy: 0.4475 - auc: 0.6368 - f1_score: 0.4421 - val_loss: 1.0497 - val_accuracy: 0.4330 - val_auc: 0.6275 - val_f1_score: 0.4264
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>82</th>
      <td>0.429649</td>
      <td>0.423740</td>
      <td>0.441633</td>
      <td>0.443346</td>
      <td>0.630471</td>
      <td>0.631014</td>
      <td>0.429898</td>
      <td>0.430345</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>83</th>
      <td>0.430894</td>
      <td>0.425086</td>
      <td>0.441786</td>
      <td>0.444061</td>
      <td>0.633122</td>
      <td>0.634155</td>
      <td>0.428058</td>
      <td>0.429072</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.431652</td>
      <td>0.425786</td>
      <td>0.445318</td>
      <td>0.447178</td>
      <td>0.635669</td>
      <td>0.636310</td>
      <td>0.434498</td>
      <td>0.435192</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>85</th>
      <td>0.431731</td>
      <td>0.425747</td>
      <td>0.445394</td>
      <td>0.447393</td>
      <td>0.637063</td>
      <td>0.637364</td>
      <td>0.437174</td>
      <td>0.438126</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>86</th>
      <td>0.433086</td>
      <td>0.427046</td>
      <td>0.450997</td>
      <td>0.452529</td>
      <td>0.641143</td>
      <td>0.641012</td>
      <td>0.444113</td>
      <td>0.444625</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>86 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1405)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          703000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 902,743
Trainable params: 902,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 28s 122ms/step - loss: 1.0694 - accuracy: 0.4192 - auc: 0.5936 - f1_score: 0.4148 - val_loss: 1.0572 - val_accuracy: 0.4309 - val_auc: 0.6171 - val_f1_score: 0.3602
Epoch 2/50
188/188 [==============================] - 24s 125ms/step - loss: 1.0525 - accuracy: 0.4381 - auc: 0.6228 - f1_score: 0.3973 - val_loss: 1.0538 - val_accuracy: 0.4351 - val_auc: 0.6219 - val_f1_score: 0.4085
Epoch 3/50
188/188 [==============================] - 24s 130ms/step - loss: 1.0498 - accuracy: 0.4428 - auc: 0.6274 - f1_score: 0.4263 - val_loss: 1.0523 - val_accuracy: 0.4379 - val_auc: 0.6249 - val_f1_score: 0.4218
Epoch 4/50
188/188 [==============================] - 24s 130ms/step - loss: 1.0475 - accuracy: 0.4434 - auc: 0.6305 - f1_score: 0.4306 - val_loss: 1.0502 - val_accuracy: 0.4414 - val_auc: 0.6278 - val_f1_score: 0.4355
Epoch 5/50
188/188 [==============================] - 22s 115ms/step - loss: 1.0454 - accuracy: 0.4440 - auc: 0.6322 - f1_score: 0.4374 - val_loss: 1.0490 - val_accuracy: 0.4409 - val_auc: 0.6297 - val_f1_score: 0.4345
Epoch 6/50
188/188 [==============================] - 23s 125ms/step - loss: 1.0426 - accuracy: 0.4442 - auc: 0.6353 - f1_score: 0.4385 - val_loss: 1.0487 - val_accuracy: 0.4415 - val_auc: 0.6298 - val_f1_score: 0.4350
Epoch 7/50
188/188 [==============================] - 24s 125ms/step - loss: 1.0430 - accuracy: 0.4456 - auc: 0.6352 - f1_score: 0.4394 - val_loss: 1.0487 - val_accuracy: 0.4440 - val_auc: 0.6298 - val_f1_score: 0.4414
Epoch 8/50
188/188 [==============================] - 23s 123ms/step - loss: 1.0411 - accuracy: 0.4481 - auc: 0.6376 - f1_score: 0.4429 - val_loss: 1.0489 - val_accuracy: 0.4427 - val_auc: 0.6300 - val_f1_score: 0.4402
Epoch 9/50
188/188 [==============================] - 24s 128ms/step - loss: 1.0405 - accuracy: 0.4492 - auc: 0.6385 - f1_score: 0.4449 - val_loss: 1.0491 - val_accuracy: 0.4399 - val_auc: 0.6293 - val_f1_score: 0.4358
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>83</th>
      <td>0.430894</td>
      <td>0.425086</td>
      <td>0.441786</td>
      <td>0.444061</td>
      <td>0.633122</td>
      <td>0.634155</td>
      <td>0.428058</td>
      <td>0.429072</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.431652</td>
      <td>0.425786</td>
      <td>0.445318</td>
      <td>0.447178</td>
      <td>0.635669</td>
      <td>0.636310</td>
      <td>0.434498</td>
      <td>0.435192</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>85</th>
      <td>0.431731</td>
      <td>0.425747</td>
      <td>0.445394</td>
      <td>0.447393</td>
      <td>0.637063</td>
      <td>0.637364</td>
      <td>0.437174</td>
      <td>0.438126</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>86</th>
      <td>0.433086</td>
      <td>0.427046</td>
      <td>0.450997</td>
      <td>0.452529</td>
      <td>0.641143</td>
      <td>0.641012</td>
      <td>0.444113</td>
      <td>0.444625</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.434065</td>
      <td>0.428043</td>
      <td>0.454245</td>
      <td>0.455048</td>
      <td>0.643693</td>
      <td>0.643410</td>
      <td>0.449494</td>
      <td>0.449419</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>87 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1605)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          803000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,002,743
Trainable params: 1,002,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 29s 131ms/step - loss: 1.0643 - accuracy: 0.4201 - auc: 0.6042 - f1_score: 0.4217 - val_loss: 1.0517 - val_accuracy: 0.4325 - val_auc: 0.6232 - val_f1_score: 0.4057
Epoch 2/50
188/188 [==============================] - 30s 162ms/step - loss: 1.0472 - accuracy: 0.4392 - auc: 0.6283 - f1_score: 0.4170 - val_loss: 1.0497 - val_accuracy: 0.4355 - val_auc: 0.6263 - val_f1_score: 0.4203
Epoch 3/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0453 - accuracy: 0.4414 - auc: 0.6315 - f1_score: 0.4246 - val_loss: 1.0503 - val_accuracy: 0.4308 - val_auc: 0.6257 - val_f1_score: 0.4215
Epoch 4/50
188/188 [==============================] - 29s 154ms/step - loss: 1.0440 - accuracy: 0.4427 - auc: 0.6330 - f1_score: 0.4288 - val_loss: 1.0501 - val_accuracy: 0.4380 - val_auc: 0.6275 - val_f1_score: 0.4284
Epoch 5/50
188/188 [==============================] - 24s 130ms/step - loss: 1.0427 - accuracy: 0.4449 - auc: 0.6350 - f1_score: 0.4339 - val_loss: 1.0495 - val_accuracy: 0.4356 - val_auc: 0.6284 - val_f1_score: 0.4268
Epoch 6/50
188/188 [==============================] - 24s 126ms/step - loss: 1.0420 - accuracy: 0.4458 - auc: 0.6360 - f1_score: 0.4362 - val_loss: 1.0495 - val_accuracy: 0.4321 - val_auc: 0.6277 - val_f1_score: 0.4230
Epoch 7/50
188/188 [==============================] - 24s 127ms/step - loss: 1.0417 - accuracy: 0.4460 - auc: 0.6363 - f1_score: 0.4374 - val_loss: 1.0494 - val_accuracy: 0.4384 - val_auc: 0.6280 - val_f1_score: 0.4313
Epoch 8/50
188/188 [==============================] - 28s 149ms/step - loss: 1.0402 - accuracy: 0.4474 - auc: 0.6380 - f1_score: 0.4388 - val_loss: 1.0499 - val_accuracy: 0.4365 - val_auc: 0.6276 - val_f1_score: 0.4330
Epoch 9/50
188/188 [==============================] - 25s 131ms/step - loss: 1.0403 - accuracy: 0.4483 - auc: 0.6383 - f1_score: 0.4419 - val_loss: 1.0500 - val_accuracy: 0.4376 - val_auc: 0.6271 - val_f1_score: 0.4344
Epoch 10/50
188/188 [==============================] - 24s 130ms/step - loss: 1.0391 - accuracy: 0.4508 - auc: 0.6404 - f1_score: 0.4440 - val_loss: 1.0501 - val_accuracy: 0.4411 - val_auc: 0.6279 - val_f1_score: 0.4359
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>84</th>
      <td>0.431652</td>
      <td>0.425786</td>
      <td>0.445318</td>
      <td>0.447178</td>
      <td>0.635669</td>
      <td>0.636310</td>
      <td>0.434498</td>
      <td>0.435192</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>85</th>
      <td>0.431731</td>
      <td>0.425747</td>
      <td>0.445394</td>
      <td>0.447393</td>
      <td>0.637063</td>
      <td>0.637364</td>
      <td>0.437174</td>
      <td>0.438126</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>86</th>
      <td>0.433086</td>
      <td>0.427046</td>
      <td>0.450997</td>
      <td>0.452529</td>
      <td>0.641143</td>
      <td>0.641012</td>
      <td>0.444113</td>
      <td>0.444625</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.434065</td>
      <td>0.428043</td>
      <td>0.454245</td>
      <td>0.455048</td>
      <td>0.643693</td>
      <td>0.643410</td>
      <td>0.449494</td>
      <td>0.449419</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.435399</td>
      <td>0.429340</td>
      <td>0.455292</td>
      <td>0.456175</td>
      <td>0.644946</td>
      <td>0.644598</td>
      <td>0.449274</td>
      <td>0.449184</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>88 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1805)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          903000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,102,743
Trainable params: 1,102,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 33s 153ms/step - loss: 1.0752 - accuracy: 0.4071 - auc: 0.5881 - f1_score: 0.4225 - val_loss: 1.0677 - val_accuracy: 0.4326 - val_auc: 0.6145 - val_f1_score: 0.3784
Epoch 2/50
188/188 [==============================] - 31s 166ms/step - loss: 1.0544 - accuracy: 0.4337 - auc: 0.6215 - f1_score: 0.4143 - val_loss: 1.0516 - val_accuracy: 0.4290 - val_auc: 0.6239 - val_f1_score: 0.4003
Epoch 3/50
188/188 [==============================] - 29s 152ms/step - loss: 1.0463 - accuracy: 0.4399 - auc: 0.6299 - f1_score: 0.4273 - val_loss: 1.0492 - val_accuracy: 0.4399 - val_auc: 0.6283 - val_f1_score: 0.4231
Epoch 4/50
188/188 [==============================] - 27s 146ms/step - loss: 1.0445 - accuracy: 0.4436 - auc: 0.6330 - f1_score: 0.4335 - val_loss: 1.0489 - val_accuracy: 0.4406 - val_auc: 0.6290 - val_f1_score: 0.4362
Epoch 5/50
188/188 [==============================] - 26s 139ms/step - loss: 1.0429 - accuracy: 0.4445 - auc: 0.6353 - f1_score: 0.4374 - val_loss: 1.0488 - val_accuracy: 0.4384 - val_auc: 0.6288 - val_f1_score: 0.4305
Epoch 6/50
188/188 [==============================] - 26s 140ms/step - loss: 1.0419 - accuracy: 0.4460 - auc: 0.6362 - f1_score: 0.4395 - val_loss: 1.0493 - val_accuracy: 0.4384 - val_auc: 0.6281 - val_f1_score: 0.4274
Epoch 7/50
188/188 [==============================] - 26s 140ms/step - loss: 1.0411 - accuracy: 0.4490 - auc: 0.6381 - f1_score: 0.4418 - val_loss: 1.0492 - val_accuracy: 0.4387 - val_auc: 0.6282 - val_f1_score: 0.4354
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>85</th>
      <td>0.431731</td>
      <td>0.425747</td>
      <td>0.445394</td>
      <td>0.447393</td>
      <td>0.637063</td>
      <td>0.637364</td>
      <td>0.437174</td>
      <td>0.438126</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>86</th>
      <td>0.433086</td>
      <td>0.427046</td>
      <td>0.450997</td>
      <td>0.452529</td>
      <td>0.641143</td>
      <td>0.641012</td>
      <td>0.444113</td>
      <td>0.444625</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.434065</td>
      <td>0.428043</td>
      <td>0.454245</td>
      <td>0.455048</td>
      <td>0.643693</td>
      <td>0.643410</td>
      <td>0.449494</td>
      <td>0.449419</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.435399</td>
      <td>0.429340</td>
      <td>0.455292</td>
      <td>0.456175</td>
      <td>0.644946</td>
      <td>0.644598</td>
      <td>0.449274</td>
      <td>0.449184</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.432650</td>
      <td>0.426589</td>
      <td>0.454082</td>
      <td>0.455068</td>
      <td>0.644391</td>
      <td>0.644040</td>
      <td>0.450565</td>
      <td>0.450837</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>89 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1003000     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,202,743
Trainable params: 1,202,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 38s 174ms/step - loss: 1.0631 - accuracy: 0.4238 - auc: 0.6071 - f1_score: 0.4279 - val_loss: 1.0514 - val_accuracy: 0.4341 - val_auc: 0.6247 - val_f1_score: 0.3838
Epoch 2/50
188/188 [==============================] - 38s 200ms/step - loss: 1.0467 - accuracy: 0.4380 - auc: 0.6288 - f1_score: 0.3999 - val_loss: 1.0494 - val_accuracy: 0.4350 - val_auc: 0.6287 - val_f1_score: 0.3904
Epoch 3/50
188/188 [==============================] - 33s 175ms/step - loss: 1.0441 - accuracy: 0.4408 - auc: 0.6327 - f1_score: 0.4148 - val_loss: 1.0500 - val_accuracy: 0.4432 - val_auc: 0.6292 - val_f1_score: 0.4250
Epoch 4/50
188/188 [==============================] - 33s 176ms/step - loss: 1.0433 - accuracy: 0.4433 - auc: 0.6343 - f1_score: 0.4298 - val_loss: 1.0493 - val_accuracy: 0.4404 - val_auc: 0.6293 - val_f1_score: 0.4301
Epoch 5/50
188/188 [==============================] - 32s 170ms/step - loss: 1.0419 - accuracy: 0.4481 - auc: 0.6369 - f1_score: 0.4382 - val_loss: 1.0489 - val_accuracy: 0.4390 - val_auc: 0.6298 - val_f1_score: 0.4241
Epoch 6/50
188/188 [==============================] - 32s 169ms/step - loss: 1.0426 - accuracy: 0.4441 - auc: 0.6360 - f1_score: 0.4346 - val_loss: 1.0494 - val_accuracy: 0.4382 - val_auc: 0.6286 - val_f1_score: 0.4306
Epoch 7/50
188/188 [==============================] - 29s 156ms/step - loss: 1.0401 - accuracy: 0.4481 - auc: 0.6392 - f1_score: 0.4381 - val_loss: 1.0496 - val_accuracy: 0.4396 - val_auc: 0.6293 - val_f1_score: 0.4295
Epoch 8/50
188/188 [==============================] - 31s 165ms/step - loss: 1.0392 - accuracy: 0.4498 - auc: 0.6405 - f1_score: 0.4429 - val_loss: 1.0493 - val_accuracy: 0.4381 - val_auc: 0.6292 - val_f1_score: 0.4307
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>86</th>
      <td>0.433086</td>
      <td>0.427046</td>
      <td>0.450997</td>
      <td>0.452529</td>
      <td>0.641143</td>
      <td>0.641012</td>
      <td>0.444113</td>
      <td>0.444625</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.434065</td>
      <td>0.428043</td>
      <td>0.454245</td>
      <td>0.455048</td>
      <td>0.643693</td>
      <td>0.643410</td>
      <td>0.449494</td>
      <td>0.449419</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.435399</td>
      <td>0.429340</td>
      <td>0.455292</td>
      <td>0.456175</td>
      <td>0.644946</td>
      <td>0.644598</td>
      <td>0.449274</td>
      <td>0.449184</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.432650</td>
      <td>0.426589</td>
      <td>0.454082</td>
      <td>0.455068</td>
      <td>0.644391</td>
      <td>0.644040</td>
      <td>0.450565</td>
      <td>0.450837</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.435631</td>
      <td>0.429580</td>
      <td>0.455913</td>
      <td>0.457498</td>
      <td>0.646738</td>
      <td>0.646341</td>
      <td>0.447819</td>
      <td>0.448335</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>90 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 205)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          103000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 241,543
Trainable params: 241,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 9s 31ms/step - loss: 1.0703 - accuracy: 0.4109 - auc: 0.5927 - f1_score: 0.4130 - val_loss: 1.0571 - val_accuracy: 0.4289 - val_auc: 0.6185 - val_f1_score: 0.3720
Epoch 2/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0555 - accuracy: 0.4293 - auc: 0.6175 - f1_score: 0.4086 - val_loss: 1.0554 - val_accuracy: 0.4320 - val_auc: 0.6208 - val_f1_score: 0.4192
Epoch 3/50
188/188 [==============================] - 5s 27ms/step - loss: 1.0532 - accuracy: 0.4313 - auc: 0.6209 - f1_score: 0.4232 - val_loss: 1.0543 - val_accuracy: 0.4313 - val_auc: 0.6230 - val_f1_score: 0.4218
Epoch 4/50
188/188 [==============================] - 6s 30ms/step - loss: 1.0524 - accuracy: 0.4331 - auc: 0.6216 - f1_score: 0.4262 - val_loss: 1.0555 - val_accuracy: 0.4305 - val_auc: 0.6218 - val_f1_score: 0.4275
Epoch 5/50
188/188 [==============================] - 6s 29ms/step - loss: 1.0520 - accuracy: 0.4347 - auc: 0.6225 - f1_score: 0.4280 - val_loss: 1.0550 - val_accuracy: 0.4355 - val_auc: 0.6224 - val_f1_score: 0.4342
Epoch 6/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0517 - accuracy: 0.4339 - auc: 0.6225 - f1_score: 0.4288 - val_loss: 1.0543 - val_accuracy: 0.4351 - val_auc: 0.6237 - val_f1_score: 0.4260
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>87</th>
      <td>0.434065</td>
      <td>0.428043</td>
      <td>0.454245</td>
      <td>0.455048</td>
      <td>0.643693</td>
      <td>0.643410</td>
      <td>0.449494</td>
      <td>0.449419</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.435399</td>
      <td>0.429340</td>
      <td>0.455292</td>
      <td>0.456175</td>
      <td>0.644946</td>
      <td>0.644598</td>
      <td>0.449274</td>
      <td>0.449184</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.432650</td>
      <td>0.426589</td>
      <td>0.454082</td>
      <td>0.455068</td>
      <td>0.644391</td>
      <td>0.644040</td>
      <td>0.450565</td>
      <td>0.450837</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.435631</td>
      <td>0.429580</td>
      <td>0.455913</td>
      <td>0.457498</td>
      <td>0.646738</td>
      <td>0.646341</td>
      <td>0.447819</td>
      <td>0.448335</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>91</th>
      <td>0.428045</td>
      <td>0.422074</td>
      <td>0.437600</td>
      <td>0.439366</td>
      <td>0.627957</td>
      <td>0.628350</td>
      <td>0.428202</td>
      <td>0.428948</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>91 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 405)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          203000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 341,543
Trainable params: 341,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 11s 42ms/step - loss: 1.0801 - accuracy: 0.3930 - auc: 0.5744 - f1_score: 0.4069 - val_loss: 1.0690 - val_accuracy: 0.4234 - val_auc: 0.6081 - val_f1_score: 0.3838
Epoch 2/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0629 - accuracy: 0.4221 - auc: 0.6088 - f1_score: 0.4020 - val_loss: 1.0582 - val_accuracy: 0.4308 - val_auc: 0.6177 - val_f1_score: 0.4055
Epoch 3/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0556 - accuracy: 0.4310 - auc: 0.6188 - f1_score: 0.4184 - val_loss: 1.0543 - val_accuracy: 0.4369 - val_auc: 0.6227 - val_f1_score: 0.4125
Epoch 4/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0527 - accuracy: 0.4347 - auc: 0.6221 - f1_score: 0.4225 - val_loss: 1.0542 - val_accuracy: 0.4319 - val_auc: 0.6217 - val_f1_score: 0.4171
Epoch 5/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0512 - accuracy: 0.4346 - auc: 0.6244 - f1_score: 0.4235 - val_loss: 1.0537 - val_accuracy: 0.4354 - val_auc: 0.6225 - val_f1_score: 0.4173
Epoch 6/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0504 - accuracy: 0.4367 - auc: 0.6247 - f1_score: 0.4232 - val_loss: 1.0539 - val_accuracy: 0.4315 - val_auc: 0.6220 - val_f1_score: 0.4144
Epoch 7/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0497 - accuracy: 0.4377 - auc: 0.6255 - f1_score: 0.4254 - val_loss: 1.0538 - val_accuracy: 0.4336 - val_auc: 0.6229 - val_f1_score: 0.4121
Epoch 8/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0490 - accuracy: 0.4367 - auc: 0.6270 - f1_score: 0.4252 - val_loss: 1.0542 - val_accuracy: 0.4318 - val_auc: 0.6231 - val_f1_score: 0.4066
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>88</th>
      <td>0.435399</td>
      <td>0.429340</td>
      <td>0.455292</td>
      <td>0.456175</td>
      <td>0.644946</td>
      <td>0.644598</td>
      <td>0.449274</td>
      <td>0.449184</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.432650</td>
      <td>0.426589</td>
      <td>0.454082</td>
      <td>0.455068</td>
      <td>0.644391</td>
      <td>0.644040</td>
      <td>0.450565</td>
      <td>0.450837</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.435631</td>
      <td>0.429580</td>
      <td>0.455913</td>
      <td>0.457498</td>
      <td>0.646738</td>
      <td>0.646341</td>
      <td>0.447819</td>
      <td>0.448335</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>91</th>
      <td>0.428045</td>
      <td>0.422074</td>
      <td>0.437600</td>
      <td>0.439366</td>
      <td>0.627957</td>
      <td>0.628350</td>
      <td>0.428202</td>
      <td>0.428948</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.431451</td>
      <td>0.425645</td>
      <td>0.441993</td>
      <td>0.444394</td>
      <td>0.632440</td>
      <td>0.633402</td>
      <td>0.416594</td>
      <td>0.417331</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>92 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 605)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          303000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 441,543
Trainable params: 441,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 46ms/step - loss: 1.0711 - accuracy: 0.4113 - auc: 0.5920 - f1_score: 0.4006 - val_loss: 1.0571 - val_accuracy: 0.4257 - val_auc: 0.6150 - val_f1_score: 0.3936
Epoch 2/50
188/188 [==============================] - 8s 41ms/step - loss: 1.0560 - accuracy: 0.4303 - auc: 0.6162 - f1_score: 0.4003 - val_loss: 1.0544 - val_accuracy: 0.4277 - val_auc: 0.6190 - val_f1_score: 0.4039
Epoch 3/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0530 - accuracy: 0.4320 - auc: 0.6210 - f1_score: 0.4082 - val_loss: 1.0543 - val_accuracy: 0.4302 - val_auc: 0.6203 - val_f1_score: 0.3994
Epoch 4/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0507 - accuracy: 0.4337 - auc: 0.6239 - f1_score: 0.4097 - val_loss: 1.0542 - val_accuracy: 0.4309 - val_auc: 0.6208 - val_f1_score: 0.4140
Epoch 5/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0494 - accuracy: 0.4349 - auc: 0.6255 - f1_score: 0.4161 - val_loss: 1.0535 - val_accuracy: 0.4282 - val_auc: 0.6218 - val_f1_score: 0.4065
Epoch 6/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0491 - accuracy: 0.4351 - auc: 0.6262 - f1_score: 0.4208 - val_loss: 1.0530 - val_accuracy: 0.4283 - val_auc: 0.6218 - val_f1_score: 0.4153
Epoch 7/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0483 - accuracy: 0.4364 - auc: 0.6274 - f1_score: 0.4238 - val_loss: 1.0534 - val_accuracy: 0.4349 - val_auc: 0.6237 - val_f1_score: 0.4200
Epoch 8/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0474 - accuracy: 0.4378 - auc: 0.6285 - f1_score: 0.4258 - val_loss: 1.0529 - val_accuracy: 0.4345 - val_auc: 0.6236 - val_f1_score: 0.4233
Epoch 9/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0482 - accuracy: 0.4372 - auc: 0.6280 - f1_score: 0.4258 - val_loss: 1.0531 - val_accuracy: 0.4301 - val_auc: 0.6225 - val_f1_score: 0.4148
Epoch 10/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0466 - accuracy: 0.4398 - auc: 0.6298 - f1_score: 0.4297 - val_loss: 1.0531 - val_accuracy: 0.4303 - val_auc: 0.6235 - val_f1_score: 0.4199
Epoch 11/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0458 - accuracy: 0.4395 - auc: 0.6306 - f1_score: 0.4288 - val_loss: 1.0523 - val_accuracy: 0.4355 - val_auc: 0.6244 - val_f1_score: 0.4299
Epoch 12/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0458 - accuracy: 0.4404 - auc: 0.6309 - f1_score: 0.4315 - val_loss: 1.0524 - val_accuracy: 0.4295 - val_auc: 0.6235 - val_f1_score: 0.4168
Epoch 13/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0456 - accuracy: 0.4404 - auc: 0.6307 - f1_score: 0.4317 - val_loss: 1.0525 - val_accuracy: 0.4311 - val_auc: 0.6238 - val_f1_score: 0.4229
Epoch 14/50
188/188 [==============================] - 9s 45ms/step - loss: 1.0447 - accuracy: 0.4425 - auc: 0.6318 - f1_score: 0.4357 - val_loss: 1.0533 - val_accuracy: 0.4306 - val_auc: 0.6226 - val_f1_score: 0.4220
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>89</th>
      <td>0.432650</td>
      <td>0.426589</td>
      <td>0.454082</td>
      <td>0.455068</td>
      <td>0.644391</td>
      <td>0.644040</td>
      <td>0.450565</td>
      <td>0.450837</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.435631</td>
      <td>0.429580</td>
      <td>0.455913</td>
      <td>0.457498</td>
      <td>0.646738</td>
      <td>0.646341</td>
      <td>0.447819</td>
      <td>0.448335</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>91</th>
      <td>0.428045</td>
      <td>0.422074</td>
      <td>0.437600</td>
      <td>0.439366</td>
      <td>0.627957</td>
      <td>0.628350</td>
      <td>0.428202</td>
      <td>0.428948</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.431451</td>
      <td>0.425645</td>
      <td>0.441993</td>
      <td>0.444394</td>
      <td>0.632440</td>
      <td>0.633402</td>
      <td>0.416594</td>
      <td>0.417331</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>93</th>
      <td>0.430796</td>
      <td>0.424767</td>
      <td>0.444860</td>
      <td>0.446051</td>
      <td>0.637315</td>
      <td>0.637345</td>
      <td>0.436331</td>
      <td>0.436428</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>93 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 805)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          403000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 541,543
Trainable params: 541,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 54ms/step - loss: 1.0785 - accuracy: 0.3963 - auc: 0.5781 - f1_score: 0.4167 - val_loss: 1.0635 - val_accuracy: 0.4228 - val_auc: 0.6077 - val_f1_score: 0.3943
Epoch 2/50
188/188 [==============================] - 10s 56ms/step - loss: 1.0598 - accuracy: 0.4238 - auc: 0.6109 - f1_score: 0.4080 - val_loss: 1.0536 - val_accuracy: 0.4314 - val_auc: 0.6222 - val_f1_score: 0.4039
Epoch 3/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0533 - accuracy: 0.4269 - auc: 0.6192 - f1_score: 0.4142 - val_loss: 1.0510 - val_accuracy: 0.4379 - val_auc: 0.6260 - val_f1_score: 0.4266
Epoch 4/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0511 - accuracy: 0.4316 - auc: 0.6228 - f1_score: 0.4239 - val_loss: 1.0524 - val_accuracy: 0.4305 - val_auc: 0.6226 - val_f1_score: 0.4218
Epoch 5/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0492 - accuracy: 0.4332 - auc: 0.6251 - f1_score: 0.4246 - val_loss: 1.0521 - val_accuracy: 0.4331 - val_auc: 0.6249 - val_f1_score: 0.4131
Epoch 6/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0479 - accuracy: 0.4344 - auc: 0.6271 - f1_score: 0.4280 - val_loss: 1.0531 - val_accuracy: 0.4327 - val_auc: 0.6227 - val_f1_score: 0.4218
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>90</th>
      <td>0.435631</td>
      <td>0.429580</td>
      <td>0.455913</td>
      <td>0.457498</td>
      <td>0.646738</td>
      <td>0.646341</td>
      <td>0.447819</td>
      <td>0.448335</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>91</th>
      <td>0.428045</td>
      <td>0.422074</td>
      <td>0.437600</td>
      <td>0.439366</td>
      <td>0.627957</td>
      <td>0.628350</td>
      <td>0.428202</td>
      <td>0.428948</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.431451</td>
      <td>0.425645</td>
      <td>0.441993</td>
      <td>0.444394</td>
      <td>0.632440</td>
      <td>0.633402</td>
      <td>0.416594</td>
      <td>0.417331</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>93</th>
      <td>0.430796</td>
      <td>0.424767</td>
      <td>0.444860</td>
      <td>0.446051</td>
      <td>0.637315</td>
      <td>0.637345</td>
      <td>0.436331</td>
      <td>0.436428</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.430483</td>
      <td>0.424461</td>
      <td>0.446549</td>
      <td>0.448432</td>
      <td>0.636666</td>
      <td>0.636886</td>
      <td>0.435103</td>
      <td>0.435771</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>94 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          503000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 641,543
Trainable params: 641,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 58ms/step - loss: 1.0834 - accuracy: 0.3932 - auc: 0.5644 - f1_score: 0.4026 - val_loss: 1.0726 - val_accuracy: 0.4120 - val_auc: 0.5892 - val_f1_score: 0.3729
Epoch 2/50
188/188 [==============================] - 11s 56ms/step - loss: 1.0665 - accuracy: 0.4238 - auc: 0.6024 - f1_score: 0.4080 - val_loss: 1.0589 - val_accuracy: 0.4343 - val_auc: 0.6143 - val_f1_score: 0.4279
Epoch 3/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0575 - accuracy: 0.4303 - auc: 0.6154 - f1_score: 0.4258 - val_loss: 1.0539 - val_accuracy: 0.4282 - val_auc: 0.6190 - val_f1_score: 0.4194
Epoch 4/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0520 - accuracy: 0.4342 - auc: 0.6230 - f1_score: 0.4292 - val_loss: 1.0516 - val_accuracy: 0.4305 - val_auc: 0.6248 - val_f1_score: 0.4218
Epoch 5/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0499 - accuracy: 0.4361 - auc: 0.6258 - f1_score: 0.4301 - val_loss: 1.0521 - val_accuracy: 0.4289 - val_auc: 0.6219 - val_f1_score: 0.4252
Epoch 6/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0488 - accuracy: 0.4377 - auc: 0.6271 - f1_score: 0.4319 - val_loss: 1.0515 - val_accuracy: 0.4336 - val_auc: 0.6236 - val_f1_score: 0.4199
Epoch 7/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0468 - accuracy: 0.4417 - auc: 0.6298 - f1_score: 0.4363 - val_loss: 1.0518 - val_accuracy: 0.4281 - val_auc: 0.6221 - val_f1_score: 0.4191
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>91</th>
      <td>0.428045</td>
      <td>0.422074</td>
      <td>0.437600</td>
      <td>0.439366</td>
      <td>0.627957</td>
      <td>0.628350</td>
      <td>0.428202</td>
      <td>0.428948</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.431451</td>
      <td>0.425645</td>
      <td>0.441993</td>
      <td>0.444394</td>
      <td>0.632440</td>
      <td>0.633402</td>
      <td>0.416594</td>
      <td>0.417331</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>93</th>
      <td>0.430796</td>
      <td>0.424767</td>
      <td>0.444860</td>
      <td>0.446051</td>
      <td>0.637315</td>
      <td>0.637345</td>
      <td>0.436331</td>
      <td>0.436428</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.430483</td>
      <td>0.424461</td>
      <td>0.446549</td>
      <td>0.448432</td>
      <td>0.636666</td>
      <td>0.636886</td>
      <td>0.435103</td>
      <td>0.435771</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>95</th>
      <td>0.432867</td>
      <td>0.426762</td>
      <td>0.448130</td>
      <td>0.449618</td>
      <td>0.639364</td>
      <td>0.639276</td>
      <td>0.439364</td>
      <td>0.439770</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>95 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1205)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          603000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 741,543
Trainable params: 741,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 72ms/step - loss: 1.0759 - accuracy: 0.4021 - auc: 0.5828 - f1_score: 0.4067 - val_loss: 1.0668 - val_accuracy: 0.4110 - val_auc: 0.6002 - val_f1_score: 0.3575
Epoch 2/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0587 - accuracy: 0.4246 - auc: 0.6122 - f1_score: 0.3933 - val_loss: 1.0572 - val_accuracy: 0.4317 - val_auc: 0.6179 - val_f1_score: 0.4057
Epoch 3/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0521 - accuracy: 0.4319 - auc: 0.6213 - f1_score: 0.4152 - val_loss: 1.0523 - val_accuracy: 0.4317 - val_auc: 0.6226 - val_f1_score: 0.4111
Epoch 4/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0482 - accuracy: 0.4361 - auc: 0.6264 - f1_score: 0.4199 - val_loss: 1.0516 - val_accuracy: 0.4293 - val_auc: 0.6238 - val_f1_score: 0.4133
Epoch 5/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0458 - accuracy: 0.4406 - auc: 0.6301 - f1_score: 0.4277 - val_loss: 1.0513 - val_accuracy: 0.4312 - val_auc: 0.6248 - val_f1_score: 0.4153
Epoch 6/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0456 - accuracy: 0.4397 - auc: 0.6301 - f1_score: 0.4284 - val_loss: 1.0512 - val_accuracy: 0.4368 - val_auc: 0.6256 - val_f1_score: 0.4229
Epoch 7/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0446 - accuracy: 0.4416 - auc: 0.6317 - f1_score: 0.4329 - val_loss: 1.0508 - val_accuracy: 0.4368 - val_auc: 0.6260 - val_f1_score: 0.4255
Epoch 8/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0436 - accuracy: 0.4444 - auc: 0.6330 - f1_score: 0.4364 - val_loss: 1.0508 - val_accuracy: 0.4380 - val_auc: 0.6264 - val_f1_score: 0.4261
Epoch 9/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0430 - accuracy: 0.4448 - auc: 0.6342 - f1_score: 0.4352 - val_loss: 1.0513 - val_accuracy: 0.4369 - val_auc: 0.6255 - val_f1_score: 0.4242
Epoch 10/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0414 - accuracy: 0.4474 - auc: 0.6369 - f1_score: 0.4395 - val_loss: 1.0509 - val_accuracy: 0.4356 - val_auc: 0.6268 - val_f1_score: 0.4246
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>92</th>
      <td>0.431451</td>
      <td>0.425645</td>
      <td>0.441993</td>
      <td>0.444394</td>
      <td>0.632440</td>
      <td>0.633402</td>
      <td>0.416594</td>
      <td>0.417331</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>93</th>
      <td>0.430796</td>
      <td>0.424767</td>
      <td>0.444860</td>
      <td>0.446051</td>
      <td>0.637315</td>
      <td>0.637345</td>
      <td>0.436331</td>
      <td>0.436428</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.430483</td>
      <td>0.424461</td>
      <td>0.446549</td>
      <td>0.448432</td>
      <td>0.636666</td>
      <td>0.636886</td>
      <td>0.435103</td>
      <td>0.435771</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>95</th>
      <td>0.432867</td>
      <td>0.426762</td>
      <td>0.448130</td>
      <td>0.449618</td>
      <td>0.639364</td>
      <td>0.639276</td>
      <td>0.439364</td>
      <td>0.439770</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.434021</td>
      <td>0.427916</td>
      <td>0.453035</td>
      <td>0.454617</td>
      <td>0.643664</td>
      <td>0.643169</td>
      <td>0.440652</td>
      <td>0.441027</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>96 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1405)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          703000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 841,543
Trainable params: 841,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 77ms/step - loss: 1.0760 - accuracy: 0.4016 - auc: 0.5834 - f1_score: 0.4203 - val_loss: 1.0655 - val_accuracy: 0.4119 - val_auc: 0.6027 - val_f1_score: 0.3790
Epoch 2/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0593 - accuracy: 0.4209 - auc: 0.6105 - f1_score: 0.4061 - val_loss: 1.0576 - val_accuracy: 0.4245 - val_auc: 0.6148 - val_f1_score: 0.4122
Epoch 3/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0526 - accuracy: 0.4293 - auc: 0.6205 - f1_score: 0.4183 - val_loss: 1.0542 - val_accuracy: 0.4297 - val_auc: 0.6209 - val_f1_score: 0.4187
Epoch 4/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0492 - accuracy: 0.4349 - auc: 0.6254 - f1_score: 0.4243 - val_loss: 1.0535 - val_accuracy: 0.4299 - val_auc: 0.6216 - val_f1_score: 0.4204
Epoch 5/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0459 - accuracy: 0.4410 - auc: 0.6298 - f1_score: 0.4328 - val_loss: 1.0521 - val_accuracy: 0.4341 - val_auc: 0.6248 - val_f1_score: 0.4209
Epoch 6/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0443 - accuracy: 0.4438 - auc: 0.6327 - f1_score: 0.4353 - val_loss: 1.0513 - val_accuracy: 0.4380 - val_auc: 0.6258 - val_f1_score: 0.4284
Epoch 7/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0430 - accuracy: 0.4466 - auc: 0.6338 - f1_score: 0.4376 - val_loss: 1.0528 - val_accuracy: 0.4381 - val_auc: 0.6237 - val_f1_score: 0.4291
Epoch 8/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0425 - accuracy: 0.4441 - auc: 0.6343 - f1_score: 0.4371 - val_loss: 1.0514 - val_accuracy: 0.4402 - val_auc: 0.6257 - val_f1_score: 0.4318
Epoch 9/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0414 - accuracy: 0.4464 - auc: 0.6368 - f1_score: 0.4409 - val_loss: 1.0517 - val_accuracy: 0.4339 - val_auc: 0.6248 - val_f1_score: 0.4272
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>93</th>
      <td>0.430796</td>
      <td>0.424767</td>
      <td>0.444860</td>
      <td>0.446051</td>
      <td>0.637315</td>
      <td>0.637345</td>
      <td>0.436331</td>
      <td>0.436428</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.430483</td>
      <td>0.424461</td>
      <td>0.446549</td>
      <td>0.448432</td>
      <td>0.636666</td>
      <td>0.636886</td>
      <td>0.435103</td>
      <td>0.435771</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>95</th>
      <td>0.432867</td>
      <td>0.426762</td>
      <td>0.448130</td>
      <td>0.449618</td>
      <td>0.639364</td>
      <td>0.639276</td>
      <td>0.439364</td>
      <td>0.439770</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.434021</td>
      <td>0.427916</td>
      <td>0.453035</td>
      <td>0.454617</td>
      <td>0.643664</td>
      <td>0.643169</td>
      <td>0.440652</td>
      <td>0.441027</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.434346</td>
      <td>0.428122</td>
      <td>0.457788</td>
      <td>0.458341</td>
      <td>0.646887</td>
      <td>0.645916</td>
      <td>0.451154</td>
      <td>0.450799</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>97 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1605)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          803000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 941,543
Trainable params: 941,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 77ms/step - loss: 1.0795 - accuracy: 0.3918 - auc: 0.5738 - f1_score: 0.4173 - val_loss: 1.0667 - val_accuracy: 0.4095 - val_auc: 0.5983 - val_f1_score: 0.3832
Epoch 2/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0612 - accuracy: 0.4235 - auc: 0.6087 - f1_score: 0.4145 - val_loss: 1.0604 - val_accuracy: 0.4150 - val_auc: 0.6088 - val_f1_score: 0.4061
Epoch 3/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0536 - accuracy: 0.4321 - auc: 0.6199 - f1_score: 0.4243 - val_loss: 1.0564 - val_accuracy: 0.4266 - val_auc: 0.6166 - val_f1_score: 0.4206
Epoch 4/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0481 - accuracy: 0.4393 - auc: 0.6273 - f1_score: 0.4330 - val_loss: 1.0537 - val_accuracy: 0.4308 - val_auc: 0.6217 - val_f1_score: 0.4173
Epoch 5/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0456 - accuracy: 0.4408 - auc: 0.6310 - f1_score: 0.4321 - val_loss: 1.0533 - val_accuracy: 0.4366 - val_auc: 0.6221 - val_f1_score: 0.4321
Epoch 6/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0443 - accuracy: 0.4427 - auc: 0.6327 - f1_score: 0.4366 - val_loss: 1.0536 - val_accuracy: 0.4329 - val_auc: 0.6199 - val_f1_score: 0.4246
Epoch 7/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0422 - accuracy: 0.4474 - auc: 0.6358 - f1_score: 0.4422 - val_loss: 1.0539 - val_accuracy: 0.4308 - val_auc: 0.6219 - val_f1_score: 0.4232
Epoch 8/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0417 - accuracy: 0.4461 - auc: 0.6359 - f1_score: 0.4416 - val_loss: 1.0524 - val_accuracy: 0.4356 - val_auc: 0.6244 - val_f1_score: 0.4256
Epoch 9/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0400 - accuracy: 0.4500 - auc: 0.6385 - f1_score: 0.4433 - val_loss: 1.0514 - val_accuracy: 0.4373 - val_auc: 0.6257 - val_f1_score: 0.4340
Epoch 10/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0395 - accuracy: 0.4497 - auc: 0.6393 - f1_score: 0.4457 - val_loss: 1.0539 - val_accuracy: 0.4325 - val_auc: 0.6223 - val_f1_score: 0.4236
Epoch 11/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0384 - accuracy: 0.4501 - auc: 0.6410 - f1_score: 0.4432 - val_loss: 1.0530 - val_accuracy: 0.4336 - val_auc: 0.6239 - val_f1_score: 0.4279
Epoch 12/50
188/188 [==============================] - 15s 82ms/step - loss: 1.0379 - accuracy: 0.4538 - auc: 0.6419 - f1_score: 0.4499 - val_loss: 1.0521 - val_accuracy: 0.4311 - val_auc: 0.6241 - val_f1_score: 0.4235
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>94</th>
      <td>0.430483</td>
      <td>0.424461</td>
      <td>0.446549</td>
      <td>0.448432</td>
      <td>0.636666</td>
      <td>0.636886</td>
      <td>0.435103</td>
      <td>0.435771</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>95</th>
      <td>0.432867</td>
      <td>0.426762</td>
      <td>0.448130</td>
      <td>0.449618</td>
      <td>0.639364</td>
      <td>0.639276</td>
      <td>0.439364</td>
      <td>0.439770</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.434021</td>
      <td>0.427916</td>
      <td>0.453035</td>
      <td>0.454617</td>
      <td>0.643664</td>
      <td>0.643169</td>
      <td>0.440652</td>
      <td>0.441027</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.434346</td>
      <td>0.428122</td>
      <td>0.457788</td>
      <td>0.458341</td>
      <td>0.646887</td>
      <td>0.645916</td>
      <td>0.451154</td>
      <td>0.450799</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>98</th>
      <td>0.437370</td>
      <td>0.431073</td>
      <td>0.461843</td>
      <td>0.461771</td>
      <td>0.652634</td>
      <td>0.651135</td>
      <td>0.454193</td>
      <td>0.453133</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>98 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1805)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          903000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,041,543
Trainable params: 1,041,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 23s 100ms/step - loss: 1.0749 - accuracy: 0.3994 - auc: 0.5836 - f1_score: 0.4284 - val_loss: 1.0651 - val_accuracy: 0.4158 - val_auc: 0.6021 - val_f1_score: 0.3954
Epoch 2/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0581 - accuracy: 0.4241 - auc: 0.6119 - f1_score: 0.4104 - val_loss: 1.0593 - val_accuracy: 0.4285 - val_auc: 0.6148 - val_f1_score: 0.4199
Epoch 3/50
188/188 [==============================] - 18s 98ms/step - loss: 1.0509 - accuracy: 0.4331 - auc: 0.6228 - f1_score: 0.4216 - val_loss: 1.0579 - val_accuracy: 0.4277 - val_auc: 0.6163 - val_f1_score: 0.4168
Epoch 4/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0467 - accuracy: 0.4367 - auc: 0.6282 - f1_score: 0.4239 - val_loss: 1.0554 - val_accuracy: 0.4305 - val_auc: 0.6200 - val_f1_score: 0.4092
Epoch 5/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0444 - accuracy: 0.4404 - auc: 0.6316 - f1_score: 0.4269 - val_loss: 1.0549 - val_accuracy: 0.4321 - val_auc: 0.6208 - val_f1_score: 0.4122
Epoch 6/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0433 - accuracy: 0.4422 - auc: 0.6327 - f1_score: 0.4326 - val_loss: 1.0530 - val_accuracy: 0.4308 - val_auc: 0.6232 - val_f1_score: 0.4161
Epoch 7/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0420 - accuracy: 0.4438 - auc: 0.6352 - f1_score: 0.4356 - val_loss: 1.0526 - val_accuracy: 0.4300 - val_auc: 0.6243 - val_f1_score: 0.4185
Epoch 8/50
188/188 [==============================] - 20s 107ms/step - loss: 1.0401 - accuracy: 0.4452 - auc: 0.6365 - f1_score: 0.4362 - val_loss: 1.0530 - val_accuracy: 0.4278 - val_auc: 0.6237 - val_f1_score: 0.4245
Epoch 9/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0387 - accuracy: 0.4489 - auc: 0.6392 - f1_score: 0.4420 - val_loss: 1.0538 - val_accuracy: 0.4301 - val_auc: 0.6227 - val_f1_score: 0.4237
Epoch 10/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0376 - accuracy: 0.4493 - auc: 0.6408 - f1_score: 0.4428 - val_loss: 1.0531 - val_accuracy: 0.4329 - val_auc: 0.6230 - val_f1_score: 0.4239
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>95</th>
      <td>0.432867</td>
      <td>0.426762</td>
      <td>0.448130</td>
      <td>0.449618</td>
      <td>0.639364</td>
      <td>0.639276</td>
      <td>0.439364</td>
      <td>0.439770</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.434021</td>
      <td>0.427916</td>
      <td>0.453035</td>
      <td>0.454617</td>
      <td>0.643664</td>
      <td>0.643169</td>
      <td>0.440652</td>
      <td>0.441027</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.434346</td>
      <td>0.428122</td>
      <td>0.457788</td>
      <td>0.458341</td>
      <td>0.646887</td>
      <td>0.645916</td>
      <td>0.451154</td>
      <td>0.450799</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>98</th>
      <td>0.437370</td>
      <td>0.431073</td>
      <td>0.461843</td>
      <td>0.461771</td>
      <td>0.652634</td>
      <td>0.651135</td>
      <td>0.454193</td>
      <td>0.453133</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>99</th>
      <td>0.436894</td>
      <td>0.430485</td>
      <td>0.460175</td>
      <td>0.460164</td>
      <td>0.651650</td>
      <td>0.649923</td>
      <td>0.451448</td>
      <td>0.450417</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>99 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1003000     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,141,543
Trainable params: 1,141,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 20s 94ms/step - loss: 1.0730 - accuracy: 0.4057 - auc: 0.5885 - f1_score: 0.4243 - val_loss: 1.0649 - val_accuracy: 0.4107 - val_auc: 0.6023 - val_f1_score: 0.3948
Epoch 2/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0574 - accuracy: 0.4266 - auc: 0.6140 - f1_score: 0.4126 - val_loss: 1.0610 - val_accuracy: 0.4187 - val_auc: 0.6090 - val_f1_score: 0.3972
Epoch 3/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0512 - accuracy: 0.4360 - auc: 0.6231 - f1_score: 0.4201 - val_loss: 1.0574 - val_accuracy: 0.4295 - val_auc: 0.6152 - val_f1_score: 0.4133
Epoch 4/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0480 - accuracy: 0.4368 - auc: 0.6273 - f1_score: 0.4254 - val_loss: 1.0550 - val_accuracy: 0.4265 - val_auc: 0.6185 - val_f1_score: 0.4073
Epoch 5/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0446 - accuracy: 0.4406 - auc: 0.6313 - f1_score: 0.4279 - val_loss: 1.0560 - val_accuracy: 0.4270 - val_auc: 0.6174 - val_f1_score: 0.4179
Epoch 6/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0428 - accuracy: 0.4434 - auc: 0.6341 - f1_score: 0.4345 - val_loss: 1.0552 - val_accuracy: 0.4279 - val_auc: 0.6186 - val_f1_score: 0.4215
Epoch 7/50
188/188 [==============================] - 18s 93ms/step - loss: 1.0401 - accuracy: 0.4451 - auc: 0.6372 - f1_score: 0.4385 - val_loss: 1.0565 - val_accuracy: 0.4251 - val_auc: 0.6176 - val_f1_score: 0.4165
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>96</th>
      <td>0.434021</td>
      <td>0.427916</td>
      <td>0.453035</td>
      <td>0.454617</td>
      <td>0.643664</td>
      <td>0.643169</td>
      <td>0.440652</td>
      <td>0.441027</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.434346</td>
      <td>0.428122</td>
      <td>0.457788</td>
      <td>0.458341</td>
      <td>0.646887</td>
      <td>0.645916</td>
      <td>0.451154</td>
      <td>0.450799</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>98</th>
      <td>0.437370</td>
      <td>0.431073</td>
      <td>0.461843</td>
      <td>0.461771</td>
      <td>0.652634</td>
      <td>0.651135</td>
      <td>0.454193</td>
      <td>0.453133</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>99</th>
      <td>0.436894</td>
      <td>0.430485</td>
      <td>0.460175</td>
      <td>0.460164</td>
      <td>0.651650</td>
      <td>0.649923</td>
      <td>0.451448</td>
      <td>0.450417</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.436845</td>
      <td>0.430477</td>
      <td>0.456327</td>
      <td>0.457410</td>
      <td>0.649124</td>
      <td>0.647918</td>
      <td>0.445791</td>
      <td>0.445722</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>100 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 208)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          104500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 304,243
Trainable params: 304,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 8s 32ms/step - loss: 1.0832 - accuracy: 0.3757 - auc: 0.5587 - f1_score: 0.4111 - val_loss: 1.0747 - val_accuracy: 0.4210 - val_auc: 0.5933 - val_f1_score: 0.3825
Epoch 2/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0734 - accuracy: 0.4170 - auc: 0.5921 - f1_score: 0.4101 - val_loss: 1.0709 - val_accuracy: 0.4247 - val_auc: 0.6028 - val_f1_score: 0.4218
Epoch 3/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0700 - accuracy: 0.4222 - auc: 0.5988 - f1_score: 0.4212 - val_loss: 1.0690 - val_accuracy: 0.4299 - val_auc: 0.6056 - val_f1_score: 0.4299
Epoch 4/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0677 - accuracy: 0.4260 - auc: 0.6033 - f1_score: 0.4259 - val_loss: 1.0669 - val_accuracy: 0.4245 - val_auc: 0.6066 - val_f1_score: 0.4178
Epoch 5/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0636 - accuracy: 0.4275 - auc: 0.6112 - f1_score: 0.4283 - val_loss: 1.0599 - val_accuracy: 0.4293 - val_auc: 0.6194 - val_f1_score: 0.4274
Epoch 6/50
188/188 [==============================] - 5s 28ms/step - loss: 1.0559 - accuracy: 0.4302 - auc: 0.6185 - f1_score: 0.4246 - val_loss: 1.0544 - val_accuracy: 0.4332 - val_auc: 0.6217 - val_f1_score: 0.3923
Epoch 7/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0526 - accuracy: 0.4328 - auc: 0.6222 - f1_score: 0.4185 - val_loss: 1.0548 - val_accuracy: 0.4327 - val_auc: 0.6208 - val_f1_score: 0.4249
Epoch 8/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0521 - accuracy: 0.4322 - auc: 0.6222 - f1_score: 0.4253 - val_loss: 1.0544 - val_accuracy: 0.4296 - val_auc: 0.6217 - val_f1_score: 0.4195
Epoch 9/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0519 - accuracy: 0.4337 - auc: 0.6231 - f1_score: 0.4247 - val_loss: 1.0541 - val_accuracy: 0.4319 - val_auc: 0.6229 - val_f1_score: 0.4144
Epoch 10/50
188/188 [==============================] - 6s 30ms/step - loss: 1.0512 - accuracy: 0.4349 - auc: 0.6242 - f1_score: 0.4244 - val_loss: 1.0542 - val_accuracy: 0.4378 - val_auc: 0.6230 - val_f1_score: 0.4250
Epoch 11/50
188/188 [==============================] - 6s 30ms/step - loss: 1.0512 - accuracy: 0.4324 - auc: 0.6239 - f1_score: 0.4245 - val_loss: 1.0544 - val_accuracy: 0.4357 - val_auc: 0.6225 - val_f1_score: 0.4233
Epoch 12/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0509 - accuracy: 0.4361 - auc: 0.6247 - f1_score: 0.4288 - val_loss: 1.0541 - val_accuracy: 0.4373 - val_auc: 0.6232 - val_f1_score: 0.4221
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>97</th>
      <td>0.434346</td>
      <td>0.428122</td>
      <td>0.457788</td>
      <td>0.458341</td>
      <td>0.646887</td>
      <td>0.645916</td>
      <td>0.451154</td>
      <td>0.450799</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>98</th>
      <td>0.437370</td>
      <td>0.431073</td>
      <td>0.461843</td>
      <td>0.461771</td>
      <td>0.652634</td>
      <td>0.651135</td>
      <td>0.454193</td>
      <td>0.453133</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>99</th>
      <td>0.436894</td>
      <td>0.430485</td>
      <td>0.460175</td>
      <td>0.460164</td>
      <td>0.651650</td>
      <td>0.649923</td>
      <td>0.451448</td>
      <td>0.450417</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.436845</td>
      <td>0.430477</td>
      <td>0.456327</td>
      <td>0.457410</td>
      <td>0.649124</td>
      <td>0.647918</td>
      <td>0.445791</td>
      <td>0.445722</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>101</th>
      <td>0.429614</td>
      <td>0.423697</td>
      <td>0.439301</td>
      <td>0.441405</td>
      <td>0.629300</td>
      <td>0.629927</td>
      <td>0.423532</td>
      <td>0.424238</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>101 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 408)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          204500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 404,243
Trainable params: 404,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 49ms/step - loss: 1.0676 - accuracy: 0.4173 - auc: 0.5993 - f1_score: 0.4135 - val_loss: 1.0552 - val_accuracy: 0.4273 - val_auc: 0.6197 - val_f1_score: 0.3614
Epoch 2/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0517 - accuracy: 0.4327 - auc: 0.6221 - f1_score: 0.3815 - val_loss: 1.0552 - val_accuracy: 0.4287 - val_auc: 0.6205 - val_f1_score: 0.3767
Epoch 3/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0508 - accuracy: 0.4330 - auc: 0.6237 - f1_score: 0.3975 - val_loss: 1.0541 - val_accuracy: 0.4336 - val_auc: 0.6222 - val_f1_score: 0.4008
Epoch 4/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0499 - accuracy: 0.4354 - auc: 0.6253 - f1_score: 0.4076 - val_loss: 1.0535 - val_accuracy: 0.4363 - val_auc: 0.6231 - val_f1_score: 0.4165
Epoch 5/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0490 - accuracy: 0.4380 - auc: 0.6266 - f1_score: 0.4163 - val_loss: 1.0540 - val_accuracy: 0.4350 - val_auc: 0.6233 - val_f1_score: 0.4141
Epoch 6/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0484 - accuracy: 0.4378 - auc: 0.6272 - f1_score: 0.4149 - val_loss: 1.0534 - val_accuracy: 0.4385 - val_auc: 0.6234 - val_f1_score: 0.4242
Epoch 7/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0478 - accuracy: 0.4373 - auc: 0.6280 - f1_score: 0.4208 - val_loss: 1.0539 - val_accuracy: 0.4355 - val_auc: 0.6235 - val_f1_score: 0.4209
Epoch 8/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0475 - accuracy: 0.4384 - auc: 0.6288 - f1_score: 0.4263 - val_loss: 1.0532 - val_accuracy: 0.4376 - val_auc: 0.6234 - val_f1_score: 0.4273
Epoch 9/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0473 - accuracy: 0.4389 - auc: 0.6293 - f1_score: 0.4252 - val_loss: 1.0532 - val_accuracy: 0.4382 - val_auc: 0.6241 - val_f1_score: 0.4283
Epoch 10/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0474 - accuracy: 0.4389 - auc: 0.6291 - f1_score: 0.4279 - val_loss: 1.0533 - val_accuracy: 0.4408 - val_auc: 0.6238 - val_f1_score: 0.4290
Epoch 11/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0470 - accuracy: 0.4393 - auc: 0.6297 - f1_score: 0.4290 - val_loss: 1.0529 - val_accuracy: 0.4369 - val_auc: 0.6242 - val_f1_score: 0.4263
Epoch 12/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0466 - accuracy: 0.4398 - auc: 0.6298 - f1_score: 0.4300 - val_loss: 1.0538 - val_accuracy: 0.4378 - val_auc: 0.6232 - val_f1_score: 0.4251
Epoch 13/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0459 - accuracy: 0.4403 - auc: 0.6309 - f1_score: 0.4311 - val_loss: 1.0533 - val_accuracy: 0.4381 - val_auc: 0.6242 - val_f1_score: 0.4273
Epoch 14/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0463 - accuracy: 0.4409 - auc: 0.6299 - f1_score: 0.4323 - val_loss: 1.0536 - val_accuracy: 0.4363 - val_auc: 0.6232 - val_f1_score: 0.4299
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>98</th>
      <td>0.437370</td>
      <td>0.431073</td>
      <td>0.461843</td>
      <td>0.461771</td>
      <td>0.652634</td>
      <td>0.651135</td>
      <td>0.454193</td>
      <td>0.453133</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>99</th>
      <td>0.436894</td>
      <td>0.430485</td>
      <td>0.460175</td>
      <td>0.460164</td>
      <td>0.651650</td>
      <td>0.649923</td>
      <td>0.451448</td>
      <td>0.450417</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.436845</td>
      <td>0.430477</td>
      <td>0.456327</td>
      <td>0.457410</td>
      <td>0.649124</td>
      <td>0.647918</td>
      <td>0.445791</td>
      <td>0.445722</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>101</th>
      <td>0.429614</td>
      <td>0.423697</td>
      <td>0.439301</td>
      <td>0.441405</td>
      <td>0.629300</td>
      <td>0.629927</td>
      <td>0.423532</td>
      <td>0.424238</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.430745</td>
      <td>0.424774</td>
      <td>0.443247</td>
      <td>0.444561</td>
      <td>0.633224</td>
      <td>0.633390</td>
      <td>0.437358</td>
      <td>0.437671</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>102 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 608)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          304500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 504,243
Trainable params: 504,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 18s 75ms/step - loss: 1.0720 - accuracy: 0.4193 - auc: 0.5928 - f1_score: 0.4104 - val_loss: 1.0602 - val_accuracy: 0.4295 - val_auc: 0.6145 - val_f1_score: 0.3474
Epoch 2/50
188/188 [==============================] - 15s 77ms/step - loss: 1.0534 - accuracy: 0.4313 - auc: 0.6206 - f1_score: 0.3724 - val_loss: 1.0528 - val_accuracy: 0.4325 - val_auc: 0.6242 - val_f1_score: 0.3821
Epoch 3/50
188/188 [==============================] - 16s 88ms/step - loss: 1.0494 - accuracy: 0.4356 - auc: 0.6257 - f1_score: 0.3979 - val_loss: 1.0521 - val_accuracy: 0.4320 - val_auc: 0.6246 - val_f1_score: 0.3988
Epoch 4/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0480 - accuracy: 0.4400 - auc: 0.6273 - f1_score: 0.4136 - val_loss: 1.0512 - val_accuracy: 0.4350 - val_auc: 0.6261 - val_f1_score: 0.4134
Epoch 5/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0474 - accuracy: 0.4389 - auc: 0.6285 - f1_score: 0.4179 - val_loss: 1.0518 - val_accuracy: 0.4347 - val_auc: 0.6255 - val_f1_score: 0.4165
Epoch 6/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0470 - accuracy: 0.4408 - auc: 0.6295 - f1_score: 0.4237 - val_loss: 1.0514 - val_accuracy: 0.4351 - val_auc: 0.6256 - val_f1_score: 0.4240
Epoch 7/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0462 - accuracy: 0.4403 - auc: 0.6302 - f1_score: 0.4274 - val_loss: 1.0508 - val_accuracy: 0.4365 - val_auc: 0.6267 - val_f1_score: 0.4229
Epoch 8/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0452 - accuracy: 0.4412 - auc: 0.6313 - f1_score: 0.4283 - val_loss: 1.0517 - val_accuracy: 0.4362 - val_auc: 0.6260 - val_f1_score: 0.4228
Epoch 9/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0456 - accuracy: 0.4407 - auc: 0.6311 - f1_score: 0.4290 - val_loss: 1.0506 - val_accuracy: 0.4387 - val_auc: 0.6270 - val_f1_score: 0.4287
Epoch 10/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0452 - accuracy: 0.4399 - auc: 0.6319 - f1_score: 0.4281 - val_loss: 1.0514 - val_accuracy: 0.4362 - val_auc: 0.6260 - val_f1_score: 0.4283
Epoch 11/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0447 - accuracy: 0.4420 - auc: 0.6321 - f1_score: 0.4317 - val_loss: 1.0516 - val_accuracy: 0.4325 - val_auc: 0.6256 - val_f1_score: 0.4229
Epoch 12/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0444 - accuracy: 0.4414 - auc: 0.6334 - f1_score: 0.4312 - val_loss: 1.0506 - val_accuracy: 0.4359 - val_auc: 0.6266 - val_f1_score: 0.4268
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>99</th>
      <td>0.436894</td>
      <td>0.430485</td>
      <td>0.460175</td>
      <td>0.460164</td>
      <td>0.651650</td>
      <td>0.649923</td>
      <td>0.451448</td>
      <td>0.450417</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.436845</td>
      <td>0.430477</td>
      <td>0.456327</td>
      <td>0.457410</td>
      <td>0.649124</td>
      <td>0.647918</td>
      <td>0.445791</td>
      <td>0.445722</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>101</th>
      <td>0.429614</td>
      <td>0.423697</td>
      <td>0.439301</td>
      <td>0.441405</td>
      <td>0.629300</td>
      <td>0.629927</td>
      <td>0.423532</td>
      <td>0.424238</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.430745</td>
      <td>0.424774</td>
      <td>0.443247</td>
      <td>0.444561</td>
      <td>0.633224</td>
      <td>0.633390</td>
      <td>0.437358</td>
      <td>0.437671</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>103</th>
      <td>0.431683</td>
      <td>0.425778</td>
      <td>0.443181</td>
      <td>0.445463</td>
      <td>0.636262</td>
      <td>0.636863</td>
      <td>0.433566</td>
      <td>0.434699</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>103 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 808)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          404500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 604,243
Trainable params: 604,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 18s 77ms/step - loss: 1.0806 - accuracy: 0.3941 - auc: 0.5703 - f1_score: 0.4120 - val_loss: 1.0671 - val_accuracy: 0.4360 - val_auc: 0.6139 - val_f1_score: 0.4068
Epoch 2/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0612 - accuracy: 0.4350 - auc: 0.6132 - f1_score: 0.4118 - val_loss: 1.0523 - val_accuracy: 0.4381 - val_auc: 0.6256 - val_f1_score: 0.4126
Epoch 3/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0514 - accuracy: 0.4354 - auc: 0.6235 - f1_score: 0.4144 - val_loss: 1.0498 - val_accuracy: 0.4344 - val_auc: 0.6279 - val_f1_score: 0.4124
Epoch 4/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0483 - accuracy: 0.4395 - auc: 0.6277 - f1_score: 0.4256 - val_loss: 1.0495 - val_accuracy: 0.4391 - val_auc: 0.6284 - val_f1_score: 0.4233
Epoch 5/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0481 - accuracy: 0.4400 - auc: 0.6284 - f1_score: 0.4290 - val_loss: 1.0490 - val_accuracy: 0.4394 - val_auc: 0.6290 - val_f1_score: 0.4267
Epoch 6/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0458 - accuracy: 0.4408 - auc: 0.6306 - f1_score: 0.4311 - val_loss: 1.0491 - val_accuracy: 0.4386 - val_auc: 0.6291 - val_f1_score: 0.4304
Epoch 7/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0447 - accuracy: 0.4409 - auc: 0.6316 - f1_score: 0.4337 - val_loss: 1.0491 - val_accuracy: 0.4398 - val_auc: 0.6291 - val_f1_score: 0.4316
Epoch 8/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0456 - accuracy: 0.4416 - auc: 0.6312 - f1_score: 0.4326 - val_loss: 1.0493 - val_accuracy: 0.4374 - val_auc: 0.6292 - val_f1_score: 0.4310
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>100</th>
      <td>0.436845</td>
      <td>0.430477</td>
      <td>0.456327</td>
      <td>0.457410</td>
      <td>0.649124</td>
      <td>0.647918</td>
      <td>0.445791</td>
      <td>0.445722</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <th>101</th>
      <td>0.429614</td>
      <td>0.423697</td>
      <td>0.439301</td>
      <td>0.441405</td>
      <td>0.629300</td>
      <td>0.629927</td>
      <td>0.423532</td>
      <td>0.424238</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.430745</td>
      <td>0.424774</td>
      <td>0.443247</td>
      <td>0.444561</td>
      <td>0.633224</td>
      <td>0.633390</td>
      <td>0.437358</td>
      <td>0.437671</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>103</th>
      <td>0.431683</td>
      <td>0.425778</td>
      <td>0.443181</td>
      <td>0.445463</td>
      <td>0.636262</td>
      <td>0.636863</td>
      <td>0.433566</td>
      <td>0.434699</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>104</th>
      <td>0.430435</td>
      <td>0.424482</td>
      <td>0.446103</td>
      <td>0.447648</td>
      <td>0.636771</td>
      <td>0.637175</td>
      <td>0.439994</td>
      <td>0.440575</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>104 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          504500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 704,243
Trainable params: 704,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 28s 110ms/step - loss: 1.0650 - accuracy: 0.4255 - auc: 0.6051 - f1_score: 0.4228 - val_loss: 1.0519 - val_accuracy: 0.4319 - val_auc: 0.6239 - val_f1_score: 0.3981
Epoch 2/50
188/188 [==============================] - 19s 104ms/step - loss: 1.0495 - accuracy: 0.4362 - auc: 0.6252 - f1_score: 0.4038 - val_loss: 1.0505 - val_accuracy: 0.4368 - val_auc: 0.6261 - val_f1_score: 0.3981
Epoch 3/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0472 - accuracy: 0.4402 - auc: 0.6289 - f1_score: 0.4129 - val_loss: 1.0500 - val_accuracy: 0.4378 - val_auc: 0.6272 - val_f1_score: 0.4277
Epoch 4/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0457 - accuracy: 0.4432 - auc: 0.6316 - f1_score: 0.4294 - val_loss: 1.0504 - val_accuracy: 0.4327 - val_auc: 0.6269 - val_f1_score: 0.4066
Epoch 5/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0454 - accuracy: 0.4436 - auc: 0.6321 - f1_score: 0.4278 - val_loss: 1.0497 - val_accuracy: 0.4391 - val_auc: 0.6272 - val_f1_score: 0.4278
Epoch 6/50
188/188 [==============================] - 21s 111ms/step - loss: 1.0438 - accuracy: 0.4443 - auc: 0.6337 - f1_score: 0.4322 - val_loss: 1.0496 - val_accuracy: 0.4369 - val_auc: 0.6277 - val_f1_score: 0.4314
Epoch 7/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0440 - accuracy: 0.4457 - auc: 0.6341 - f1_score: 0.4374 - val_loss: 1.0496 - val_accuracy: 0.4373 - val_auc: 0.6281 - val_f1_score: 0.4289
Epoch 8/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0429 - accuracy: 0.4472 - auc: 0.6349 - f1_score: 0.4392 - val_loss: 1.0495 - val_accuracy: 0.4351 - val_auc: 0.6283 - val_f1_score: 0.4233
Epoch 9/50
188/188 [==============================] - 27s 146ms/step - loss: 1.0425 - accuracy: 0.4475 - auc: 0.6357 - f1_score: 0.4385 - val_loss: 1.0503 - val_accuracy: 0.4363 - val_auc: 0.6275 - val_f1_score: 0.4257
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>101</th>
      <td>0.429614</td>
      <td>0.423697</td>
      <td>0.439301</td>
      <td>0.441405</td>
      <td>0.629300</td>
      <td>0.629927</td>
      <td>0.423532</td>
      <td>0.424238</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.430745</td>
      <td>0.424774</td>
      <td>0.443247</td>
      <td>0.444561</td>
      <td>0.633224</td>
      <td>0.633390</td>
      <td>0.437358</td>
      <td>0.437671</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>103</th>
      <td>0.431683</td>
      <td>0.425778</td>
      <td>0.443181</td>
      <td>0.445463</td>
      <td>0.636262</td>
      <td>0.636863</td>
      <td>0.433566</td>
      <td>0.434699</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>104</th>
      <td>0.430435</td>
      <td>0.424482</td>
      <td>0.446103</td>
      <td>0.447648</td>
      <td>0.636771</td>
      <td>0.637175</td>
      <td>0.439994</td>
      <td>0.440575</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>105</th>
      <td>0.435130</td>
      <td>0.429273</td>
      <td>0.450528</td>
      <td>0.452715</td>
      <td>0.640019</td>
      <td>0.640486</td>
      <td>0.439367</td>
      <td>0.440342</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>105 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1208)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          604500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 804,243
Trainable params: 804,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 28s 128ms/step - loss: 1.0687 - accuracy: 0.4176 - auc: 0.5983 - f1_score: 0.4163 - val_loss: 1.0502 - val_accuracy: 0.4398 - val_auc: 0.6269 - val_f1_score: 0.4271
Epoch 2/50
188/188 [==============================] - 21s 114ms/step - loss: 1.0476 - accuracy: 0.4394 - auc: 0.6287 - f1_score: 0.4303 - val_loss: 1.0486 - val_accuracy: 0.4388 - val_auc: 0.6292 - val_f1_score: 0.4333
Epoch 3/50
188/188 [==============================] - 21s 111ms/step - loss: 1.0447 - accuracy: 0.4440 - auc: 0.6323 - f1_score: 0.4350 - val_loss: 1.0485 - val_accuracy: 0.4390 - val_auc: 0.6303 - val_f1_score: 0.4325
Epoch 4/50
188/188 [==============================] - 22s 119ms/step - loss: 1.0444 - accuracy: 0.4452 - auc: 0.6332 - f1_score: 0.4388 - val_loss: 1.0483 - val_accuracy: 0.4397 - val_auc: 0.6304 - val_f1_score: 0.4317
Epoch 5/50
188/188 [==============================] - 22s 116ms/step - loss: 1.0437 - accuracy: 0.4459 - auc: 0.6345 - f1_score: 0.4385 - val_loss: 1.0484 - val_accuracy: 0.4380 - val_auc: 0.6300 - val_f1_score: 0.4309
Epoch 6/50
188/188 [==============================] - 22s 119ms/step - loss: 1.0432 - accuracy: 0.4476 - auc: 0.6353 - f1_score: 0.4399 - val_loss: 1.0477 - val_accuracy: 0.4384 - val_auc: 0.6308 - val_f1_score: 0.4310
Epoch 7/50
188/188 [==============================] - 23s 125ms/step - loss: 1.0420 - accuracy: 0.4490 - auc: 0.6367 - f1_score: 0.4421 - val_loss: 1.0483 - val_accuracy: 0.4378 - val_auc: 0.6301 - val_f1_score: 0.4288
Epoch 8/50
188/188 [==============================] - 23s 122ms/step - loss: 1.0414 - accuracy: 0.4489 - auc: 0.6377 - f1_score: 0.4415 - val_loss: 1.0484 - val_accuracy: 0.4409 - val_auc: 0.6301 - val_f1_score: 0.4373
Epoch 9/50
188/188 [==============================] - 22s 116ms/step - loss: 1.0407 - accuracy: 0.4498 - auc: 0.6382 - f1_score: 0.4436 - val_loss: 1.0481 - val_accuracy: 0.4347 - val_auc: 0.6300 - val_f1_score: 0.4294
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>102</th>
      <td>0.430745</td>
      <td>0.424774</td>
      <td>0.443247</td>
      <td>0.444561</td>
      <td>0.633224</td>
      <td>0.633390</td>
      <td>0.437358</td>
      <td>0.437671</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>103</th>
      <td>0.431683</td>
      <td>0.425778</td>
      <td>0.443181</td>
      <td>0.445463</td>
      <td>0.636262</td>
      <td>0.636863</td>
      <td>0.433566</td>
      <td>0.434699</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>104</th>
      <td>0.430435</td>
      <td>0.424482</td>
      <td>0.446103</td>
      <td>0.447648</td>
      <td>0.636771</td>
      <td>0.637175</td>
      <td>0.439994</td>
      <td>0.440575</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>105</th>
      <td>0.435130</td>
      <td>0.429273</td>
      <td>0.450528</td>
      <td>0.452715</td>
      <td>0.640019</td>
      <td>0.640486</td>
      <td>0.439367</td>
      <td>0.440342</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>106</th>
      <td>0.432948</td>
      <td>0.426958</td>
      <td>0.453199</td>
      <td>0.454342</td>
      <td>0.643384</td>
      <td>0.643264</td>
      <td>0.447778</td>
      <td>0.448015</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>106 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1408)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          704500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 904,243
Trainable params: 904,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 32s 135ms/step - loss: 1.0697 - accuracy: 0.4164 - auc: 0.5991 - f1_score: 0.4333 - val_loss: 1.0506 - val_accuracy: 0.4361 - val_auc: 0.6267 - val_f1_score: 0.4043
Epoch 2/50
188/188 [==============================] - 23s 122ms/step - loss: 1.0481 - accuracy: 0.4375 - auc: 0.6275 - f1_score: 0.4158 - val_loss: 1.0490 - val_accuracy: 0.4410 - val_auc: 0.6284 - val_f1_score: 0.4237
Epoch 3/50
188/188 [==============================] - 23s 120ms/step - loss: 1.0459 - accuracy: 0.4410 - auc: 0.6306 - f1_score: 0.4233 - val_loss: 1.0489 - val_accuracy: 0.4421 - val_auc: 0.6289 - val_f1_score: 0.4258
Epoch 4/50
188/188 [==============================] - 23s 122ms/step - loss: 1.0447 - accuracy: 0.4404 - auc: 0.6319 - f1_score: 0.4251 - val_loss: 1.0485 - val_accuracy: 0.4392 - val_auc: 0.6290 - val_f1_score: 0.4215
Epoch 5/50
188/188 [==============================] - 23s 123ms/step - loss: 1.0432 - accuracy: 0.4449 - auc: 0.6344 - f1_score: 0.4341 - val_loss: 1.0486 - val_accuracy: 0.4424 - val_auc: 0.6295 - val_f1_score: 0.4284
Epoch 6/50
188/188 [==============================] - 23s 120ms/step - loss: 1.0427 - accuracy: 0.4461 - auc: 0.6356 - f1_score: 0.4359 - val_loss: 1.0485 - val_accuracy: 0.4415 - val_auc: 0.6293 - val_f1_score: 0.4319
Epoch 7/50
188/188 [==============================] - 24s 126ms/step - loss: 1.0425 - accuracy: 0.4457 - auc: 0.6352 - f1_score: 0.4373 - val_loss: 1.0489 - val_accuracy: 0.4403 - val_auc: 0.6291 - val_f1_score: 0.4287
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>103</th>
      <td>0.431683</td>
      <td>0.425778</td>
      <td>0.443181</td>
      <td>0.445463</td>
      <td>0.636262</td>
      <td>0.636863</td>
      <td>0.433566</td>
      <td>0.434699</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>104</th>
      <td>0.430435</td>
      <td>0.424482</td>
      <td>0.446103</td>
      <td>0.447648</td>
      <td>0.636771</td>
      <td>0.637175</td>
      <td>0.439994</td>
      <td>0.440575</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>105</th>
      <td>0.435130</td>
      <td>0.429273</td>
      <td>0.450528</td>
      <td>0.452715</td>
      <td>0.640019</td>
      <td>0.640486</td>
      <td>0.439367</td>
      <td>0.440342</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>106</th>
      <td>0.432948</td>
      <td>0.426958</td>
      <td>0.453199</td>
      <td>0.454342</td>
      <td>0.643384</td>
      <td>0.643264</td>
      <td>0.447778</td>
      <td>0.448015</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>107</th>
      <td>0.434152</td>
      <td>0.428299</td>
      <td>0.450092</td>
      <td>0.452431</td>
      <td>0.640481</td>
      <td>0.641050</td>
      <td>0.437035</td>
      <td>0.438128</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>107 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1608)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          804500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,004,243
Trainable params: 1,004,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 32s 143ms/step - loss: 1.0685 - accuracy: 0.4149 - auc: 0.6004 - f1_score: 0.4153 - val_loss: 1.0518 - val_accuracy: 0.4349 - val_auc: 0.6253 - val_f1_score: 0.4053
Epoch 2/50
188/188 [==============================] - 24s 126ms/step - loss: 1.0470 - accuracy: 0.4412 - auc: 0.6296 - f1_score: 0.4238 - val_loss: 1.0488 - val_accuracy: 0.4394 - val_auc: 0.6292 - val_f1_score: 0.4268
Epoch 3/50
188/188 [==============================] - 23s 125ms/step - loss: 1.0447 - accuracy: 0.4442 - auc: 0.6324 - f1_score: 0.4319 - val_loss: 1.0485 - val_accuracy: 0.4375 - val_auc: 0.6296 - val_f1_score: 0.4257
Epoch 4/50
188/188 [==============================] - 24s 129ms/step - loss: 1.0428 - accuracy: 0.4467 - auc: 0.6352 - f1_score: 0.4363 - val_loss: 1.0481 - val_accuracy: 0.4375 - val_auc: 0.6301 - val_f1_score: 0.4276
Epoch 5/50
188/188 [==============================] - 23s 125ms/step - loss: 1.0422 - accuracy: 0.4477 - auc: 0.6363 - f1_score: 0.4375 - val_loss: 1.0486 - val_accuracy: 0.4366 - val_auc: 0.6291 - val_f1_score: 0.4342
Epoch 6/50
188/188 [==============================] - 23s 121ms/step - loss: 1.0417 - accuracy: 0.4465 - auc: 0.6365 - f1_score: 0.4383 - val_loss: 1.0493 - val_accuracy: 0.4365 - val_auc: 0.6287 - val_f1_score: 0.4298
Epoch 7/50
188/188 [==============================] - 23s 124ms/step - loss: 1.0407 - accuracy: 0.4517 - auc: 0.6382 - f1_score: 0.4437 - val_loss: 1.0486 - val_accuracy: 0.4365 - val_auc: 0.6294 - val_f1_score: 0.4300
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>104</th>
      <td>0.430435</td>
      <td>0.424482</td>
      <td>0.446103</td>
      <td>0.447648</td>
      <td>0.636771</td>
      <td>0.637175</td>
      <td>0.439994</td>
      <td>0.440575</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>105</th>
      <td>0.435130</td>
      <td>0.429273</td>
      <td>0.450528</td>
      <td>0.452715</td>
      <td>0.640019</td>
      <td>0.640486</td>
      <td>0.439367</td>
      <td>0.440342</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>106</th>
      <td>0.432948</td>
      <td>0.426958</td>
      <td>0.453199</td>
      <td>0.454342</td>
      <td>0.643384</td>
      <td>0.643264</td>
      <td>0.447778</td>
      <td>0.448015</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>107</th>
      <td>0.434152</td>
      <td>0.428299</td>
      <td>0.450092</td>
      <td>0.452431</td>
      <td>0.640481</td>
      <td>0.641050</td>
      <td>0.437035</td>
      <td>0.438128</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0.434052</td>
      <td>0.428132</td>
      <td>0.455074</td>
      <td>0.456459</td>
      <td>0.643914</td>
      <td>0.644040</td>
      <td>0.447814</td>
      <td>0.448161</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>108 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,104,243
Trainable params: 1,104,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 37s 169ms/step - loss: 1.0738 - accuracy: 0.3776 - auc: 0.5878 - f1_score: 0.4232 - val_loss: 1.0596 - val_accuracy: 0.4289 - val_auc: 0.6112 - val_f1_score: 0.3756
Epoch 2/50
188/188 [==============================] - 31s 167ms/step - loss: 1.0521 - accuracy: 0.4344 - auc: 0.6222 - f1_score: 0.3846 - val_loss: 1.0516 - val_accuracy: 0.4303 - val_auc: 0.6244 - val_f1_score: 0.3699
Epoch 3/50
188/188 [==============================] - 32s 170ms/step - loss: 1.0467 - accuracy: 0.4395 - auc: 0.6297 - f1_score: 0.3994 - val_loss: 1.0497 - val_accuracy: 0.4373 - val_auc: 0.6274 - val_f1_score: 0.4093
Epoch 4/50
188/188 [==============================] - 30s 158ms/step - loss: 1.0445 - accuracy: 0.4439 - auc: 0.6329 - f1_score: 0.4264 - val_loss: 1.0495 - val_accuracy: 0.4417 - val_auc: 0.6285 - val_f1_score: 0.4242
Epoch 5/50
188/188 [==============================] - 32s 172ms/step - loss: 1.0435 - accuracy: 0.4459 - auc: 0.6346 - f1_score: 0.4341 - val_loss: 1.0487 - val_accuracy: 0.4397 - val_auc: 0.6293 - val_f1_score: 0.4311
Epoch 6/50
188/188 [==============================] - 32s 173ms/step - loss: 1.0419 - accuracy: 0.4472 - auc: 0.6365 - f1_score: 0.4375 - val_loss: 1.0490 - val_accuracy: 0.4402 - val_auc: 0.6290 - val_f1_score: 0.4295
Epoch 7/50
188/188 [==============================] - 28s 151ms/step - loss: 1.0411 - accuracy: 0.4493 - auc: 0.6376 - f1_score: 0.4394 - val_loss: 1.0489 - val_accuracy: 0.4380 - val_auc: 0.6287 - val_f1_score: 0.4344
Epoch 8/50
188/188 [==============================] - 31s 164ms/step - loss: 1.0408 - accuracy: 0.4511 - auc: 0.6386 - f1_score: 0.4448 - val_loss: 1.0497 - val_accuracy: 0.4381 - val_auc: 0.6276 - val_f1_score: 0.4212
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>105</th>
      <td>0.435130</td>
      <td>0.429273</td>
      <td>0.450528</td>
      <td>0.452715</td>
      <td>0.640019</td>
      <td>0.640486</td>
      <td>0.439367</td>
      <td>0.440342</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>106</th>
      <td>0.432948</td>
      <td>0.426958</td>
      <td>0.453199</td>
      <td>0.454342</td>
      <td>0.643384</td>
      <td>0.643264</td>
      <td>0.447778</td>
      <td>0.448015</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>107</th>
      <td>0.434152</td>
      <td>0.428299</td>
      <td>0.450092</td>
      <td>0.452431</td>
      <td>0.640481</td>
      <td>0.641050</td>
      <td>0.437035</td>
      <td>0.438128</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0.434052</td>
      <td>0.428132</td>
      <td>0.455074</td>
      <td>0.456459</td>
      <td>0.643914</td>
      <td>0.644040</td>
      <td>0.447814</td>
      <td>0.448161</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.434399</td>
      <td>0.428598</td>
      <td>0.454616</td>
      <td>0.456734</td>
      <td>0.643582</td>
      <td>0.644277</td>
      <td>0.436821</td>
      <td>0.437494</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>109 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      40800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      40800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,204,243
Trainable params: 1,204,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 38s 176ms/step - loss: 1.0654 - accuracy: 0.4225 - auc: 0.6048 - f1_score: 0.4254 - val_loss: 1.0508 - val_accuracy: 0.4347 - val_auc: 0.6251 - val_f1_score: 0.3985
Epoch 2/50
188/188 [==============================] - 33s 176ms/step - loss: 1.0473 - accuracy: 0.4394 - auc: 0.6289 - f1_score: 0.4202 - val_loss: 1.0490 - val_accuracy: 0.4391 - val_auc: 0.6290 - val_f1_score: 0.4217
Epoch 3/50
188/188 [==============================] - 31s 165ms/step - loss: 1.0443 - accuracy: 0.4435 - auc: 0.6330 - f1_score: 0.4280 - val_loss: 1.0502 - val_accuracy: 0.4362 - val_auc: 0.6278 - val_f1_score: 0.4187
Epoch 4/50
188/188 [==============================] - 33s 173ms/step - loss: 1.0425 - accuracy: 0.4463 - auc: 0.6359 - f1_score: 0.4352 - val_loss: 1.0488 - val_accuracy: 0.4368 - val_auc: 0.6294 - val_f1_score: 0.4227
Epoch 5/50
188/188 [==============================] - 38s 204ms/step - loss: 1.0420 - accuracy: 0.4469 - auc: 0.6362 - f1_score: 0.4360 - val_loss: 1.0489 - val_accuracy: 0.4398 - val_auc: 0.6296 - val_f1_score: 0.4296
Epoch 6/50
188/188 [==============================] - 35s 184ms/step - loss: 1.0404 - accuracy: 0.4497 - auc: 0.6386 - f1_score: 0.4412 - val_loss: 1.0488 - val_accuracy: 0.4378 - val_auc: 0.6295 - val_f1_score: 0.4312
Epoch 7/50
188/188 [==============================] - 33s 176ms/step - loss: 1.0400 - accuracy: 0.4511 - auc: 0.6398 - f1_score: 0.4426 - val_loss: 1.0489 - val_accuracy: 0.4387 - val_auc: 0.6296 - val_f1_score: 0.4326
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>106</th>
      <td>0.432948</td>
      <td>0.426958</td>
      <td>0.453199</td>
      <td>0.454342</td>
      <td>0.643384</td>
      <td>0.643264</td>
      <td>0.447778</td>
      <td>0.448015</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>107</th>
      <td>0.434152</td>
      <td>0.428299</td>
      <td>0.450092</td>
      <td>0.452431</td>
      <td>0.640481</td>
      <td>0.641050</td>
      <td>0.437035</td>
      <td>0.438128</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0.434052</td>
      <td>0.428132</td>
      <td>0.455074</td>
      <td>0.456459</td>
      <td>0.643914</td>
      <td>0.644040</td>
      <td>0.447814</td>
      <td>0.448161</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.434399</td>
      <td>0.428598</td>
      <td>0.454616</td>
      <td>0.456734</td>
      <td>0.643582</td>
      <td>0.644277</td>
      <td>0.436821</td>
      <td>0.437494</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.435845</td>
      <td>0.429846</td>
      <td>0.455946</td>
      <td>0.457214</td>
      <td>0.645521</td>
      <td>0.645328</td>
      <td>0.449321</td>
      <td>0.449602</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>110 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 1, 2)
Test shape: (10292, 1, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 208)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          104500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 243,043
Trainable params: 243,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 36ms/step - loss: 1.0850 - accuracy: 0.3896 - auc: 0.5742 - f1_score: 0.4281 - val_loss: 1.0645 - val_accuracy: 0.4269 - val_auc: 0.6112 - val_f1_score: 0.3668
Epoch 2/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0589 - accuracy: 0.4276 - auc: 0.6136 - f1_score: 0.4074 - val_loss: 1.0557 - val_accuracy: 0.4321 - val_auc: 0.6202 - val_f1_score: 0.3859
Epoch 3/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0546 - accuracy: 0.4326 - auc: 0.6197 - f1_score: 0.4109 - val_loss: 1.0550 - val_accuracy: 0.4313 - val_auc: 0.6210 - val_f1_score: 0.4086
Epoch 4/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0529 - accuracy: 0.4328 - auc: 0.6214 - f1_score: 0.4163 - val_loss: 1.0549 - val_accuracy: 0.4313 - val_auc: 0.6211 - val_f1_score: 0.4035
Epoch 5/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0522 - accuracy: 0.4360 - auc: 0.6232 - f1_score: 0.4189 - val_loss: 1.0541 - val_accuracy: 0.4371 - val_auc: 0.6230 - val_f1_score: 0.4106
Epoch 6/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0512 - accuracy: 0.4353 - auc: 0.6242 - f1_score: 0.4184 - val_loss: 1.0538 - val_accuracy: 0.4323 - val_auc: 0.6230 - val_f1_score: 0.4140
Epoch 7/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0505 - accuracy: 0.4358 - auc: 0.6246 - f1_score: 0.4217 - val_loss: 1.0539 - val_accuracy: 0.4375 - val_auc: 0.6237 - val_f1_score: 0.4212
Epoch 8/50
188/188 [==============================] - 5s 26ms/step - loss: 1.0506 - accuracy: 0.4359 - auc: 0.6246 - f1_score: 0.4236 - val_loss: 1.0538 - val_accuracy: 0.4373 - val_auc: 0.6237 - val_f1_score: 0.4246
Epoch 9/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0498 - accuracy: 0.4363 - auc: 0.6257 - f1_score: 0.4238 - val_loss: 1.0542 - val_accuracy: 0.4373 - val_auc: 0.6235 - val_f1_score: 0.4213
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>107</th>
      <td>0.434152</td>
      <td>0.428299</td>
      <td>0.450092</td>
      <td>0.452431</td>
      <td>0.640481</td>
      <td>0.641050</td>
      <td>0.437035</td>
      <td>0.438128</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0.434052</td>
      <td>0.428132</td>
      <td>0.455074</td>
      <td>0.456459</td>
      <td>0.643914</td>
      <td>0.644040</td>
      <td>0.447814</td>
      <td>0.448161</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.434399</td>
      <td>0.428598</td>
      <td>0.454616</td>
      <td>0.456734</td>
      <td>0.643582</td>
      <td>0.644277</td>
      <td>0.436821</td>
      <td>0.437494</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.435845</td>
      <td>0.429846</td>
      <td>0.455946</td>
      <td>0.457214</td>
      <td>0.645521</td>
      <td>0.645328</td>
      <td>0.449321</td>
      <td>0.449602</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.430151</td>
      <td>0.424279</td>
      <td>0.440423</td>
      <td>0.442522</td>
      <td>0.629295</td>
      <td>0.630061</td>
      <td>0.423756</td>
      <td>0.424412</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>111 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 2, 2)
Test shape: (10292, 2, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 408)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          204500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 343,043
Trainable params: 343,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 62ms/step - loss: 1.0790 - accuracy: 0.3908 - auc: 0.5769 - f1_score: 0.4098 - val_loss: 1.0593 - val_accuracy: 0.4290 - val_auc: 0.6136 - val_f1_score: 0.3896
Epoch 2/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0585 - accuracy: 0.4244 - auc: 0.6128 - f1_score: 0.4000 - val_loss: 1.0554 - val_accuracy: 0.4285 - val_auc: 0.6199 - val_f1_score: 0.3827
Epoch 3/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0539 - accuracy: 0.4300 - auc: 0.6194 - f1_score: 0.4044 - val_loss: 1.0533 - val_accuracy: 0.4361 - val_auc: 0.6235 - val_f1_score: 0.4091
Epoch 4/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0510 - accuracy: 0.4351 - auc: 0.6244 - f1_score: 0.4188 - val_loss: 1.0540 - val_accuracy: 0.4351 - val_auc: 0.6229 - val_f1_score: 0.4031
Epoch 5/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0498 - accuracy: 0.4363 - auc: 0.6253 - f1_score: 0.4206 - val_loss: 1.0536 - val_accuracy: 0.4373 - val_auc: 0.6235 - val_f1_score: 0.4104
Epoch 6/50
188/188 [==============================] - 7s 40ms/step - loss: 1.0490 - accuracy: 0.4388 - auc: 0.6268 - f1_score: 0.4213 - val_loss: 1.0534 - val_accuracy: 0.4355 - val_auc: 0.6239 - val_f1_score: 0.4158
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>108</th>
      <td>0.434052</td>
      <td>0.428132</td>
      <td>0.455074</td>
      <td>0.456459</td>
      <td>0.643914</td>
      <td>0.644040</td>
      <td>0.447814</td>
      <td>0.448161</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.434399</td>
      <td>0.428598</td>
      <td>0.454616</td>
      <td>0.456734</td>
      <td>0.643582</td>
      <td>0.644277</td>
      <td>0.436821</td>
      <td>0.437494</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.435845</td>
      <td>0.429846</td>
      <td>0.455946</td>
      <td>0.457214</td>
      <td>0.645521</td>
      <td>0.645328</td>
      <td>0.449321</td>
      <td>0.449602</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.430151</td>
      <td>0.424279</td>
      <td>0.440423</td>
      <td>0.442522</td>
      <td>0.629295</td>
      <td>0.630061</td>
      <td>0.423756</td>
      <td>0.424412</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.430834</td>
      <td>0.424930</td>
      <td>0.443465</td>
      <td>0.445678</td>
      <td>0.632982</td>
      <td>0.633522</td>
      <td>0.424672</td>
      <td>0.425438</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>112 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 3, 2)
Test shape: (10292, 3, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 608)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          304500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 443,043
Trainable params: 443,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 59ms/step - loss: 1.0804 - accuracy: 0.3719 - auc: 0.5755 - f1_score: 0.4098 - val_loss: 1.0662 - val_accuracy: 0.3860 - val_auc: 0.6015 - val_f1_score: 0.3222
Epoch 2/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0615 - accuracy: 0.4060 - auc: 0.6082 - f1_score: 0.3997 - val_loss: 1.0559 - val_accuracy: 0.4275 - val_auc: 0.6177 - val_f1_score: 0.4040
Epoch 3/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0520 - accuracy: 0.4329 - auc: 0.6217 - f1_score: 0.4163 - val_loss: 1.0522 - val_accuracy: 0.4323 - val_auc: 0.6226 - val_f1_score: 0.4113
Epoch 4/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0499 - accuracy: 0.4356 - auc: 0.6249 - f1_score: 0.4188 - val_loss: 1.0539 - val_accuracy: 0.4324 - val_auc: 0.6218 - val_f1_score: 0.4167
Epoch 5/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0488 - accuracy: 0.4364 - auc: 0.6262 - f1_score: 0.4206 - val_loss: 1.0525 - val_accuracy: 0.4293 - val_auc: 0.6239 - val_f1_score: 0.4156
Epoch 6/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0471 - accuracy: 0.4378 - auc: 0.6286 - f1_score: 0.4232 - val_loss: 1.0533 - val_accuracy: 0.4338 - val_auc: 0.6229 - val_f1_score: 0.4155
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>109</th>
      <td>0.434399</td>
      <td>0.428598</td>
      <td>0.454616</td>
      <td>0.456734</td>
      <td>0.643582</td>
      <td>0.644277</td>
      <td>0.436821</td>
      <td>0.437494</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.435845</td>
      <td>0.429846</td>
      <td>0.455946</td>
      <td>0.457214</td>
      <td>0.645521</td>
      <td>0.645328</td>
      <td>0.449321</td>
      <td>0.449602</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.430151</td>
      <td>0.424279</td>
      <td>0.440423</td>
      <td>0.442522</td>
      <td>0.629295</td>
      <td>0.630061</td>
      <td>0.423756</td>
      <td>0.424412</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.430834</td>
      <td>0.424930</td>
      <td>0.443465</td>
      <td>0.445678</td>
      <td>0.632982</td>
      <td>0.633522</td>
      <td>0.424672</td>
      <td>0.425438</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.431771</td>
      <td>0.425934</td>
      <td>0.441448</td>
      <td>0.444091</td>
      <td>0.634050</td>
      <td>0.635056</td>
      <td>0.422752</td>
      <td>0.423947</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>113 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 4, 2)
Test shape: (10292, 4, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 808)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          404500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 543,043
Trainable params: 543,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 59ms/step - loss: 1.0898 - accuracy: 0.3715 - auc: 0.5495 - f1_score: 0.3996 - val_loss: 1.0738 - val_accuracy: 0.4026 - val_auc: 0.5914 - val_f1_score: 0.3867
Epoch 2/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0661 - accuracy: 0.4167 - auc: 0.6012 - f1_score: 0.3991 - val_loss: 1.0585 - val_accuracy: 0.4248 - val_auc: 0.6129 - val_f1_score: 0.3628
Epoch 3/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0560 - accuracy: 0.4283 - auc: 0.6153 - f1_score: 0.3928 - val_loss: 1.0544 - val_accuracy: 0.4275 - val_auc: 0.6193 - val_f1_score: 0.3767
Epoch 4/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0524 - accuracy: 0.4333 - auc: 0.6214 - f1_score: 0.3908 - val_loss: 1.0525 - val_accuracy: 0.4339 - val_auc: 0.6221 - val_f1_score: 0.3848
Epoch 5/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0493 - accuracy: 0.4356 - auc: 0.6254 - f1_score: 0.3930 - val_loss: 1.0515 - val_accuracy: 0.4341 - val_auc: 0.6253 - val_f1_score: 0.3920
Epoch 6/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0474 - accuracy: 0.4366 - auc: 0.6273 - f1_score: 0.3984 - val_loss: 1.0517 - val_accuracy: 0.4369 - val_auc: 0.6251 - val_f1_score: 0.4017
Epoch 7/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0464 - accuracy: 0.4383 - auc: 0.6295 - f1_score: 0.4074 - val_loss: 1.0511 - val_accuracy: 0.4386 - val_auc: 0.6265 - val_f1_score: 0.4109
Epoch 8/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0461 - accuracy: 0.4415 - auc: 0.6302 - f1_score: 0.4182 - val_loss: 1.0516 - val_accuracy: 0.4402 - val_auc: 0.6257 - val_f1_score: 0.4198
Epoch 9/50
188/188 [==============================] - 11s 56ms/step - loss: 1.0446 - accuracy: 0.4435 - auc: 0.6322 - f1_score: 0.4256 - val_loss: 1.0517 - val_accuracy: 0.4337 - val_auc: 0.6248 - val_f1_score: 0.4180
Epoch 10/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0435 - accuracy: 0.4422 - auc: 0.6335 - f1_score: 0.4287 - val_loss: 1.0514 - val_accuracy: 0.4348 - val_auc: 0.6259 - val_f1_score: 0.4182
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>110</th>
      <td>0.435845</td>
      <td>0.429846</td>
      <td>0.455946</td>
      <td>0.457214</td>
      <td>0.645521</td>
      <td>0.645328</td>
      <td>0.449321</td>
      <td>0.449602</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.430151</td>
      <td>0.424279</td>
      <td>0.440423</td>
      <td>0.442522</td>
      <td>0.629295</td>
      <td>0.630061</td>
      <td>0.423756</td>
      <td>0.424412</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.430834</td>
      <td>0.424930</td>
      <td>0.443465</td>
      <td>0.445678</td>
      <td>0.632982</td>
      <td>0.633522</td>
      <td>0.424672</td>
      <td>0.425438</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.431771</td>
      <td>0.425934</td>
      <td>0.441448</td>
      <td>0.444091</td>
      <td>0.634050</td>
      <td>0.635056</td>
      <td>0.422752</td>
      <td>0.423947</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.432082</td>
      <td>0.426098</td>
      <td>0.447443</td>
      <td>0.450108</td>
      <td>0.639474</td>
      <td>0.639868</td>
      <td>0.429693</td>
      <td>0.430976</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>114 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 5, 2)
Test shape: (10292, 5, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          504500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 643,043
Trainable params: 643,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 65ms/step - loss: 1.0782 - accuracy: 0.4008 - auc: 0.5781 - f1_score: 0.4071 - val_loss: 1.0627 - val_accuracy: 0.4223 - val_auc: 0.6071 - val_f1_score: 0.3904
Epoch 2/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0581 - accuracy: 0.4268 - auc: 0.6132 - f1_score: 0.4054 - val_loss: 1.0551 - val_accuracy: 0.4313 - val_auc: 0.6189 - val_f1_score: 0.4097
Epoch 3/50
188/188 [==============================] - 12s 61ms/step - loss: 1.0516 - accuracy: 0.4316 - auc: 0.6216 - f1_score: 0.4146 - val_loss: 1.0518 - val_accuracy: 0.4343 - val_auc: 0.6244 - val_f1_score: 0.4208
Epoch 4/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0473 - accuracy: 0.4389 - auc: 0.6281 - f1_score: 0.4237 - val_loss: 1.0521 - val_accuracy: 0.4324 - val_auc: 0.6248 - val_f1_score: 0.4152
Epoch 5/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0464 - accuracy: 0.4396 - auc: 0.6296 - f1_score: 0.4268 - val_loss: 1.0511 - val_accuracy: 0.4351 - val_auc: 0.6256 - val_f1_score: 0.4242
Epoch 6/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0444 - accuracy: 0.4427 - auc: 0.6322 - f1_score: 0.4290 - val_loss: 1.0513 - val_accuracy: 0.4343 - val_auc: 0.6253 - val_f1_score: 0.4197
Epoch 7/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0441 - accuracy: 0.4425 - auc: 0.6326 - f1_score: 0.4315 - val_loss: 1.0506 - val_accuracy: 0.4330 - val_auc: 0.6261 - val_f1_score: 0.4260
Epoch 8/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0433 - accuracy: 0.4444 - auc: 0.6341 - f1_score: 0.4353 - val_loss: 1.0514 - val_accuracy: 0.4345 - val_auc: 0.6255 - val_f1_score: 0.4254
Epoch 9/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0424 - accuracy: 0.4459 - auc: 0.6361 - f1_score: 0.4356 - val_loss: 1.0512 - val_accuracy: 0.4350 - val_auc: 0.6259 - val_f1_score: 0.4260
Epoch 10/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0420 - accuracy: 0.4479 - auc: 0.6365 - f1_score: 0.4365 - val_loss: 1.0507 - val_accuracy: 0.4313 - val_auc: 0.6263 - val_f1_score: 0.4242
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>111</th>
      <td>0.430151</td>
      <td>0.424279</td>
      <td>0.440423</td>
      <td>0.442522</td>
      <td>0.629295</td>
      <td>0.630061</td>
      <td>0.423756</td>
      <td>0.424412</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.430834</td>
      <td>0.424930</td>
      <td>0.443465</td>
      <td>0.445678</td>
      <td>0.632982</td>
      <td>0.633522</td>
      <td>0.424672</td>
      <td>0.425438</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.431771</td>
      <td>0.425934</td>
      <td>0.441448</td>
      <td>0.444091</td>
      <td>0.634050</td>
      <td>0.635056</td>
      <td>0.422752</td>
      <td>0.423947</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.432082</td>
      <td>0.426098</td>
      <td>0.447443</td>
      <td>0.450108</td>
      <td>0.639474</td>
      <td>0.639868</td>
      <td>0.429693</td>
      <td>0.430976</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.433563</td>
      <td>0.427400</td>
      <td>0.452163</td>
      <td>0.453578</td>
      <td>0.644873</td>
      <td>0.644318</td>
      <td>0.444437</td>
      <td>0.444841</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>115 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 6, 2)
Test shape: (10292, 6, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1208)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          604500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 743,043
Trainable params: 743,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 18s 79ms/step - loss: 1.0834 - accuracy: 0.3868 - auc: 0.5675 - f1_score: 0.4186 - val_loss: 1.0673 - val_accuracy: 0.4138 - val_auc: 0.5996 - val_f1_score: 0.3816
Epoch 2/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0623 - accuracy: 0.4204 - auc: 0.6073 - f1_score: 0.4062 - val_loss: 1.0582 - val_accuracy: 0.4237 - val_auc: 0.6141 - val_f1_score: 0.4052
Epoch 3/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0551 - accuracy: 0.4300 - auc: 0.6183 - f1_score: 0.4185 - val_loss: 1.0533 - val_accuracy: 0.4295 - val_auc: 0.6210 - val_f1_score: 0.4129
Epoch 4/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0506 - accuracy: 0.4352 - auc: 0.6244 - f1_score: 0.4198 - val_loss: 1.0512 - val_accuracy: 0.4365 - val_auc: 0.6243 - val_f1_score: 0.4230
Epoch 5/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0482 - accuracy: 0.4368 - auc: 0.6276 - f1_score: 0.4259 - val_loss: 1.0509 - val_accuracy: 0.4347 - val_auc: 0.6247 - val_f1_score: 0.4197
Epoch 6/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0456 - accuracy: 0.4418 - auc: 0.6313 - f1_score: 0.4310 - val_loss: 1.0511 - val_accuracy: 0.4307 - val_auc: 0.6238 - val_f1_score: 0.4184
Epoch 7/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0439 - accuracy: 0.4429 - auc: 0.6339 - f1_score: 0.4326 - val_loss: 1.0511 - val_accuracy: 0.4349 - val_auc: 0.6244 - val_f1_score: 0.4237
Epoch 8/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0428 - accuracy: 0.4459 - auc: 0.6351 - f1_score: 0.4370 - val_loss: 1.0519 - val_accuracy: 0.4373 - val_auc: 0.6251 - val_f1_score: 0.4260
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>112</th>
      <td>0.430834</td>
      <td>0.424930</td>
      <td>0.443465</td>
      <td>0.445678</td>
      <td>0.632982</td>
      <td>0.633522</td>
      <td>0.424672</td>
      <td>0.425438</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.431771</td>
      <td>0.425934</td>
      <td>0.441448</td>
      <td>0.444091</td>
      <td>0.634050</td>
      <td>0.635056</td>
      <td>0.422752</td>
      <td>0.423947</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.432082</td>
      <td>0.426098</td>
      <td>0.447443</td>
      <td>0.450108</td>
      <td>0.639474</td>
      <td>0.639868</td>
      <td>0.429693</td>
      <td>0.430976</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.433563</td>
      <td>0.427400</td>
      <td>0.452163</td>
      <td>0.453578</td>
      <td>0.644873</td>
      <td>0.644318</td>
      <td>0.444437</td>
      <td>0.444841</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.435514</td>
      <td>0.429363</td>
      <td>0.454518</td>
      <td>0.455607</td>
      <td>0.645517</td>
      <td>0.644857</td>
      <td>0.442536</td>
      <td>0.442483</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>116 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 7, 2)
Test shape: (10292, 7, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1408)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          704500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 843,043
Trainable params: 843,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 78ms/step - loss: 1.0771 - accuracy: 0.3953 - auc: 0.5798 - f1_score: 0.4230 - val_loss: 1.0652 - val_accuracy: 0.4163 - val_auc: 0.6034 - val_f1_score: 0.3848
Epoch 2/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0601 - accuracy: 0.4234 - auc: 0.6101 - f1_score: 0.4071 - val_loss: 1.0572 - val_accuracy: 0.4257 - val_auc: 0.6165 - val_f1_score: 0.3964
Epoch 3/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0517 - accuracy: 0.4322 - auc: 0.6227 - f1_score: 0.4191 - val_loss: 1.0544 - val_accuracy: 0.4269 - val_auc: 0.6197 - val_f1_score: 0.4146
Epoch 4/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0489 - accuracy: 0.4394 - auc: 0.6269 - f1_score: 0.4275 - val_loss: 1.0522 - val_accuracy: 0.4330 - val_auc: 0.6229 - val_f1_score: 0.4158
Epoch 5/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0467 - accuracy: 0.4395 - auc: 0.6296 - f1_score: 0.4288 - val_loss: 1.0519 - val_accuracy: 0.4379 - val_auc: 0.6247 - val_f1_score: 0.4178
Epoch 6/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0437 - accuracy: 0.4425 - auc: 0.6334 - f1_score: 0.4347 - val_loss: 1.0522 - val_accuracy: 0.4393 - val_auc: 0.6249 - val_f1_score: 0.4288
Epoch 7/50
188/188 [==============================] - 15s 78ms/step - loss: 1.0427 - accuracy: 0.4455 - auc: 0.6349 - f1_score: 0.4366 - val_loss: 1.0511 - val_accuracy: 0.4324 - val_auc: 0.6254 - val_f1_score: 0.4282
Epoch 8/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0408 - accuracy: 0.4479 - auc: 0.6381 - f1_score: 0.4415 - val_loss: 1.0498 - val_accuracy: 0.4402 - val_auc: 0.6291 - val_f1_score: 0.4339
Epoch 9/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0399 - accuracy: 0.4506 - auc: 0.6391 - f1_score: 0.4442 - val_loss: 1.0507 - val_accuracy: 0.4362 - val_auc: 0.6265 - val_f1_score: 0.4251
Epoch 10/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0384 - accuracy: 0.4475 - auc: 0.6405 - f1_score: 0.4407 - val_loss: 1.0513 - val_accuracy: 0.4388 - val_auc: 0.6254 - val_f1_score: 0.4252
Epoch 11/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0381 - accuracy: 0.4512 - auc: 0.6413 - f1_score: 0.4444 - val_loss: 1.0505 - val_accuracy: 0.4390 - val_auc: 0.6275 - val_f1_score: 0.4326
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>113</th>
      <td>0.431771</td>
      <td>0.425934</td>
      <td>0.441448</td>
      <td>0.444091</td>
      <td>0.634050</td>
      <td>0.635056</td>
      <td>0.422752</td>
      <td>0.423947</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.432082</td>
      <td>0.426098</td>
      <td>0.447443</td>
      <td>0.450108</td>
      <td>0.639474</td>
      <td>0.639868</td>
      <td>0.429693</td>
      <td>0.430976</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.433563</td>
      <td>0.427400</td>
      <td>0.452163</td>
      <td>0.453578</td>
      <td>0.644873</td>
      <td>0.644318</td>
      <td>0.444437</td>
      <td>0.444841</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.435514</td>
      <td>0.429363</td>
      <td>0.454518</td>
      <td>0.455607</td>
      <td>0.645517</td>
      <td>0.644857</td>
      <td>0.442536</td>
      <td>0.442483</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.437006</td>
      <td>0.430669</td>
      <td>0.463009</td>
      <td>0.463016</td>
      <td>0.653833</td>
      <td>0.652196</td>
      <td>0.456752</td>
      <td>0.455808</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>117 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8, 2)
Test shape: (10292, 8, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1608)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          804500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 943,043
Trainable params: 943,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 19s 87ms/step - loss: 1.0758 - accuracy: 0.3980 - auc: 0.5822 - f1_score: 0.4265 - val_loss: 1.0640 - val_accuracy: 0.4179 - val_auc: 0.6041 - val_f1_score: 0.3766
Epoch 2/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0592 - accuracy: 0.4251 - auc: 0.6115 - f1_score: 0.3976 - val_loss: 1.0583 - val_accuracy: 0.4248 - val_auc: 0.6142 - val_f1_score: 0.3777
Epoch 3/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0524 - accuracy: 0.4306 - auc: 0.6208 - f1_score: 0.4072 - val_loss: 1.0555 - val_accuracy: 0.4300 - val_auc: 0.6186 - val_f1_score: 0.3921
Epoch 4/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0482 - accuracy: 0.4383 - auc: 0.6272 - f1_score: 0.4194 - val_loss: 1.0537 - val_accuracy: 0.4335 - val_auc: 0.6223 - val_f1_score: 0.4105
Epoch 5/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0453 - accuracy: 0.4411 - auc: 0.6309 - f1_score: 0.4278 - val_loss: 1.0506 - val_accuracy: 0.4317 - val_auc: 0.6262 - val_f1_score: 0.4163
Epoch 6/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0436 - accuracy: 0.4434 - auc: 0.6343 - f1_score: 0.4319 - val_loss: 1.0517 - val_accuracy: 0.4331 - val_auc: 0.6237 - val_f1_score: 0.4178
Epoch 7/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0417 - accuracy: 0.4473 - auc: 0.6365 - f1_score: 0.4370 - val_loss: 1.0511 - val_accuracy: 0.4353 - val_auc: 0.6250 - val_f1_score: 0.4252
Epoch 8/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0400 - accuracy: 0.4491 - auc: 0.6386 - f1_score: 0.4386 - val_loss: 1.0527 - val_accuracy: 0.4343 - val_auc: 0.6252 - val_f1_score: 0.4212
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>114</th>
      <td>0.432082</td>
      <td>0.426098</td>
      <td>0.447443</td>
      <td>0.450108</td>
      <td>0.639474</td>
      <td>0.639868</td>
      <td>0.429693</td>
      <td>0.430976</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.433563</td>
      <td>0.427400</td>
      <td>0.452163</td>
      <td>0.453578</td>
      <td>0.644873</td>
      <td>0.644318</td>
      <td>0.444437</td>
      <td>0.444841</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.435514</td>
      <td>0.429363</td>
      <td>0.454518</td>
      <td>0.455607</td>
      <td>0.645517</td>
      <td>0.644857</td>
      <td>0.442536</td>
      <td>0.442483</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.437006</td>
      <td>0.430669</td>
      <td>0.463009</td>
      <td>0.463016</td>
      <td>0.653833</td>
      <td>0.652196</td>
      <td>0.456752</td>
      <td>0.455808</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.437309</td>
      <td>0.430990</td>
      <td>0.460611</td>
      <td>0.461252</td>
      <td>0.651774</td>
      <td>0.650485</td>
      <td>0.447959</td>
      <td>0.447464</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>118 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 9, 2)
Test shape: (10292, 9, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 2)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,043,043
Trainable params: 1,043,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 20s 93ms/step - loss: 1.0725 - accuracy: 0.4060 - auc: 0.5878 - f1_score: 0.4172 - val_loss: 1.0636 - val_accuracy: 0.4173 - val_auc: 0.6052 - val_f1_score: 0.4026
Epoch 2/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0558 - accuracy: 0.4284 - auc: 0.6170 - f1_score: 0.4134 - val_loss: 1.0571 - val_accuracy: 0.4302 - val_auc: 0.6156 - val_f1_score: 0.4233
Epoch 3/50
188/188 [==============================] - 18s 95ms/step - loss: 1.0494 - accuracy: 0.4371 - auc: 0.6255 - f1_score: 0.4222 - val_loss: 1.0553 - val_accuracy: 0.4318 - val_auc: 0.6185 - val_f1_score: 0.4241
Epoch 4/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0460 - accuracy: 0.4395 - auc: 0.6304 - f1_score: 0.4260 - val_loss: 1.0535 - val_accuracy: 0.4320 - val_auc: 0.6216 - val_f1_score: 0.4293
Epoch 5/50
188/188 [==============================] - 18s 93ms/step - loss: 1.0442 - accuracy: 0.4438 - auc: 0.6330 - f1_score: 0.4353 - val_loss: 1.0536 - val_accuracy: 0.4311 - val_auc: 0.6216 - val_f1_score: 0.4207
Epoch 6/50
188/188 [==============================] - 18s 93ms/step - loss: 1.0423 - accuracy: 0.4454 - auc: 0.6350 - f1_score: 0.4354 - val_loss: 1.0528 - val_accuracy: 0.4343 - val_auc: 0.6229 - val_f1_score: 0.4264
Epoch 7/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0402 - accuracy: 0.4482 - auc: 0.6381 - f1_score: 0.4396 - val_loss: 1.0539 - val_accuracy: 0.4368 - val_auc: 0.6217 - val_f1_score: 0.4309
Epoch 8/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0393 - accuracy: 0.4496 - auc: 0.6397 - f1_score: 0.4404 - val_loss: 1.0531 - val_accuracy: 0.4326 - val_auc: 0.6217 - val_f1_score: 0.4244
Epoch 9/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0376 - accuracy: 0.4519 - auc: 0.6418 - f1_score: 0.4427 - val_loss: 1.0530 - val_accuracy: 0.4288 - val_auc: 0.6234 - val_f1_score: 0.4172
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>115</th>
      <td>0.433563</td>
      <td>0.427400</td>
      <td>0.452163</td>
      <td>0.453578</td>
      <td>0.644873</td>
      <td>0.644318</td>
      <td>0.444437</td>
      <td>0.444841</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.435514</td>
      <td>0.429363</td>
      <td>0.454518</td>
      <td>0.455607</td>
      <td>0.645517</td>
      <td>0.644857</td>
      <td>0.442536</td>
      <td>0.442483</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.437006</td>
      <td>0.430669</td>
      <td>0.463009</td>
      <td>0.463016</td>
      <td>0.653833</td>
      <td>0.652196</td>
      <td>0.456752</td>
      <td>0.455808</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.437309</td>
      <td>0.430990</td>
      <td>0.460611</td>
      <td>0.461252</td>
      <td>0.651774</td>
      <td>0.650485</td>
      <td>0.447959</td>
      <td>0.447464</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.439139</td>
      <td>0.432810</td>
      <td>0.461985</td>
      <td>0.462124</td>
      <td>0.653254</td>
      <td>0.651975</td>
      <td>0.451026</td>
      <td>0.449984</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>119 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 10, 2)
Test shape: (10292, 10, 2)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 2)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10200       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10200       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,143,043
Trainable params: 1,143,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 22s 104ms/step - loss: 1.0770 - accuracy: 0.4006 - auc: 0.5808 - f1_score: 0.4165 - val_loss: 1.0679 - val_accuracy: 0.4149 - val_auc: 0.5986 - val_f1_score: 0.3812
Epoch 2/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0597 - accuracy: 0.4240 - auc: 0.6104 - f1_score: 0.4003 - val_loss: 1.0624 - val_accuracy: 0.4201 - val_auc: 0.6076 - val_f1_score: 0.3827
Epoch 3/50
188/188 [==============================] - 18s 98ms/step - loss: 1.0541 - accuracy: 0.4299 - auc: 0.6186 - f1_score: 0.4042 - val_loss: 1.0557 - val_accuracy: 0.4300 - val_auc: 0.6182 - val_f1_score: 0.4071
Epoch 4/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0487 - accuracy: 0.4365 - auc: 0.6256 - f1_score: 0.4184 - val_loss: 1.0548 - val_accuracy: 0.4314 - val_auc: 0.6193 - val_f1_score: 0.4132
Epoch 5/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0458 - accuracy: 0.4406 - auc: 0.6300 - f1_score: 0.4263 - val_loss: 1.0549 - val_accuracy: 0.4272 - val_auc: 0.6189 - val_f1_score: 0.4070
Epoch 6/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0439 - accuracy: 0.4439 - auc: 0.6328 - f1_score: 0.4313 - val_loss: 1.0536 - val_accuracy: 0.4289 - val_auc: 0.6212 - val_f1_score: 0.4162
Epoch 7/50
188/188 [==============================] - 18s 97ms/step - loss: 1.0416 - accuracy: 0.4462 - auc: 0.6364 - f1_score: 0.4347 - val_loss: 1.0538 - val_accuracy: 0.4336 - val_auc: 0.6220 - val_f1_score: 0.4283
Epoch 8/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0405 - accuracy: 0.4466 - auc: 0.6370 - f1_score: 0.4359 - val_loss: 1.0530 - val_accuracy: 0.4351 - val_auc: 0.6228 - val_f1_score: 0.4250
Epoch 9/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0395 - accuracy: 0.4494 - auc: 0.6388 - f1_score: 0.4394 - val_loss: 1.0536 - val_accuracy: 0.4320 - val_auc: 0.6213 - val_f1_score: 0.4244
Epoch 10/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0375 - accuracy: 0.4519 - auc: 0.6413 - f1_score: 0.4438 - val_loss: 1.0536 - val_accuracy: 0.4329 - val_auc: 0.6221 - val_f1_score: 0.4248
Epoch 11/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0361 - accuracy: 0.4527 - auc: 0.6430 - f1_score: 0.4442 - val_loss: 1.0535 - val_accuracy: 0.4329 - val_auc: 0.6211 - val_f1_score: 0.4276
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>116</th>
      <td>0.435514</td>
      <td>0.429363</td>
      <td>0.454518</td>
      <td>0.455607</td>
      <td>0.645517</td>
      <td>0.644857</td>
      <td>0.442536</td>
      <td>0.442483</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.437006</td>
      <td>0.430669</td>
      <td>0.463009</td>
      <td>0.463016</td>
      <td>0.653833</td>
      <td>0.652196</td>
      <td>0.456752</td>
      <td>0.455808</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.437309</td>
      <td>0.430990</td>
      <td>0.460611</td>
      <td>0.461252</td>
      <td>0.651774</td>
      <td>0.650485</td>
      <td>0.447959</td>
      <td>0.447464</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.439139</td>
      <td>0.432810</td>
      <td>0.461985</td>
      <td>0.462124</td>
      <td>0.653254</td>
      <td>0.651975</td>
      <td>0.451026</td>
      <td>0.449984</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.436536</td>
      <td>0.430086</td>
      <td>0.464448</td>
      <td>0.464476</td>
      <td>0.654278</td>
      <td>0.652474</td>
      <td>0.459117</td>
      <td>0.458334</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
<p>120 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 202)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          101500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 305,243
Trainable params: 305,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 8s 33ms/step - loss: 1.0912 - accuracy: 0.3781 - auc: 0.5509 - f1_score: 0.4082 - val_loss: 1.0815 - val_accuracy: 0.4086 - val_auc: 0.5830 - val_f1_score: 0.3584
Epoch 2/50
188/188 [==============================] - 6s 29ms/step - loss: 1.0754 - accuracy: 0.4100 - auc: 0.5887 - f1_score: 0.3681 - val_loss: 1.0733 - val_accuracy: 0.4090 - val_auc: 0.5937 - val_f1_score: 0.3741
Epoch 3/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0724 - accuracy: 0.4130 - auc: 0.5943 - f1_score: 0.3879 - val_loss: 1.0716 - val_accuracy: 0.4103 - val_auc: 0.5957 - val_f1_score: 0.3885
Epoch 4/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0710 - accuracy: 0.4154 - auc: 0.5968 - f1_score: 0.3998 - val_loss: 1.0709 - val_accuracy: 0.4145 - val_auc: 0.5967 - val_f1_score: 0.3964
Epoch 5/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0698 - accuracy: 0.4159 - auc: 0.5984 - f1_score: 0.4022 - val_loss: 1.0705 - val_accuracy: 0.4138 - val_auc: 0.5976 - val_f1_score: 0.4032
Epoch 6/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0692 - accuracy: 0.4167 - auc: 0.5992 - f1_score: 0.4063 - val_loss: 1.0699 - val_accuracy: 0.4163 - val_auc: 0.5989 - val_f1_score: 0.4095
Epoch 7/50
188/188 [==============================] - 6s 32ms/step - loss: 1.0692 - accuracy: 0.4164 - auc: 0.6000 - f1_score: 0.4053 - val_loss: 1.0702 - val_accuracy: 0.4121 - val_auc: 0.5979 - val_f1_score: 0.4063
Epoch 8/50
188/188 [==============================] - 6s 31ms/step - loss: 1.0686 - accuracy: 0.4170 - auc: 0.6002 - f1_score: 0.4062 - val_loss: 1.0701 - val_accuracy: 0.4092 - val_auc: 0.5977 - val_f1_score: 0.4043
Epoch 9/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0692 - accuracy: 0.4162 - auc: 0.6000 - f1_score: 0.4090 - val_loss: 1.0706 - val_accuracy: 0.4088 - val_auc: 0.5965 - val_f1_score: 0.4008
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>117</th>
      <td>0.437006</td>
      <td>0.430669</td>
      <td>0.463009</td>
      <td>0.463016</td>
      <td>0.653833</td>
      <td>0.652196</td>
      <td>0.456752</td>
      <td>0.455808</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.437309</td>
      <td>0.430990</td>
      <td>0.460611</td>
      <td>0.461252</td>
      <td>0.651774</td>
      <td>0.650485</td>
      <td>0.447959</td>
      <td>0.447464</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.439139</td>
      <td>0.432810</td>
      <td>0.461985</td>
      <td>0.462124</td>
      <td>0.653254</td>
      <td>0.651975</td>
      <td>0.451026</td>
      <td>0.449984</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.436536</td>
      <td>0.430086</td>
      <td>0.464448</td>
      <td>0.464476</td>
      <td>0.654278</td>
      <td>0.652474</td>
      <td>0.459117</td>
      <td>0.458334</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>121</th>
      <td>0.419249</td>
      <td>0.412813</td>
      <td>0.420704</td>
      <td>0.422558</td>
      <td>0.606606</td>
      <td>0.606748</td>
      <td>0.411648</td>
      <td>0.412549</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>121 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 402)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          201500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 405,243
Trainable params: 405,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 58ms/step - loss: 1.0841 - accuracy: 0.3972 - auc: 0.5743 - f1_score: 0.4008 - val_loss: 1.0646 - val_accuracy: 0.4248 - val_auc: 0.6104 - val_f1_score: 0.4088
Epoch 2/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0630 - accuracy: 0.4245 - auc: 0.6093 - f1_score: 0.4154 - val_loss: 1.0595 - val_accuracy: 0.4260 - val_auc: 0.6142 - val_f1_score: 0.4192
Epoch 3/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0604 - accuracy: 0.4262 - auc: 0.6123 - f1_score: 0.4182 - val_loss: 1.0592 - val_accuracy: 0.4230 - val_auc: 0.6140 - val_f1_score: 0.4100
Epoch 4/50
188/188 [==============================] - 11s 56ms/step - loss: 1.0585 - accuracy: 0.4308 - auc: 0.6161 - f1_score: 0.4222 - val_loss: 1.0587 - val_accuracy: 0.4245 - val_auc: 0.6152 - val_f1_score: 0.4184
Epoch 5/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0570 - accuracy: 0.4310 - auc: 0.6177 - f1_score: 0.4236 - val_loss: 1.0592 - val_accuracy: 0.4240 - val_auc: 0.6139 - val_f1_score: 0.4146
Epoch 6/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0572 - accuracy: 0.4329 - auc: 0.6178 - f1_score: 0.4273 - val_loss: 1.0587 - val_accuracy: 0.4218 - val_auc: 0.6150 - val_f1_score: 0.4114
Epoch 7/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0575 - accuracy: 0.4321 - auc: 0.6174 - f1_score: 0.4270 - val_loss: 1.0589 - val_accuracy: 0.4189 - val_auc: 0.6141 - val_f1_score: 0.4130
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>118</th>
      <td>0.437309</td>
      <td>0.430990</td>
      <td>0.460611</td>
      <td>0.461252</td>
      <td>0.651774</td>
      <td>0.650485</td>
      <td>0.447959</td>
      <td>0.447464</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.439139</td>
      <td>0.432810</td>
      <td>0.461985</td>
      <td>0.462124</td>
      <td>0.653254</td>
      <td>0.651975</td>
      <td>0.451026</td>
      <td>0.449984</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.436536</td>
      <td>0.430086</td>
      <td>0.464448</td>
      <td>0.464476</td>
      <td>0.654278</td>
      <td>0.652474</td>
      <td>0.459117</td>
      <td>0.458334</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>121</th>
      <td>0.419249</td>
      <td>0.412813</td>
      <td>0.420704</td>
      <td>0.422558</td>
      <td>0.606606</td>
      <td>0.606748</td>
      <td>0.411648</td>
      <td>0.412549</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.425894</td>
      <td>0.419715</td>
      <td>0.437546</td>
      <td>0.438563</td>
      <td>0.626280</td>
      <td>0.626258</td>
      <td>0.431737</td>
      <td>0.431796</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>122 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 602)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          301500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 505,243
Trainable params: 505,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 75ms/step - loss: 1.0744 - accuracy: 0.4112 - auc: 0.5900 - f1_score: 0.4093 - val_loss: 1.0577 - val_accuracy: 0.4217 - val_auc: 0.6149 - val_f1_score: 0.3597
Epoch 2/50
188/188 [==============================] - 13s 72ms/step - loss: 1.0564 - accuracy: 0.4289 - auc: 0.6169 - f1_score: 0.3769 - val_loss: 1.0539 - val_accuracy: 0.4260 - val_auc: 0.6218 - val_f1_score: 0.3784
Epoch 3/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0527 - accuracy: 0.4330 - auc: 0.6229 - f1_score: 0.3946 - val_loss: 1.0523 - val_accuracy: 0.4265 - val_auc: 0.6239 - val_f1_score: 0.4005
Epoch 4/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0518 - accuracy: 0.4362 - auc: 0.6243 - f1_score: 0.4083 - val_loss: 1.0513 - val_accuracy: 0.4265 - val_auc: 0.6245 - val_f1_score: 0.4059
Epoch 5/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0507 - accuracy: 0.4381 - auc: 0.6259 - f1_score: 0.4170 - val_loss: 1.0508 - val_accuracy: 0.4275 - val_auc: 0.6249 - val_f1_score: 0.4071
Epoch 6/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0504 - accuracy: 0.4387 - auc: 0.6264 - f1_score: 0.4209 - val_loss: 1.0511 - val_accuracy: 0.4307 - val_auc: 0.6248 - val_f1_score: 0.4137
Epoch 7/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0504 - accuracy: 0.4394 - auc: 0.6266 - f1_score: 0.4243 - val_loss: 1.0504 - val_accuracy: 0.4317 - val_auc: 0.6264 - val_f1_score: 0.4208
Epoch 8/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0499 - accuracy: 0.4397 - auc: 0.6270 - f1_score: 0.4277 - val_loss: 1.0504 - val_accuracy: 0.4288 - val_auc: 0.6263 - val_f1_score: 0.4163
Epoch 9/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0489 - accuracy: 0.4403 - auc: 0.6287 - f1_score: 0.4277 - val_loss: 1.0506 - val_accuracy: 0.4313 - val_auc: 0.6263 - val_f1_score: 0.4230
Epoch 10/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0482 - accuracy: 0.4406 - auc: 0.6293 - f1_score: 0.4291 - val_loss: 1.0503 - val_accuracy: 0.4331 - val_auc: 0.6269 - val_f1_score: 0.4270
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>119</th>
      <td>0.439139</td>
      <td>0.432810</td>
      <td>0.461985</td>
      <td>0.462124</td>
      <td>0.653254</td>
      <td>0.651975</td>
      <td>0.451026</td>
      <td>0.449984</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.436536</td>
      <td>0.430086</td>
      <td>0.464448</td>
      <td>0.464476</td>
      <td>0.654278</td>
      <td>0.652474</td>
      <td>0.459117</td>
      <td>0.458334</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>121</th>
      <td>0.419249</td>
      <td>0.412813</td>
      <td>0.420704</td>
      <td>0.422558</td>
      <td>0.606606</td>
      <td>0.606748</td>
      <td>0.411648</td>
      <td>0.412549</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.425894</td>
      <td>0.419715</td>
      <td>0.437546</td>
      <td>0.438563</td>
      <td>0.626280</td>
      <td>0.626258</td>
      <td>0.431737</td>
      <td>0.431796</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.428498</td>
      <td>0.422400</td>
      <td>0.447312</td>
      <td>0.448687</td>
      <td>0.637695</td>
      <td>0.637619</td>
      <td>0.441210</td>
      <td>0.441601</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>123 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 802)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          401500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 605,243
Trainable params: 605,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 18s 85ms/step - loss: 1.0709 - accuracy: 0.4149 - auc: 0.5938 - f1_score: 0.4320 - val_loss: 1.0572 - val_accuracy: 0.4368 - val_auc: 0.6164 - val_f1_score: 0.4104
Epoch 2/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0552 - accuracy: 0.4346 - auc: 0.6182 - f1_score: 0.4009 - val_loss: 1.0555 - val_accuracy: 0.4315 - val_auc: 0.6175 - val_f1_score: 0.3871
Epoch 3/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0525 - accuracy: 0.4347 - auc: 0.6212 - f1_score: 0.3836 - val_loss: 1.0538 - val_accuracy: 0.4290 - val_auc: 0.6193 - val_f1_score: 0.3771
Epoch 4/50
188/188 [==============================] - 15s 80ms/step - loss: 1.0505 - accuracy: 0.4368 - auc: 0.6251 - f1_score: 0.3837 - val_loss: 1.0529 - val_accuracy: 0.4287 - val_auc: 0.6213 - val_f1_score: 0.3682
Epoch 5/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0496 - accuracy: 0.4381 - auc: 0.6274 - f1_score: 0.3874 - val_loss: 1.0523 - val_accuracy: 0.4284 - val_auc: 0.6230 - val_f1_score: 0.3854
Epoch 6/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0479 - accuracy: 0.4384 - auc: 0.6291 - f1_score: 0.3987 - val_loss: 1.0508 - val_accuracy: 0.4313 - val_auc: 0.6252 - val_f1_score: 0.3970
Epoch 7/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0473 - accuracy: 0.4399 - auc: 0.6299 - f1_score: 0.4090 - val_loss: 1.0505 - val_accuracy: 0.4353 - val_auc: 0.6252 - val_f1_score: 0.4126
Epoch 8/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0458 - accuracy: 0.4431 - auc: 0.6328 - f1_score: 0.4219 - val_loss: 1.0504 - val_accuracy: 0.4354 - val_auc: 0.6256 - val_f1_score: 0.4121
Epoch 9/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0462 - accuracy: 0.4423 - auc: 0.6324 - f1_score: 0.4250 - val_loss: 1.0497 - val_accuracy: 0.4404 - val_auc: 0.6273 - val_f1_score: 0.4275
Epoch 10/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0454 - accuracy: 0.4447 - auc: 0.6332 - f1_score: 0.4311 - val_loss: 1.0495 - val_accuracy: 0.4368 - val_auc: 0.6271 - val_f1_score: 0.4259
Epoch 11/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0446 - accuracy: 0.4445 - auc: 0.6345 - f1_score: 0.4349 - val_loss: 1.0495 - val_accuracy: 0.4394 - val_auc: 0.6266 - val_f1_score: 0.4324
Epoch 12/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0433 - accuracy: 0.4462 - auc: 0.6353 - f1_score: 0.4374 - val_loss: 1.0489 - val_accuracy: 0.4382 - val_auc: 0.6268 - val_f1_score: 0.4324
Epoch 13/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0436 - accuracy: 0.4472 - auc: 0.6359 - f1_score: 0.4413 - val_loss: 1.0493 - val_accuracy: 0.4417 - val_auc: 0.6271 - val_f1_score: 0.4338
Epoch 14/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0426 - accuracy: 0.4482 - auc: 0.6366 - f1_score: 0.4418 - val_loss: 1.0490 - val_accuracy: 0.4365 - val_auc: 0.6265 - val_f1_score: 0.4295
Epoch 15/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0418 - accuracy: 0.4501 - auc: 0.6380 - f1_score: 0.4447 - val_loss: 1.0494 - val_accuracy: 0.4357 - val_auc: 0.6260 - val_f1_score: 0.4338
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>120</th>
      <td>0.436536</td>
      <td>0.430086</td>
      <td>0.464448</td>
      <td>0.464476</td>
      <td>0.654278</td>
      <td>0.652474</td>
      <td>0.459117</td>
      <td>0.458334</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>2</td>
    </tr>
    <tr>
      <th>121</th>
      <td>0.419249</td>
      <td>0.412813</td>
      <td>0.420704</td>
      <td>0.422558</td>
      <td>0.606606</td>
      <td>0.606748</td>
      <td>0.411648</td>
      <td>0.412549</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.425894</td>
      <td>0.419715</td>
      <td>0.437546</td>
      <td>0.438563</td>
      <td>0.626280</td>
      <td>0.626258</td>
      <td>0.431737</td>
      <td>0.431796</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.428498</td>
      <td>0.422400</td>
      <td>0.447312</td>
      <td>0.448687</td>
      <td>0.637695</td>
      <td>0.637619</td>
      <td>0.441210</td>
      <td>0.441601</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>124</th>
      <td>0.433625</td>
      <td>0.427496</td>
      <td>0.458289</td>
      <td>0.458802</td>
      <td>0.647836</td>
      <td>0.647237</td>
      <td>0.455858</td>
      <td>0.455612</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>124 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          501500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 705,243
Trainable params: 705,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 22s 104ms/step - loss: 1.0745 - accuracy: 0.3998 - auc: 0.5895 - f1_score: 0.4321 - val_loss: 1.0531 - val_accuracy: 0.4313 - val_auc: 0.6227 - val_f1_score: 0.3764
Epoch 2/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0507 - accuracy: 0.4339 - auc: 0.6251 - f1_score: 0.4092 - val_loss: 1.0496 - val_accuracy: 0.4345 - val_auc: 0.6255 - val_f1_score: 0.4054
Epoch 3/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0478 - accuracy: 0.4415 - auc: 0.6297 - f1_score: 0.4305 - val_loss: 1.0490 - val_accuracy: 0.4356 - val_auc: 0.6259 - val_f1_score: 0.4191
Epoch 4/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0462 - accuracy: 0.4456 - auc: 0.6316 - f1_score: 0.4371 - val_loss: 1.0472 - val_accuracy: 0.4349 - val_auc: 0.6287 - val_f1_score: 0.4209
Epoch 5/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0441 - accuracy: 0.4465 - auc: 0.6345 - f1_score: 0.4391 - val_loss: 1.0472 - val_accuracy: 0.4417 - val_auc: 0.6293 - val_f1_score: 0.4398
Epoch 6/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0435 - accuracy: 0.4457 - auc: 0.6347 - f1_score: 0.4407 - val_loss: 1.0464 - val_accuracy: 0.4365 - val_auc: 0.6298 - val_f1_score: 0.4319
Epoch 7/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0422 - accuracy: 0.4473 - auc: 0.6369 - f1_score: 0.4435 - val_loss: 1.0465 - val_accuracy: 0.4379 - val_auc: 0.6298 - val_f1_score: 0.4326
Epoch 8/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0419 - accuracy: 0.4483 - auc: 0.6372 - f1_score: 0.4431 - val_loss: 1.0470 - val_accuracy: 0.4372 - val_auc: 0.6294 - val_f1_score: 0.4328
Epoch 9/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0399 - accuracy: 0.4513 - auc: 0.6401 - f1_score: 0.4479 - val_loss: 1.0483 - val_accuracy: 0.4335 - val_auc: 0.6277 - val_f1_score: 0.4239
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>121</th>
      <td>0.419249</td>
      <td>0.412813</td>
      <td>0.420704</td>
      <td>0.422558</td>
      <td>0.606606</td>
      <td>0.606748</td>
      <td>0.411648</td>
      <td>0.412549</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.425894</td>
      <td>0.419715</td>
      <td>0.437546</td>
      <td>0.438563</td>
      <td>0.626280</td>
      <td>0.626258</td>
      <td>0.431737</td>
      <td>0.431796</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.428498</td>
      <td>0.422400</td>
      <td>0.447312</td>
      <td>0.448687</td>
      <td>0.637695</td>
      <td>0.637619</td>
      <td>0.441210</td>
      <td>0.441601</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>124</th>
      <td>0.433625</td>
      <td>0.427496</td>
      <td>0.458289</td>
      <td>0.458802</td>
      <td>0.647836</td>
      <td>0.647237</td>
      <td>0.455858</td>
      <td>0.455612</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>125</th>
      <td>0.437437</td>
      <td>0.431571</td>
      <td>0.456513</td>
      <td>0.457733</td>
      <td>0.647148</td>
      <td>0.647397</td>
      <td>0.446097</td>
      <td>0.446063</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>125 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1202)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          601500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 805,243
Trainable params: 805,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 26s 123ms/step - loss: 1.0670 - accuracy: 0.4228 - auc: 0.6016 - f1_score: 0.4220 - val_loss: 1.0487 - val_accuracy: 0.4345 - val_auc: 0.6277 - val_f1_score: 0.4163
Epoch 2/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0475 - accuracy: 0.4425 - auc: 0.6296 - f1_score: 0.4236 - val_loss: 1.0470 - val_accuracy: 0.4355 - val_auc: 0.6297 - val_f1_score: 0.4240
Epoch 3/50
188/188 [==============================] - 23s 122ms/step - loss: 1.0448 - accuracy: 0.4439 - auc: 0.6330 - f1_score: 0.4310 - val_loss: 1.0461 - val_accuracy: 0.4396 - val_auc: 0.6311 - val_f1_score: 0.4283
Epoch 4/50
188/188 [==============================] - 22s 119ms/step - loss: 1.0437 - accuracy: 0.4462 - auc: 0.6348 - f1_score: 0.4347 - val_loss: 1.0449 - val_accuracy: 0.4420 - val_auc: 0.6325 - val_f1_score: 0.4327
Epoch 5/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0420 - accuracy: 0.4488 - auc: 0.6372 - f1_score: 0.4384 - val_loss: 1.0454 - val_accuracy: 0.4429 - val_auc: 0.6321 - val_f1_score: 0.4382
Epoch 6/50
188/188 [==============================] - 21s 113ms/step - loss: 1.0409 - accuracy: 0.4487 - auc: 0.6381 - f1_score: 0.4413 - val_loss: 1.0453 - val_accuracy: 0.4418 - val_auc: 0.6319 - val_f1_score: 0.4304
Epoch 7/50
188/188 [==============================] - 22s 115ms/step - loss: 1.0408 - accuracy: 0.4494 - auc: 0.6390 - f1_score: 0.4423 - val_loss: 1.0459 - val_accuracy: 0.4438 - val_auc: 0.6316 - val_f1_score: 0.4363
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>122</th>
      <td>0.425894</td>
      <td>0.419715</td>
      <td>0.437546</td>
      <td>0.438563</td>
      <td>0.626280</td>
      <td>0.626258</td>
      <td>0.431737</td>
      <td>0.431796</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.428498</td>
      <td>0.422400</td>
      <td>0.447312</td>
      <td>0.448687</td>
      <td>0.637695</td>
      <td>0.637619</td>
      <td>0.441210</td>
      <td>0.441601</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>124</th>
      <td>0.433625</td>
      <td>0.427496</td>
      <td>0.458289</td>
      <td>0.458802</td>
      <td>0.647836</td>
      <td>0.647237</td>
      <td>0.455858</td>
      <td>0.455612</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>125</th>
      <td>0.437437</td>
      <td>0.431571</td>
      <td>0.456513</td>
      <td>0.457733</td>
      <td>0.647148</td>
      <td>0.647397</td>
      <td>0.446097</td>
      <td>0.446063</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>126</th>
      <td>0.433858</td>
      <td>0.427815</td>
      <td>0.459586</td>
      <td>0.460654</td>
      <td>0.649579</td>
      <td>0.649312</td>
      <td>0.451371</td>
      <td>0.451369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>126 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1402)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          701500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 905,243
Trainable params: 905,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 30s 142ms/step - loss: 1.0711 - accuracy: 0.4236 - auc: 0.5920 - f1_score: 0.4172 - val_loss: 1.0577 - val_accuracy: 0.4284 - val_auc: 0.6142 - val_f1_score: 0.3458
Epoch 2/50
188/188 [==============================] - 26s 139ms/step - loss: 1.0524 - accuracy: 0.4386 - auc: 0.6229 - f1_score: 0.3724 - val_loss: 1.0528 - val_accuracy: 0.4333 - val_auc: 0.6224 - val_f1_score: 0.3901
Epoch 3/50
188/188 [==============================] - 26s 136ms/step - loss: 1.0481 - accuracy: 0.4413 - auc: 0.6289 - f1_score: 0.3977 - val_loss: 1.0505 - val_accuracy: 0.4376 - val_auc: 0.6251 - val_f1_score: 0.4051
Epoch 4/50
188/188 [==============================] - 26s 137ms/step - loss: 1.0453 - accuracy: 0.4449 - auc: 0.6322 - f1_score: 0.4176 - val_loss: 1.0490 - val_accuracy: 0.4362 - val_auc: 0.6268 - val_f1_score: 0.4127
Epoch 5/50
188/188 [==============================] - 26s 136ms/step - loss: 1.0436 - accuracy: 0.4466 - auc: 0.6344 - f1_score: 0.4245 - val_loss: 1.0479 - val_accuracy: 0.4415 - val_auc: 0.6290 - val_f1_score: 0.4276
Epoch 6/50
188/188 [==============================] - 25s 133ms/step - loss: 1.0415 - accuracy: 0.4496 - auc: 0.6374 - f1_score: 0.4343 - val_loss: 1.0476 - val_accuracy: 0.4420 - val_auc: 0.6299 - val_f1_score: 0.4234
Epoch 7/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0406 - accuracy: 0.4500 - auc: 0.6387 - f1_score: 0.4355 - val_loss: 1.0472 - val_accuracy: 0.4408 - val_auc: 0.6302 - val_f1_score: 0.4287
Epoch 8/50
188/188 [==============================] - 26s 139ms/step - loss: 1.0395 - accuracy: 0.4498 - auc: 0.6395 - f1_score: 0.4384 - val_loss: 1.0467 - val_accuracy: 0.4428 - val_auc: 0.6310 - val_f1_score: 0.4383
Epoch 9/50
188/188 [==============================] - 25s 133ms/step - loss: 1.0384 - accuracy: 0.4531 - auc: 0.6411 - f1_score: 0.4425 - val_loss: 1.0470 - val_accuracy: 0.4418 - val_auc: 0.6300 - val_f1_score: 0.4360
Epoch 10/50
188/188 [==============================] - 26s 137ms/step - loss: 1.0368 - accuracy: 0.4538 - auc: 0.6433 - f1_score: 0.4442 - val_loss: 1.0458 - val_accuracy: 0.4417 - val_auc: 0.6311 - val_f1_score: 0.4345
Epoch 11/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0362 - accuracy: 0.4556 - auc: 0.6442 - f1_score: 0.4470 - val_loss: 1.0463 - val_accuracy: 0.4452 - val_auc: 0.6309 - val_f1_score: 0.4401
Epoch 12/50
188/188 [==============================] - 26s 137ms/step - loss: 1.0346 - accuracy: 0.4580 - auc: 0.6462 - f1_score: 0.4510 - val_loss: 1.0464 - val_accuracy: 0.4430 - val_auc: 0.6309 - val_f1_score: 0.4385
Epoch 13/50
188/188 [==============================] - 25s 134ms/step - loss: 1.0340 - accuracy: 0.4562 - auc: 0.6462 - f1_score: 0.4483 - val_loss: 1.0471 - val_accuracy: 0.4427 - val_auc: 0.6298 - val_f1_score: 0.4382
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>123</th>
      <td>0.428498</td>
      <td>0.422400</td>
      <td>0.447312</td>
      <td>0.448687</td>
      <td>0.637695</td>
      <td>0.637619</td>
      <td>0.441210</td>
      <td>0.441601</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>124</th>
      <td>0.433625</td>
      <td>0.427496</td>
      <td>0.458289</td>
      <td>0.458802</td>
      <td>0.647836</td>
      <td>0.647237</td>
      <td>0.455858</td>
      <td>0.455612</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>125</th>
      <td>0.437437</td>
      <td>0.431571</td>
      <td>0.456513</td>
      <td>0.457733</td>
      <td>0.647148</td>
      <td>0.647397</td>
      <td>0.446097</td>
      <td>0.446063</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>126</th>
      <td>0.433858</td>
      <td>0.427815</td>
      <td>0.459586</td>
      <td>0.460654</td>
      <td>0.649579</td>
      <td>0.649312</td>
      <td>0.451371</td>
      <td>0.451369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>127</th>
      <td>0.436716</td>
      <td>0.430564</td>
      <td>0.466279</td>
      <td>0.466417</td>
      <td>0.655239</td>
      <td>0.654447</td>
      <td>0.460767</td>
      <td>0.459933</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>127 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1602)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          801500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,005,243
Trainable params: 1,005,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 32s 155ms/step - loss: 1.0635 - accuracy: 0.4221 - auc: 0.6075 - f1_score: 0.4362 - val_loss: 1.0507 - val_accuracy: 0.4347 - val_auc: 0.6261 - val_f1_score: 0.4010
Epoch 2/50
188/188 [==============================] - 31s 165ms/step - loss: 1.0468 - accuracy: 0.4427 - auc: 0.6305 - f1_score: 0.4293 - val_loss: 1.0472 - val_accuracy: 0.4399 - val_auc: 0.6294 - val_f1_score: 0.4284
Epoch 3/50
188/188 [==============================] - 38s 201ms/step - loss: 1.0429 - accuracy: 0.4475 - auc: 0.6358 - f1_score: 0.4392 - val_loss: 1.0460 - val_accuracy: 0.4411 - val_auc: 0.6305 - val_f1_score: 0.4303
Epoch 4/50
188/188 [==============================] - 32s 171ms/step - loss: 1.0413 - accuracy: 0.4512 - auc: 0.6382 - f1_score: 0.4434 - val_loss: 1.0453 - val_accuracy: 0.4428 - val_auc: 0.6320 - val_f1_score: 0.4346
Epoch 5/50
188/188 [==============================] - 31s 162ms/step - loss: 1.0394 - accuracy: 0.4516 - auc: 0.6402 - f1_score: 0.4453 - val_loss: 1.0468 - val_accuracy: 0.4399 - val_auc: 0.6305 - val_f1_score: 0.4351
Epoch 6/50
188/188 [==============================] - 34s 181ms/step - loss: 1.0384 - accuracy: 0.4548 - auc: 0.6422 - f1_score: 0.4502 - val_loss: 1.0447 - val_accuracy: 0.4446 - val_auc: 0.6333 - val_f1_score: 0.4377
Epoch 7/50
188/188 [==============================] - 32s 170ms/step - loss: 1.0374 - accuracy: 0.4558 - auc: 0.6436 - f1_score: 0.4502 - val_loss: 1.0453 - val_accuracy: 0.4397 - val_auc: 0.6324 - val_f1_score: 0.4327
Epoch 8/50
188/188 [==============================] - 30s 159ms/step - loss: 1.0361 - accuracy: 0.4587 - auc: 0.6453 - f1_score: 0.4548 - val_loss: 1.0444 - val_accuracy: 0.4406 - val_auc: 0.6332 - val_f1_score: 0.4323
Epoch 9/50
188/188 [==============================] - 29s 155ms/step - loss: 1.0343 - accuracy: 0.4605 - auc: 0.6477 - f1_score: 0.4560 - val_loss: 1.0443 - val_accuracy: 0.4404 - val_auc: 0.6333 - val_f1_score: 0.4323
Epoch 10/50
188/188 [==============================] - 29s 157ms/step - loss: 1.0327 - accuracy: 0.4603 - auc: 0.6494 - f1_score: 0.4556 - val_loss: 1.0452 - val_accuracy: 0.4430 - val_auc: 0.6326 - val_f1_score: 0.4368
Epoch 11/50
188/188 [==============================] - 30s 159ms/step - loss: 1.0316 - accuracy: 0.4629 - auc: 0.6512 - f1_score: 0.4592 - val_loss: 1.0445 - val_accuracy: 0.4409 - val_auc: 0.6332 - val_f1_score: 0.4299
Epoch 12/50
188/188 [==============================] - 29s 156ms/step - loss: 1.0293 - accuracy: 0.4667 - auc: 0.6539 - f1_score: 0.4625 - val_loss: 1.0469 - val_accuracy: 0.4386 - val_auc: 0.6307 - val_f1_score: 0.4342
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>124</th>
      <td>0.433625</td>
      <td>0.427496</td>
      <td>0.458289</td>
      <td>0.458802</td>
      <td>0.647836</td>
      <td>0.647237</td>
      <td>0.455858</td>
      <td>0.455612</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>125</th>
      <td>0.437437</td>
      <td>0.431571</td>
      <td>0.456513</td>
      <td>0.457733</td>
      <td>0.647148</td>
      <td>0.647397</td>
      <td>0.446097</td>
      <td>0.446063</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>126</th>
      <td>0.433858</td>
      <td>0.427815</td>
      <td>0.459586</td>
      <td>0.460654</td>
      <td>0.649579</td>
      <td>0.649312</td>
      <td>0.451371</td>
      <td>0.451369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>127</th>
      <td>0.436716</td>
      <td>0.430564</td>
      <td>0.466279</td>
      <td>0.466417</td>
      <td>0.655239</td>
      <td>0.654447</td>
      <td>0.460767</td>
      <td>0.459933</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>128</th>
      <td>0.443499</td>
      <td>0.437217</td>
      <td>0.476449</td>
      <td>0.475169</td>
      <td>0.665003</td>
      <td>0.663396</td>
      <td>0.471698</td>
      <td>0.469507</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>128 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1802)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          901500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,105,243
Trainable params: 1,105,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 44s 207ms/step - loss: 1.0676 - accuracy: 0.3980 - auc: 0.5986 - f1_score: 0.4388 - val_loss: 1.0554 - val_accuracy: 0.4360 - val_auc: 0.6178 - val_f1_score: 0.4148
Epoch 2/50
188/188 [==============================] - 34s 183ms/step - loss: 1.0486 - accuracy: 0.4392 - auc: 0.6274 - f1_score: 0.4215 - val_loss: 1.0472 - val_accuracy: 0.4398 - val_auc: 0.6304 - val_f1_score: 0.4193
Epoch 3/50
188/188 [==============================] - 33s 176ms/step - loss: 1.0427 - accuracy: 0.4458 - auc: 0.6355 - f1_score: 0.4303 - val_loss: 1.0466 - val_accuracy: 0.4452 - val_auc: 0.6314 - val_f1_score: 0.4285
Epoch 4/50
188/188 [==============================] - 32s 172ms/step - loss: 1.0408 - accuracy: 0.4497 - auc: 0.6383 - f1_score: 0.4354 - val_loss: 1.0463 - val_accuracy: 0.4430 - val_auc: 0.6320 - val_f1_score: 0.4362
Epoch 5/50
188/188 [==============================] - 34s 182ms/step - loss: 1.0388 - accuracy: 0.4529 - auc: 0.6416 - f1_score: 0.4442 - val_loss: 1.0460 - val_accuracy: 0.4412 - val_auc: 0.6315 - val_f1_score: 0.4292
Epoch 6/50
188/188 [==============================] - 34s 178ms/step - loss: 1.0378 - accuracy: 0.4545 - auc: 0.6432 - f1_score: 0.4452 - val_loss: 1.0457 - val_accuracy: 0.4409 - val_auc: 0.6326 - val_f1_score: 0.4343
Epoch 7/50
188/188 [==============================] - 32s 169ms/step - loss: 1.0362 - accuracy: 0.4571 - auc: 0.6456 - f1_score: 0.4507 - val_loss: 1.0452 - val_accuracy: 0.4436 - val_auc: 0.6328 - val_f1_score: 0.4355
Epoch 8/50
188/188 [==============================] - 32s 169ms/step - loss: 1.0342 - accuracy: 0.4597 - auc: 0.6481 - f1_score: 0.4542 - val_loss: 1.0461 - val_accuracy: 0.4423 - val_auc: 0.6313 - val_f1_score: 0.4371
Epoch 9/50
188/188 [==============================] - 34s 179ms/step - loss: 1.0325 - accuracy: 0.4600 - auc: 0.6495 - f1_score: 0.4547 - val_loss: 1.0462 - val_accuracy: 0.4502 - val_auc: 0.6329 - val_f1_score: 0.4465
Epoch 10/50
188/188 [==============================] - 34s 181ms/step - loss: 1.0297 - accuracy: 0.4637 - auc: 0.6530 - f1_score: 0.4594 - val_loss: 1.0479 - val_accuracy: 0.4409 - val_auc: 0.6324 - val_f1_score: 0.4324
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>125</th>
      <td>0.437437</td>
      <td>0.431571</td>
      <td>0.456513</td>
      <td>0.457733</td>
      <td>0.647148</td>
      <td>0.647397</td>
      <td>0.446097</td>
      <td>0.446063</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>126</th>
      <td>0.433858</td>
      <td>0.427815</td>
      <td>0.459586</td>
      <td>0.460654</td>
      <td>0.649579</td>
      <td>0.649312</td>
      <td>0.451371</td>
      <td>0.451369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>127</th>
      <td>0.436716</td>
      <td>0.430564</td>
      <td>0.466279</td>
      <td>0.466417</td>
      <td>0.655239</td>
      <td>0.654447</td>
      <td>0.460767</td>
      <td>0.459933</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>128</th>
      <td>0.443499</td>
      <td>0.437217</td>
      <td>0.476449</td>
      <td>0.475169</td>
      <td>0.665003</td>
      <td>0.663396</td>
      <td>0.471698</td>
      <td>0.469507</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>129</th>
      <td>0.444824</td>
      <td>0.438636</td>
      <td>0.471806</td>
      <td>0.471621</td>
      <td>0.662844</td>
      <td>0.661722</td>
      <td>0.462585</td>
      <td>0.461235</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>129 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1001500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,205,243
Trainable params: 1,205,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 39s 186ms/step - loss: 1.0679 - accuracy: 0.4115 - auc: 0.5974 - f1_score: 0.4274 - val_loss: 1.0478 - val_accuracy: 0.4369 - val_auc: 0.6295 - val_f1_score: 0.4287
Epoch 2/50
188/188 [==============================] - 38s 201ms/step - loss: 1.0451 - accuracy: 0.4442 - auc: 0.6328 - f1_score: 0.4352 - val_loss: 1.0464 - val_accuracy: 0.4424 - val_auc: 0.6315 - val_f1_score: 0.4362
Epoch 3/50
188/188 [==============================] - 36s 189ms/step - loss: 1.0427 - accuracy: 0.4464 - auc: 0.6356 - f1_score: 0.4363 - val_loss: 1.0449 - val_accuracy: 0.4439 - val_auc: 0.6332 - val_f1_score: 0.4357
Epoch 4/50
188/188 [==============================] - 43s 230ms/step - loss: 1.0405 - accuracy: 0.4502 - auc: 0.6391 - f1_score: 0.4422 - val_loss: 1.0445 - val_accuracy: 0.4472 - val_auc: 0.6348 - val_f1_score: 0.4415
Epoch 5/50
188/188 [==============================] - 38s 201ms/step - loss: 1.0387 - accuracy: 0.4523 - auc: 0.6411 - f1_score: 0.4447 - val_loss: 1.0447 - val_accuracy: 0.4469 - val_auc: 0.6334 - val_f1_score: 0.4403
Epoch 6/50
188/188 [==============================] - 32s 172ms/step - loss: 1.0367 - accuracy: 0.4548 - auc: 0.6434 - f1_score: 0.4478 - val_loss: 1.0432 - val_accuracy: 0.4480 - val_auc: 0.6362 - val_f1_score: 0.4435
Epoch 7/50
188/188 [==============================] - 33s 173ms/step - loss: 1.0359 - accuracy: 0.4560 - auc: 0.6448 - f1_score: 0.4503 - val_loss: 1.0438 - val_accuracy: 0.4478 - val_auc: 0.6353 - val_f1_score: 0.4450
Epoch 8/50
188/188 [==============================] - 33s 173ms/step - loss: 1.0346 - accuracy: 0.4559 - auc: 0.6465 - f1_score: 0.4492 - val_loss: 1.0444 - val_accuracy: 0.4507 - val_auc: 0.6356 - val_f1_score: 0.4482
Epoch 9/50
188/188 [==============================] - 32s 173ms/step - loss: 1.0332 - accuracy: 0.4579 - auc: 0.6478 - f1_score: 0.4534 - val_loss: 1.0444 - val_accuracy: 0.4522 - val_auc: 0.6352 - val_f1_score: 0.4508
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>126</th>
      <td>0.433858</td>
      <td>0.427815</td>
      <td>0.459586</td>
      <td>0.460654</td>
      <td>0.649579</td>
      <td>0.649312</td>
      <td>0.451371</td>
      <td>0.451369</td>
      <td>6</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>127</th>
      <td>0.436716</td>
      <td>0.430564</td>
      <td>0.466279</td>
      <td>0.466417</td>
      <td>0.655239</td>
      <td>0.654447</td>
      <td>0.460767</td>
      <td>0.459933</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>128</th>
      <td>0.443499</td>
      <td>0.437217</td>
      <td>0.476449</td>
      <td>0.475169</td>
      <td>0.665003</td>
      <td>0.663396</td>
      <td>0.471698</td>
      <td>0.469507</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>129</th>
      <td>0.444824</td>
      <td>0.438636</td>
      <td>0.471806</td>
      <td>0.471621</td>
      <td>0.662844</td>
      <td>0.661722</td>
      <td>0.462585</td>
      <td>0.461235</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>130</th>
      <td>0.440709</td>
      <td>0.434575</td>
      <td>0.468143</td>
      <td>0.467809</td>
      <td>0.657931</td>
      <td>0.656786</td>
      <td>0.466323</td>
      <td>0.465363</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>130 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 202)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          101500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 241,043
Trainable params: 241,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 8s 33ms/step - loss: 1.0940 - accuracy: 0.3573 - auc: 0.5397 - f1_score: 0.4115 - val_loss: 1.0857 - val_accuracy: 0.3755 - val_auc: 0.5684 - val_f1_score: 0.3294
Epoch 2/50
188/188 [==============================] - 6s 30ms/step - loss: 1.0806 - accuracy: 0.4002 - auc: 0.5802 - f1_score: 0.3991 - val_loss: 1.0745 - val_accuracy: 0.4012 - val_auc: 0.5917 - val_f1_score: 0.4002
Epoch 3/50
188/188 [==============================] - 6s 29ms/step - loss: 1.0741 - accuracy: 0.4090 - auc: 0.5919 - f1_score: 0.4074 - val_loss: 1.0717 - val_accuracy: 0.4041 - val_auc: 0.5949 - val_f1_score: 0.4020
Epoch 4/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0717 - accuracy: 0.4122 - auc: 0.5955 - f1_score: 0.4100 - val_loss: 1.0712 - val_accuracy: 0.4080 - val_auc: 0.5964 - val_f1_score: 0.4046
Epoch 5/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0711 - accuracy: 0.4117 - auc: 0.5964 - f1_score: 0.4078 - val_loss: 1.0701 - val_accuracy: 0.4088 - val_auc: 0.5978 - val_f1_score: 0.4050
Epoch 6/50
188/188 [==============================] - 6s 29ms/step - loss: 1.0692 - accuracy: 0.4152 - auc: 0.6000 - f1_score: 0.4121 - val_loss: 1.0694 - val_accuracy: 0.4113 - val_auc: 0.5988 - val_f1_score: 0.4012
Epoch 7/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0697 - accuracy: 0.4137 - auc: 0.5983 - f1_score: 0.4094 - val_loss: 1.0698 - val_accuracy: 0.4119 - val_auc: 0.5983 - val_f1_score: 0.4108
Epoch 8/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0693 - accuracy: 0.4157 - auc: 0.5999 - f1_score: 0.4107 - val_loss: 1.0692 - val_accuracy: 0.4144 - val_auc: 0.6003 - val_f1_score: 0.4125
Epoch 9/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0686 - accuracy: 0.4166 - auc: 0.6002 - f1_score: 0.4128 - val_loss: 1.0695 - val_accuracy: 0.4157 - val_auc: 0.5992 - val_f1_score: 0.4122
Epoch 10/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0674 - accuracy: 0.4179 - auc: 0.6025 - f1_score: 0.4140 - val_loss: 1.0695 - val_accuracy: 0.4100 - val_auc: 0.5991 - val_f1_score: 0.4080
Epoch 11/50
188/188 [==============================] - 5s 29ms/step - loss: 1.0679 - accuracy: 0.4167 - auc: 0.6014 - f1_score: 0.4139 - val_loss: 1.0693 - val_accuracy: 0.4128 - val_auc: 0.5991 - val_f1_score: 0.4109
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>127</th>
      <td>0.436716</td>
      <td>0.430564</td>
      <td>0.466279</td>
      <td>0.466417</td>
      <td>0.655239</td>
      <td>0.654447</td>
      <td>0.460767</td>
      <td>0.459933</td>
      <td>7</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>128</th>
      <td>0.443499</td>
      <td>0.437217</td>
      <td>0.476449</td>
      <td>0.475169</td>
      <td>0.665003</td>
      <td>0.663396</td>
      <td>0.471698</td>
      <td>0.469507</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>129</th>
      <td>0.444824</td>
      <td>0.438636</td>
      <td>0.471806</td>
      <td>0.471621</td>
      <td>0.662844</td>
      <td>0.661722</td>
      <td>0.462585</td>
      <td>0.461235</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>130</th>
      <td>0.440709</td>
      <td>0.434575</td>
      <td>0.468143</td>
      <td>0.467809</td>
      <td>0.657931</td>
      <td>0.656786</td>
      <td>0.466323</td>
      <td>0.465363</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.419813</td>
      <td>0.413255</td>
      <td>0.426373</td>
      <td>0.427066</td>
      <td>0.611495</td>
      <td>0.610833</td>
      <td>0.424309</td>
      <td>0.424372</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>131 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 402)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          201500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 341,043
Trainable params: 341,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 11s 45ms/step - loss: 1.0839 - accuracy: 0.3883 - auc: 0.5637 - f1_score: 0.3991 - val_loss: 1.0672 - val_accuracy: 0.4135 - val_auc: 0.6038 - val_f1_score: 0.3553
Epoch 2/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0663 - accuracy: 0.4160 - auc: 0.6022 - f1_score: 0.3873 - val_loss: 1.0627 - val_accuracy: 0.4205 - val_auc: 0.6094 - val_f1_score: 0.3952
Epoch 3/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0616 - accuracy: 0.4233 - auc: 0.6098 - f1_score: 0.4040 - val_loss: 1.0621 - val_accuracy: 0.4181 - val_auc: 0.6105 - val_f1_score: 0.4025
Epoch 4/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0601 - accuracy: 0.4253 - auc: 0.6124 - f1_score: 0.4134 - val_loss: 1.0604 - val_accuracy: 0.4241 - val_auc: 0.6135 - val_f1_score: 0.4128
Epoch 5/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0586 - accuracy: 0.4288 - auc: 0.6152 - f1_score: 0.4208 - val_loss: 1.0595 - val_accuracy: 0.4255 - val_auc: 0.6147 - val_f1_score: 0.4204
Epoch 6/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0572 - accuracy: 0.4287 - auc: 0.6168 - f1_score: 0.4214 - val_loss: 1.0596 - val_accuracy: 0.4257 - val_auc: 0.6146 - val_f1_score: 0.4169
Epoch 7/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0567 - accuracy: 0.4304 - auc: 0.6178 - f1_score: 0.4250 - val_loss: 1.0595 - val_accuracy: 0.4290 - val_auc: 0.6156 - val_f1_score: 0.4258
Epoch 8/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0548 - accuracy: 0.4327 - auc: 0.6205 - f1_score: 0.4267 - val_loss: 1.0587 - val_accuracy: 0.4247 - val_auc: 0.6155 - val_f1_score: 0.4194
Epoch 9/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0551 - accuracy: 0.4315 - auc: 0.6200 - f1_score: 0.4261 - val_loss: 1.0589 - val_accuracy: 0.4302 - val_auc: 0.6157 - val_f1_score: 0.4257
Epoch 10/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0544 - accuracy: 0.4367 - auc: 0.6223 - f1_score: 0.4322 - val_loss: 1.0587 - val_accuracy: 0.4270 - val_auc: 0.6157 - val_f1_score: 0.4227
Epoch 11/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0536 - accuracy: 0.4350 - auc: 0.6225 - f1_score: 0.4299 - val_loss: 1.0587 - val_accuracy: 0.4295 - val_auc: 0.6160 - val_f1_score: 0.4281
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>128</th>
      <td>0.443499</td>
      <td>0.437217</td>
      <td>0.476449</td>
      <td>0.475169</td>
      <td>0.665003</td>
      <td>0.663396</td>
      <td>0.471698</td>
      <td>0.469507</td>
      <td>8</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>129</th>
      <td>0.444824</td>
      <td>0.438636</td>
      <td>0.471806</td>
      <td>0.471621</td>
      <td>0.662844</td>
      <td>0.661722</td>
      <td>0.462585</td>
      <td>0.461235</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>130</th>
      <td>0.440709</td>
      <td>0.434575</td>
      <td>0.468143</td>
      <td>0.467809</td>
      <td>0.657931</td>
      <td>0.656786</td>
      <td>0.466323</td>
      <td>0.465363</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.419813</td>
      <td>0.413255</td>
      <td>0.426373</td>
      <td>0.427066</td>
      <td>0.611495</td>
      <td>0.610833</td>
      <td>0.424309</td>
      <td>0.424372</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>132</th>
      <td>0.426294</td>
      <td>0.419919</td>
      <td>0.444282</td>
      <td>0.444228</td>
      <td>0.632010</td>
      <td>0.631019</td>
      <td>0.442681</td>
      <td>0.442041</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>132 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 602)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          301500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 441,043
Trainable params: 441,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 51ms/step - loss: 1.0759 - accuracy: 0.4027 - auc: 0.5842 - f1_score: 0.4201 - val_loss: 1.0604 - val_accuracy: 0.4218 - val_auc: 0.6123 - val_f1_score: 0.3977
Epoch 2/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0588 - accuracy: 0.4275 - auc: 0.6140 - f1_score: 0.4156 - val_loss: 1.0561 - val_accuracy: 0.4269 - val_auc: 0.6174 - val_f1_score: 0.4090
Epoch 3/50
188/188 [==============================] - 9s 46ms/step - loss: 1.0548 - accuracy: 0.4339 - auc: 0.6205 - f1_score: 0.4238 - val_loss: 1.0549 - val_accuracy: 0.4255 - val_auc: 0.6191 - val_f1_score: 0.4099
Epoch 4/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0525 - accuracy: 0.4370 - auc: 0.6232 - f1_score: 0.4283 - val_loss: 1.0541 - val_accuracy: 0.4312 - val_auc: 0.6212 - val_f1_score: 0.4186
Epoch 5/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0512 - accuracy: 0.4375 - auc: 0.6256 - f1_score: 0.4300 - val_loss: 1.0530 - val_accuracy: 0.4342 - val_auc: 0.6221 - val_f1_score: 0.4282
Epoch 6/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0504 - accuracy: 0.4404 - auc: 0.6271 - f1_score: 0.4343 - val_loss: 1.0530 - val_accuracy: 0.4281 - val_auc: 0.6208 - val_f1_score: 0.4164
Epoch 7/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0492 - accuracy: 0.4409 - auc: 0.6279 - f1_score: 0.4356 - val_loss: 1.0526 - val_accuracy: 0.4309 - val_auc: 0.6225 - val_f1_score: 0.4229
Epoch 8/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0466 - accuracy: 0.4427 - auc: 0.6313 - f1_score: 0.4364 - val_loss: 1.0528 - val_accuracy: 0.4312 - val_auc: 0.6219 - val_f1_score: 0.4242
Epoch 9/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0466 - accuracy: 0.4437 - auc: 0.6319 - f1_score: 0.4391 - val_loss: 1.0526 - val_accuracy: 0.4269 - val_auc: 0.6222 - val_f1_score: 0.4170
Epoch 10/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0460 - accuracy: 0.4461 - auc: 0.6329 - f1_score: 0.4406 - val_loss: 1.0528 - val_accuracy: 0.4347 - val_auc: 0.6230 - val_f1_score: 0.4311
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>129</th>
      <td>0.444824</td>
      <td>0.438636</td>
      <td>0.471806</td>
      <td>0.471621</td>
      <td>0.662844</td>
      <td>0.661722</td>
      <td>0.462585</td>
      <td>0.461235</td>
      <td>9</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>130</th>
      <td>0.440709</td>
      <td>0.434575</td>
      <td>0.468143</td>
      <td>0.467809</td>
      <td>0.657931</td>
      <td>0.656786</td>
      <td>0.466323</td>
      <td>0.465363</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.419813</td>
      <td>0.413255</td>
      <td>0.426373</td>
      <td>0.427066</td>
      <td>0.611495</td>
      <td>0.610833</td>
      <td>0.424309</td>
      <td>0.424372</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>132</th>
      <td>0.426294</td>
      <td>0.419919</td>
      <td>0.444282</td>
      <td>0.444228</td>
      <td>0.632010</td>
      <td>0.631019</td>
      <td>0.442681</td>
      <td>0.442041</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>133</th>
      <td>0.433179</td>
      <td>0.426882</td>
      <td>0.457897</td>
      <td>0.457920</td>
      <td>0.644898</td>
      <td>0.643857</td>
      <td>0.453634</td>
      <td>0.452780</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>133 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 802)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          401500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 541,043
Trainable params: 541,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 62ms/step - loss: 1.0763 - accuracy: 0.4008 - auc: 0.5812 - f1_score: 0.4303 - val_loss: 1.0657 - val_accuracy: 0.4277 - val_auc: 0.6032 - val_f1_score: 0.3841
Epoch 2/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0612 - accuracy: 0.4233 - auc: 0.6081 - f1_score: 0.3869 - val_loss: 1.0607 - val_accuracy: 0.4249 - val_auc: 0.6096 - val_f1_score: 0.3619
Epoch 3/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0555 - accuracy: 0.4319 - auc: 0.6162 - f1_score: 0.3934 - val_loss: 1.0580 - val_accuracy: 0.4281 - val_auc: 0.6129 - val_f1_score: 0.3721
Epoch 4/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0522 - accuracy: 0.4334 - auc: 0.6207 - f1_score: 0.3920 - val_loss: 1.0561 - val_accuracy: 0.4305 - val_auc: 0.6157 - val_f1_score: 0.3828
Epoch 5/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0499 - accuracy: 0.4363 - auc: 0.6247 - f1_score: 0.3991 - val_loss: 1.0531 - val_accuracy: 0.4300 - val_auc: 0.6213 - val_f1_score: 0.3838
Epoch 6/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0469 - accuracy: 0.4404 - auc: 0.6302 - f1_score: 0.4091 - val_loss: 1.0524 - val_accuracy: 0.4297 - val_auc: 0.6226 - val_f1_score: 0.4013
Epoch 7/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0461 - accuracy: 0.4426 - auc: 0.6323 - f1_score: 0.4210 - val_loss: 1.0521 - val_accuracy: 0.4306 - val_auc: 0.6226 - val_f1_score: 0.4026
Epoch 8/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0450 - accuracy: 0.4439 - auc: 0.6331 - f1_score: 0.4239 - val_loss: 1.0524 - val_accuracy: 0.4320 - val_auc: 0.6239 - val_f1_score: 0.4168
Epoch 9/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0427 - accuracy: 0.4451 - auc: 0.6355 - f1_score: 0.4302 - val_loss: 1.0522 - val_accuracy: 0.4295 - val_auc: 0.6239 - val_f1_score: 0.4141
Epoch 10/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0419 - accuracy: 0.4473 - auc: 0.6367 - f1_score: 0.4333 - val_loss: 1.0506 - val_accuracy: 0.4307 - val_auc: 0.6248 - val_f1_score: 0.4182
Epoch 11/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0403 - accuracy: 0.4504 - auc: 0.6393 - f1_score: 0.4379 - val_loss: 1.0501 - val_accuracy: 0.4338 - val_auc: 0.6255 - val_f1_score: 0.4166
Epoch 12/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0393 - accuracy: 0.4496 - auc: 0.6406 - f1_score: 0.4376 - val_loss: 1.0502 - val_accuracy: 0.4305 - val_auc: 0.6251 - val_f1_score: 0.4149
Epoch 13/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0380 - accuracy: 0.4517 - auc: 0.6418 - f1_score: 0.4413 - val_loss: 1.0515 - val_accuracy: 0.4307 - val_auc: 0.6252 - val_f1_score: 0.4198
Epoch 14/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0375 - accuracy: 0.4528 - auc: 0.6425 - f1_score: 0.4443 - val_loss: 1.0518 - val_accuracy: 0.4351 - val_auc: 0.6240 - val_f1_score: 0.4242
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>130</th>
      <td>0.440709</td>
      <td>0.434575</td>
      <td>0.468143</td>
      <td>0.467809</td>
      <td>0.657931</td>
      <td>0.656786</td>
      <td>0.466323</td>
      <td>0.465363</td>
      <td>10</td>
      <td>LSTM</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.419813</td>
      <td>0.413255</td>
      <td>0.426373</td>
      <td>0.427066</td>
      <td>0.611495</td>
      <td>0.610833</td>
      <td>0.424309</td>
      <td>0.424372</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>132</th>
      <td>0.426294</td>
      <td>0.419919</td>
      <td>0.444282</td>
      <td>0.444228</td>
      <td>0.632010</td>
      <td>0.631019</td>
      <td>0.442681</td>
      <td>0.442041</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>133</th>
      <td>0.433179</td>
      <td>0.426882</td>
      <td>0.457897</td>
      <td>0.457920</td>
      <td>0.644898</td>
      <td>0.643857</td>
      <td>0.453634</td>
      <td>0.452780</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>134</th>
      <td>0.437145</td>
      <td>0.430733</td>
      <td>0.465593</td>
      <td>0.465947</td>
      <td>0.656052</td>
      <td>0.654516</td>
      <td>0.454056</td>
      <td>0.453177</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>134 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          501500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 641,043
Trainable params: 641,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 15s 69ms/step - loss: 1.0753 - accuracy: 0.4032 - auc: 0.5823 - f1_score: 0.4187 - val_loss: 1.0605 - val_accuracy: 0.4186 - val_auc: 0.6109 - val_f1_score: 0.4113
Epoch 2/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0549 - accuracy: 0.4311 - auc: 0.6183 - f1_score: 0.4217 - val_loss: 1.0535 - val_accuracy: 0.4337 - val_auc: 0.6205 - val_f1_score: 0.4303
Epoch 3/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0495 - accuracy: 0.4375 - auc: 0.6258 - f1_score: 0.4278 - val_loss: 1.0506 - val_accuracy: 0.4355 - val_auc: 0.6251 - val_f1_score: 0.4338
Epoch 4/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0465 - accuracy: 0.4420 - auc: 0.6309 - f1_score: 0.4342 - val_loss: 1.0496 - val_accuracy: 0.4385 - val_auc: 0.6264 - val_f1_score: 0.4361
Epoch 5/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0443 - accuracy: 0.4457 - auc: 0.6341 - f1_score: 0.4376 - val_loss: 1.0498 - val_accuracy: 0.4357 - val_auc: 0.6258 - val_f1_score: 0.4355
Epoch 6/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0424 - accuracy: 0.4480 - auc: 0.6366 - f1_score: 0.4436 - val_loss: 1.0479 - val_accuracy: 0.4355 - val_auc: 0.6280 - val_f1_score: 0.4322
Epoch 7/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0403 - accuracy: 0.4497 - auc: 0.6391 - f1_score: 0.4424 - val_loss: 1.0480 - val_accuracy: 0.4415 - val_auc: 0.6276 - val_f1_score: 0.4397
Epoch 8/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0392 - accuracy: 0.4515 - auc: 0.6407 - f1_score: 0.4465 - val_loss: 1.0476 - val_accuracy: 0.4421 - val_auc: 0.6283 - val_f1_score: 0.4402
Epoch 9/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0381 - accuracy: 0.4506 - auc: 0.6411 - f1_score: 0.4444 - val_loss: 1.0472 - val_accuracy: 0.4376 - val_auc: 0.6287 - val_f1_score: 0.4356
Epoch 10/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0355 - accuracy: 0.4558 - auc: 0.6453 - f1_score: 0.4502 - val_loss: 1.0481 - val_accuracy: 0.4373 - val_auc: 0.6285 - val_f1_score: 0.4369
Epoch 11/50
188/188 [==============================] - 13s 66ms/step - loss: 1.0336 - accuracy: 0.4562 - auc: 0.6473 - f1_score: 0.4508 - val_loss: 1.0486 - val_accuracy: 0.4357 - val_auc: 0.6266 - val_f1_score: 0.4336
Epoch 12/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0332 - accuracy: 0.4552 - auc: 0.6474 - f1_score: 0.4493 - val_loss: 1.0485 - val_accuracy: 0.4405 - val_auc: 0.6279 - val_f1_score: 0.4383
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>131</th>
      <td>0.419813</td>
      <td>0.413255</td>
      <td>0.426373</td>
      <td>0.427066</td>
      <td>0.611495</td>
      <td>0.610833</td>
      <td>0.424309</td>
      <td>0.424372</td>
      <td>1</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>132</th>
      <td>0.426294</td>
      <td>0.419919</td>
      <td>0.444282</td>
      <td>0.444228</td>
      <td>0.632010</td>
      <td>0.631019</td>
      <td>0.442681</td>
      <td>0.442041</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>133</th>
      <td>0.433179</td>
      <td>0.426882</td>
      <td>0.457897</td>
      <td>0.457920</td>
      <td>0.644898</td>
      <td>0.643857</td>
      <td>0.453634</td>
      <td>0.452780</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>134</th>
      <td>0.437145</td>
      <td>0.430733</td>
      <td>0.465593</td>
      <td>0.465947</td>
      <td>0.656052</td>
      <td>0.654516</td>
      <td>0.454056</td>
      <td>0.453177</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.439640</td>
      <td>0.432968</td>
      <td>0.472111</td>
      <td>0.470239</td>
      <td>0.663504</td>
      <td>0.660690</td>
      <td>0.469338</td>
      <td>0.466793</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>135 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1202)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          601500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 741,043
Trainable params: 741,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 18s 80ms/step - loss: 1.0736 - accuracy: 0.4012 - auc: 0.5862 - f1_score: 0.4344 - val_loss: 1.0598 - val_accuracy: 0.4186 - val_auc: 0.6087 - val_f1_score: 0.3801
Epoch 2/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0540 - accuracy: 0.4292 - auc: 0.6191 - f1_score: 0.4118 - val_loss: 1.0541 - val_accuracy: 0.4325 - val_auc: 0.6180 - val_f1_score: 0.4072
Epoch 3/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0475 - accuracy: 0.4348 - auc: 0.6279 - f1_score: 0.4240 - val_loss: 1.0511 - val_accuracy: 0.4362 - val_auc: 0.6230 - val_f1_score: 0.4265
Epoch 4/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0437 - accuracy: 0.4409 - auc: 0.6339 - f1_score: 0.4334 - val_loss: 1.0505 - val_accuracy: 0.4339 - val_auc: 0.6239 - val_f1_score: 0.4195
Epoch 5/50
188/188 [==============================] - 14s 75ms/step - loss: 1.0402 - accuracy: 0.4469 - auc: 0.6381 - f1_score: 0.4381 - val_loss: 1.0493 - val_accuracy: 0.4379 - val_auc: 0.6260 - val_f1_score: 0.4249
Epoch 6/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0386 - accuracy: 0.4487 - auc: 0.6400 - f1_score: 0.4427 - val_loss: 1.0480 - val_accuracy: 0.4391 - val_auc: 0.6278 - val_f1_score: 0.4317
Epoch 7/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0365 - accuracy: 0.4516 - auc: 0.6431 - f1_score: 0.4469 - val_loss: 1.0483 - val_accuracy: 0.4424 - val_auc: 0.6273 - val_f1_score: 0.4367
Epoch 8/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0345 - accuracy: 0.4554 - auc: 0.6458 - f1_score: 0.4505 - val_loss: 1.0483 - val_accuracy: 0.4412 - val_auc: 0.6280 - val_f1_score: 0.4359
Epoch 9/50
188/188 [==============================] - 20s 104ms/step - loss: 1.0325 - accuracy: 0.4579 - auc: 0.6490 - f1_score: 0.4529 - val_loss: 1.0499 - val_accuracy: 0.4371 - val_auc: 0.6266 - val_f1_score: 0.4284
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>132</th>
      <td>0.426294</td>
      <td>0.419919</td>
      <td>0.444282</td>
      <td>0.444228</td>
      <td>0.632010</td>
      <td>0.631019</td>
      <td>0.442681</td>
      <td>0.442041</td>
      <td>2</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>133</th>
      <td>0.433179</td>
      <td>0.426882</td>
      <td>0.457897</td>
      <td>0.457920</td>
      <td>0.644898</td>
      <td>0.643857</td>
      <td>0.453634</td>
      <td>0.452780</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>134</th>
      <td>0.437145</td>
      <td>0.430733</td>
      <td>0.465593</td>
      <td>0.465947</td>
      <td>0.656052</td>
      <td>0.654516</td>
      <td>0.454056</td>
      <td>0.453177</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.439640</td>
      <td>0.432968</td>
      <td>0.472111</td>
      <td>0.470239</td>
      <td>0.663504</td>
      <td>0.660690</td>
      <td>0.469338</td>
      <td>0.466793</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>136</th>
      <td>0.442673</td>
      <td>0.436216</td>
      <td>0.470967</td>
      <td>0.471033</td>
      <td>0.663809</td>
      <td>0.661921</td>
      <td>0.462056</td>
      <td>0.461017</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>136 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1402)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          701500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 841,043
Trainable params: 841,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 20s 91ms/step - loss: 1.0697 - accuracy: 0.4102 - auc: 0.5945 - f1_score: 0.4368 - val_loss: 1.0576 - val_accuracy: 0.4272 - val_auc: 0.6150 - val_f1_score: 0.4102
Epoch 2/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0527 - accuracy: 0.4321 - auc: 0.6210 - f1_score: 0.4239 - val_loss: 1.0545 - val_accuracy: 0.4278 - val_auc: 0.6183 - val_f1_score: 0.4019
Epoch 3/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0450 - accuracy: 0.4439 - auc: 0.6320 - f1_score: 0.4352 - val_loss: 1.0520 - val_accuracy: 0.4372 - val_auc: 0.6226 - val_f1_score: 0.4208
Epoch 4/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0437 - accuracy: 0.4444 - auc: 0.6333 - f1_score: 0.4380 - val_loss: 1.0509 - val_accuracy: 0.4386 - val_auc: 0.6249 - val_f1_score: 0.4274
Epoch 5/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0402 - accuracy: 0.4477 - auc: 0.6380 - f1_score: 0.4395 - val_loss: 1.0502 - val_accuracy: 0.4338 - val_auc: 0.6251 - val_f1_score: 0.4155
Epoch 6/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0366 - accuracy: 0.4546 - auc: 0.6431 - f1_score: 0.4484 - val_loss: 1.0517 - val_accuracy: 0.4416 - val_auc: 0.6247 - val_f1_score: 0.4259
Epoch 7/50
188/188 [==============================] - 16s 88ms/step - loss: 1.0358 - accuracy: 0.4530 - auc: 0.6445 - f1_score: 0.4469 - val_loss: 1.0485 - val_accuracy: 0.4411 - val_auc: 0.6279 - val_f1_score: 0.4324
Epoch 8/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0331 - accuracy: 0.4554 - auc: 0.6465 - f1_score: 0.4501 - val_loss: 1.0481 - val_accuracy: 0.4421 - val_auc: 0.6289 - val_f1_score: 0.4358
Epoch 9/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0303 - accuracy: 0.4586 - auc: 0.6505 - f1_score: 0.4542 - val_loss: 1.0498 - val_accuracy: 0.4403 - val_auc: 0.6260 - val_f1_score: 0.4326
Epoch 10/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0282 - accuracy: 0.4646 - auc: 0.6540 - f1_score: 0.4593 - val_loss: 1.0495 - val_accuracy: 0.4369 - val_auc: 0.6267 - val_f1_score: 0.4271
Epoch 11/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0271 - accuracy: 0.4631 - auc: 0.6541 - f1_score: 0.4588 - val_loss: 1.0486 - val_accuracy: 0.4435 - val_auc: 0.6295 - val_f1_score: 0.4361
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>133</th>
      <td>0.433179</td>
      <td>0.426882</td>
      <td>0.457897</td>
      <td>0.457920</td>
      <td>0.644898</td>
      <td>0.643857</td>
      <td>0.453634</td>
      <td>0.452780</td>
      <td>3</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>134</th>
      <td>0.437145</td>
      <td>0.430733</td>
      <td>0.465593</td>
      <td>0.465947</td>
      <td>0.656052</td>
      <td>0.654516</td>
      <td>0.454056</td>
      <td>0.453177</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.439640</td>
      <td>0.432968</td>
      <td>0.472111</td>
      <td>0.470239</td>
      <td>0.663504</td>
      <td>0.660690</td>
      <td>0.469338</td>
      <td>0.466793</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>136</th>
      <td>0.442673</td>
      <td>0.436216</td>
      <td>0.470967</td>
      <td>0.471033</td>
      <td>0.663809</td>
      <td>0.661921</td>
      <td>0.462056</td>
      <td>0.461017</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.447949</td>
      <td>0.441129</td>
      <td>0.485431</td>
      <td>0.483020</td>
      <td>0.676126</td>
      <td>0.672693</td>
      <td>0.477340</td>
      <td>0.473801</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>137 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1602)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          801500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 941,043
Trainable params: 941,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 20s 95ms/step - loss: 1.0771 - accuracy: 0.3971 - auc: 0.5807 - f1_score: 0.4411 - val_loss: 1.0603 - val_accuracy: 0.4212 - val_auc: 0.6102 - val_f1_score: 0.4041
Epoch 2/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0528 - accuracy: 0.4300 - auc: 0.6214 - f1_score: 0.4228 - val_loss: 1.0548 - val_accuracy: 0.4278 - val_auc: 0.6188 - val_f1_score: 0.4067
Epoch 3/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0463 - accuracy: 0.4411 - auc: 0.6312 - f1_score: 0.4330 - val_loss: 1.0522 - val_accuracy: 0.4360 - val_auc: 0.6229 - val_f1_score: 0.4194
Epoch 4/50
188/188 [==============================] - 18s 97ms/step - loss: 1.0418 - accuracy: 0.4456 - auc: 0.6366 - f1_score: 0.4372 - val_loss: 1.0500 - val_accuracy: 0.4400 - val_auc: 0.6265 - val_f1_score: 0.4311
Epoch 5/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0372 - accuracy: 0.4539 - auc: 0.6429 - f1_score: 0.4476 - val_loss: 1.0504 - val_accuracy: 0.4386 - val_auc: 0.6270 - val_f1_score: 0.4246
Epoch 6/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0345 - accuracy: 0.4562 - auc: 0.6458 - f1_score: 0.4490 - val_loss: 1.0493 - val_accuracy: 0.4381 - val_auc: 0.6266 - val_f1_score: 0.4290
Epoch 7/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0320 - accuracy: 0.4596 - auc: 0.6497 - f1_score: 0.4531 - val_loss: 1.0513 - val_accuracy: 0.4371 - val_auc: 0.6263 - val_f1_score: 0.4248
Epoch 8/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0282 - accuracy: 0.4642 - auc: 0.6542 - f1_score: 0.4585 - val_loss: 1.0505 - val_accuracy: 0.4362 - val_auc: 0.6275 - val_f1_score: 0.4280
Epoch 9/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0251 - accuracy: 0.4676 - auc: 0.6583 - f1_score: 0.4637 - val_loss: 1.0506 - val_accuracy: 0.4412 - val_auc: 0.6277 - val_f1_score: 0.4297
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>134</th>
      <td>0.437145</td>
      <td>0.430733</td>
      <td>0.465593</td>
      <td>0.465947</td>
      <td>0.656052</td>
      <td>0.654516</td>
      <td>0.454056</td>
      <td>0.453177</td>
      <td>4</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.439640</td>
      <td>0.432968</td>
      <td>0.472111</td>
      <td>0.470239</td>
      <td>0.663504</td>
      <td>0.660690</td>
      <td>0.469338</td>
      <td>0.466793</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>136</th>
      <td>0.442673</td>
      <td>0.436216</td>
      <td>0.470967</td>
      <td>0.471033</td>
      <td>0.663809</td>
      <td>0.661921</td>
      <td>0.462056</td>
      <td>0.461017</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.447949</td>
      <td>0.441129</td>
      <td>0.485431</td>
      <td>0.483020</td>
      <td>0.676126</td>
      <td>0.672693</td>
      <td>0.477340</td>
      <td>0.473801</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>138</th>
      <td>0.448555</td>
      <td>0.441939</td>
      <td>0.485126</td>
      <td>0.483941</td>
      <td>0.675458</td>
      <td>0.672918</td>
      <td>0.473531</td>
      <td>0.471083</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>138 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1802)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          901500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,041,043
Trainable params: 1,041,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 23s 109ms/step - loss: 1.0666 - accuracy: 0.4132 - auc: 0.5983 - f1_score: 0.4429 - val_loss: 1.0548 - val_accuracy: 0.4308 - val_auc: 0.6180 - val_f1_score: 0.4283
Epoch 2/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0508 - accuracy: 0.4367 - auc: 0.6247 - f1_score: 0.4298 - val_loss: 1.0523 - val_accuracy: 0.4337 - val_auc: 0.6230 - val_f1_score: 0.4294
Epoch 3/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0442 - accuracy: 0.4449 - auc: 0.6333 - f1_score: 0.4395 - val_loss: 1.0510 - val_accuracy: 0.4356 - val_auc: 0.6255 - val_f1_score: 0.4255
Epoch 4/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0405 - accuracy: 0.4494 - auc: 0.6391 - f1_score: 0.4426 - val_loss: 1.0494 - val_accuracy: 0.4373 - val_auc: 0.6282 - val_f1_score: 0.4300
Epoch 5/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0366 - accuracy: 0.4561 - auc: 0.6440 - f1_score: 0.4498 - val_loss: 1.0494 - val_accuracy: 0.4367 - val_auc: 0.6268 - val_f1_score: 0.4341
Epoch 6/50
188/188 [==============================] - 18s 97ms/step - loss: 1.0335 - accuracy: 0.4592 - auc: 0.6474 - f1_score: 0.4547 - val_loss: 1.0485 - val_accuracy: 0.4415 - val_auc: 0.6284 - val_f1_score: 0.4385
Epoch 7/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0298 - accuracy: 0.4633 - auc: 0.6525 - f1_score: 0.4578 - val_loss: 1.0489 - val_accuracy: 0.4412 - val_auc: 0.6291 - val_f1_score: 0.4322
Epoch 8/50
188/188 [==============================] - 18s 98ms/step - loss: 1.0270 - accuracy: 0.4652 - auc: 0.6554 - f1_score: 0.4595 - val_loss: 1.0493 - val_accuracy: 0.4416 - val_auc: 0.6278 - val_f1_score: 0.4393
Epoch 9/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0244 - accuracy: 0.4699 - auc: 0.6588 - f1_score: 0.4646 - val_loss: 1.0500 - val_accuracy: 0.4368 - val_auc: 0.6275 - val_f1_score: 0.4342
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>135</th>
      <td>0.439640</td>
      <td>0.432968</td>
      <td>0.472111</td>
      <td>0.470239</td>
      <td>0.663504</td>
      <td>0.660690</td>
      <td>0.469338</td>
      <td>0.466793</td>
      <td>5</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>136</th>
      <td>0.442673</td>
      <td>0.436216</td>
      <td>0.470967</td>
      <td>0.471033</td>
      <td>0.663809</td>
      <td>0.661921</td>
      <td>0.462056</td>
      <td>0.461017</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.447949</td>
      <td>0.441129</td>
      <td>0.485431</td>
      <td>0.483020</td>
      <td>0.676126</td>
      <td>0.672693</td>
      <td>0.477340</td>
      <td>0.473801</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>138</th>
      <td>0.448555</td>
      <td>0.441939</td>
      <td>0.485126</td>
      <td>0.483941</td>
      <td>0.675458</td>
      <td>0.672918</td>
      <td>0.473531</td>
      <td>0.471083</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>139</th>
      <td>0.448673</td>
      <td>0.441760</td>
      <td>0.488211</td>
      <td>0.485578</td>
      <td>0.678334</td>
      <td>0.674655</td>
      <td>0.485613</td>
      <td>0.482312</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>139 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 2)
Test shape: (10292, 2)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 2)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2002)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1001500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,141,043
Trainable params: 1,141,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 24s 112ms/step - loss: 1.0767 - accuracy: 0.3967 - auc: 0.5827 - f1_score: 0.4462 - val_loss: 1.0607 - val_accuracy: 0.4281 - val_auc: 0.6100 - val_f1_score: 0.4233
Epoch 2/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0523 - accuracy: 0.4353 - auc: 0.6234 - f1_score: 0.4315 - val_loss: 1.0528 - val_accuracy: 0.4385 - val_auc: 0.6227 - val_f1_score: 0.4293
Epoch 3/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0445 - accuracy: 0.4432 - auc: 0.6332 - f1_score: 0.4394 - val_loss: 1.0500 - val_accuracy: 0.4372 - val_auc: 0.6268 - val_f1_score: 0.4324
Epoch 4/50
188/188 [==============================] - 22s 116ms/step - loss: 1.0390 - accuracy: 0.4526 - auc: 0.6406 - f1_score: 0.4494 - val_loss: 1.0502 - val_accuracy: 0.4448 - val_auc: 0.6281 - val_f1_score: 0.4422
Epoch 5/50
188/188 [==============================] - 21s 113ms/step - loss: 1.0339 - accuracy: 0.4582 - auc: 0.6471 - f1_score: 0.4555 - val_loss: 1.0516 - val_accuracy: 0.4388 - val_auc: 0.6258 - val_f1_score: 0.4345
Epoch 6/50
188/188 [==============================] - 21s 112ms/step - loss: 1.0320 - accuracy: 0.4610 - auc: 0.6499 - f1_score: 0.4598 - val_loss: 1.0497 - val_accuracy: 0.4417 - val_auc: 0.6281 - val_f1_score: 0.4417
Epoch 7/50
188/188 [==============================] - 21s 112ms/step - loss: 1.0274 - accuracy: 0.4666 - auc: 0.6553 - f1_score: 0.4652 - val_loss: 1.0499 - val_accuracy: 0.4441 - val_auc: 0.6285 - val_f1_score: 0.4439
Epoch 8/50
188/188 [==============================] - 22s 114ms/step - loss: 1.0237 - accuracy: 0.4703 - auc: 0.6599 - f1_score: 0.4681 - val_loss: 1.0501 - val_accuracy: 0.4436 - val_auc: 0.6275 - val_f1_score: 0.4430
Epoch 9/50
188/188 [==============================] - 21s 113ms/step - loss: 1.0194 - accuracy: 0.4772 - auc: 0.6647 - f1_score: 0.4762 - val_loss: 1.0513 - val_accuracy: 0.4348 - val_auc: 0.6267 - val_f1_score: 0.4338
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>136</th>
      <td>0.442673</td>
      <td>0.436216</td>
      <td>0.470967</td>
      <td>0.471033</td>
      <td>0.663809</td>
      <td>0.661921</td>
      <td>0.462056</td>
      <td>0.461017</td>
      <td>6</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.447949</td>
      <td>0.441129</td>
      <td>0.485431</td>
      <td>0.483020</td>
      <td>0.676126</td>
      <td>0.672693</td>
      <td>0.477340</td>
      <td>0.473801</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>138</th>
      <td>0.448555</td>
      <td>0.441939</td>
      <td>0.485126</td>
      <td>0.483941</td>
      <td>0.675458</td>
      <td>0.672918</td>
      <td>0.473531</td>
      <td>0.471083</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>139</th>
      <td>0.448673</td>
      <td>0.441760</td>
      <td>0.488211</td>
      <td>0.485578</td>
      <td>0.678334</td>
      <td>0.674655</td>
      <td>0.485613</td>
      <td>0.482312</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>140</th>
      <td>0.451428</td>
      <td>0.444303</td>
      <td>0.496081</td>
      <td>0.492145</td>
      <td>0.685261</td>
      <td>0.680838</td>
      <td>0.495120</td>
      <td>0.490604</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>140 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 205)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          103000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 306,743
Trainable params: 306,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 40ms/step - loss: 1.0661 - accuracy: 0.4198 - auc: 0.6029 - f1_score: 0.4471 - val_loss: 1.0561 - val_accuracy: 0.4330 - val_auc: 0.6199 - val_f1_score: 0.3972
Epoch 2/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0531 - accuracy: 0.4339 - auc: 0.6216 - f1_score: 0.4071 - val_loss: 1.0553 - val_accuracy: 0.4336 - val_auc: 0.6208 - val_f1_score: 0.4056
Epoch 3/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0510 - accuracy: 0.4360 - auc: 0.6241 - f1_score: 0.4179 - val_loss: 1.0547 - val_accuracy: 0.4333 - val_auc: 0.6217 - val_f1_score: 0.4187
Epoch 4/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0494 - accuracy: 0.4366 - auc: 0.6262 - f1_score: 0.4228 - val_loss: 1.0546 - val_accuracy: 0.4314 - val_auc: 0.6209 - val_f1_score: 0.4217
Epoch 5/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0487 - accuracy: 0.4394 - auc: 0.6274 - f1_score: 0.4272 - val_loss: 1.0539 - val_accuracy: 0.4317 - val_auc: 0.6223 - val_f1_score: 0.4218
Epoch 6/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0488 - accuracy: 0.4373 - auc: 0.6271 - f1_score: 0.4272 - val_loss: 1.0544 - val_accuracy: 0.4296 - val_auc: 0.6219 - val_f1_score: 0.4158
Epoch 7/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0482 - accuracy: 0.4396 - auc: 0.6280 - f1_score: 0.4281 - val_loss: 1.0543 - val_accuracy: 0.4318 - val_auc: 0.6217 - val_f1_score: 0.4227
Epoch 8/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0479 - accuracy: 0.4401 - auc: 0.6284 - f1_score: 0.4289 - val_loss: 1.0544 - val_accuracy: 0.4314 - val_auc: 0.6211 - val_f1_score: 0.4221
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>137</th>
      <td>0.447949</td>
      <td>0.441129</td>
      <td>0.485431</td>
      <td>0.483020</td>
      <td>0.676126</td>
      <td>0.672693</td>
      <td>0.477340</td>
      <td>0.473801</td>
      <td>7</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>138</th>
      <td>0.448555</td>
      <td>0.441939</td>
      <td>0.485126</td>
      <td>0.483941</td>
      <td>0.675458</td>
      <td>0.672918</td>
      <td>0.473531</td>
      <td>0.471083</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>139</th>
      <td>0.448673</td>
      <td>0.441760</td>
      <td>0.488211</td>
      <td>0.485578</td>
      <td>0.678334</td>
      <td>0.674655</td>
      <td>0.485613</td>
      <td>0.482312</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>140</th>
      <td>0.451428</td>
      <td>0.444303</td>
      <td>0.496081</td>
      <td>0.492145</td>
      <td>0.685261</td>
      <td>0.680838</td>
      <td>0.495120</td>
      <td>0.490604</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>141</th>
      <td>0.429937</td>
      <td>0.423962</td>
      <td>0.441012</td>
      <td>0.442914</td>
      <td>0.630998</td>
      <td>0.631433</td>
      <td>0.431248</td>
      <td>0.431992</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>141 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 405)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          203000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 406,743
Trainable params: 406,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 57ms/step - loss: 1.0665 - accuracy: 0.4216 - auc: 0.6044 - f1_score: 0.4064 - val_loss: 1.0551 - val_accuracy: 0.4259 - val_auc: 0.6191 - val_f1_score: 0.3543
Epoch 2/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0488 - accuracy: 0.4358 - auc: 0.6266 - f1_score: 0.3721 - val_loss: 1.0527 - val_accuracy: 0.4314 - val_auc: 0.6239 - val_f1_score: 0.3702
Epoch 3/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0464 - accuracy: 0.4365 - auc: 0.6294 - f1_score: 0.3864 - val_loss: 1.0525 - val_accuracy: 0.4289 - val_auc: 0.6232 - val_f1_score: 0.3847
Epoch 4/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0452 - accuracy: 0.4404 - auc: 0.6311 - f1_score: 0.4051 - val_loss: 1.0515 - val_accuracy: 0.4313 - val_auc: 0.6245 - val_f1_score: 0.4035
Epoch 5/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0447 - accuracy: 0.4406 - auc: 0.6320 - f1_score: 0.4162 - val_loss: 1.0513 - val_accuracy: 0.4336 - val_auc: 0.6251 - val_f1_score: 0.4122
Epoch 6/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0445 - accuracy: 0.4433 - auc: 0.6331 - f1_score: 0.4244 - val_loss: 1.0510 - val_accuracy: 0.4355 - val_auc: 0.6257 - val_f1_score: 0.4208
Epoch 7/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0440 - accuracy: 0.4456 - auc: 0.6344 - f1_score: 0.4282 - val_loss: 1.0508 - val_accuracy: 0.4342 - val_auc: 0.6259 - val_f1_score: 0.4193
Epoch 8/50
188/188 [==============================] - 11s 56ms/step - loss: 1.0435 - accuracy: 0.4445 - auc: 0.6340 - f1_score: 0.4323 - val_loss: 1.0515 - val_accuracy: 0.4372 - val_auc: 0.6254 - val_f1_score: 0.4266
Epoch 9/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0433 - accuracy: 0.4457 - auc: 0.6349 - f1_score: 0.4342 - val_loss: 1.0514 - val_accuracy: 0.4330 - val_auc: 0.6252 - val_f1_score: 0.4204
Epoch 10/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0427 - accuracy: 0.4463 - auc: 0.6358 - f1_score: 0.4366 - val_loss: 1.0515 - val_accuracy: 0.4313 - val_auc: 0.6252 - val_f1_score: 0.4172
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>138</th>
      <td>0.448555</td>
      <td>0.441939</td>
      <td>0.485126</td>
      <td>0.483941</td>
      <td>0.675458</td>
      <td>0.672918</td>
      <td>0.473531</td>
      <td>0.471083</td>
      <td>8</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>139</th>
      <td>0.448673</td>
      <td>0.441760</td>
      <td>0.488211</td>
      <td>0.485578</td>
      <td>0.678334</td>
      <td>0.674655</td>
      <td>0.485613</td>
      <td>0.482312</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>140</th>
      <td>0.451428</td>
      <td>0.444303</td>
      <td>0.496081</td>
      <td>0.492145</td>
      <td>0.685261</td>
      <td>0.680838</td>
      <td>0.495120</td>
      <td>0.490604</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>141</th>
      <td>0.429937</td>
      <td>0.423962</td>
      <td>0.441012</td>
      <td>0.442914</td>
      <td>0.630998</td>
      <td>0.631433</td>
      <td>0.431248</td>
      <td>0.431992</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>142</th>
      <td>0.433087</td>
      <td>0.427183</td>
      <td>0.448152</td>
      <td>0.449942</td>
      <td>0.638633</td>
      <td>0.638949</td>
      <td>0.434724</td>
      <td>0.435165</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>142 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 605)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          303000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 506,743
Trainable params: 506,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 75ms/step - loss: 1.0608 - accuracy: 0.4261 - auc: 0.6102 - f1_score: 0.4068 - val_loss: 1.0508 - val_accuracy: 0.4326 - val_auc: 0.6259 - val_f1_score: 0.3585
Epoch 2/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0471 - accuracy: 0.4365 - auc: 0.6293 - f1_score: 0.3821 - val_loss: 1.0490 - val_accuracy: 0.4397 - val_auc: 0.6284 - val_f1_score: 0.4054
Epoch 3/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0446 - accuracy: 0.4426 - auc: 0.6322 - f1_score: 0.4163 - val_loss: 1.0494 - val_accuracy: 0.4404 - val_auc: 0.6280 - val_f1_score: 0.4242
Epoch 4/50
188/188 [==============================] - 13s 70ms/step - loss: 1.0432 - accuracy: 0.4451 - auc: 0.6345 - f1_score: 0.4271 - val_loss: 1.0481 - val_accuracy: 0.4404 - val_auc: 0.6294 - val_f1_score: 0.4291
Epoch 5/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0425 - accuracy: 0.4450 - auc: 0.6356 - f1_score: 0.4339 - val_loss: 1.0482 - val_accuracy: 0.4390 - val_auc: 0.6295 - val_f1_score: 0.4348
Epoch 6/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0423 - accuracy: 0.4467 - auc: 0.6355 - f1_score: 0.4396 - val_loss: 1.0481 - val_accuracy: 0.4381 - val_auc: 0.6301 - val_f1_score: 0.4307
Epoch 7/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0412 - accuracy: 0.4473 - auc: 0.6368 - f1_score: 0.4389 - val_loss: 1.0481 - val_accuracy: 0.4409 - val_auc: 0.6297 - val_f1_score: 0.4348
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>139</th>
      <td>0.448673</td>
      <td>0.441760</td>
      <td>0.488211</td>
      <td>0.485578</td>
      <td>0.678334</td>
      <td>0.674655</td>
      <td>0.485613</td>
      <td>0.482312</td>
      <td>9</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>140</th>
      <td>0.451428</td>
      <td>0.444303</td>
      <td>0.496081</td>
      <td>0.492145</td>
      <td>0.685261</td>
      <td>0.680838</td>
      <td>0.495120</td>
      <td>0.490604</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>141</th>
      <td>0.429937</td>
      <td>0.423962</td>
      <td>0.441012</td>
      <td>0.442914</td>
      <td>0.630998</td>
      <td>0.631433</td>
      <td>0.431248</td>
      <td>0.431992</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>142</th>
      <td>0.433087</td>
      <td>0.427183</td>
      <td>0.448152</td>
      <td>0.449942</td>
      <td>0.638633</td>
      <td>0.638949</td>
      <td>0.434724</td>
      <td>0.435165</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>143</th>
      <td>0.432942</td>
      <td>0.427066</td>
      <td>0.451313</td>
      <td>0.452872</td>
      <td>0.640787</td>
      <td>0.641096</td>
      <td>0.444817</td>
      <td>0.445348</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>143 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 805)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          403000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 606,743
Trainable params: 606,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 23s 104ms/step - loss: 1.0662 - accuracy: 0.4237 - auc: 0.6034 - f1_score: 0.4255 - val_loss: 1.0493 - val_accuracy: 0.4376 - val_auc: 0.6277 - val_f1_score: 0.4289
Epoch 2/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0471 - accuracy: 0.4402 - auc: 0.6299 - f1_score: 0.4310 - val_loss: 1.0468 - val_accuracy: 0.4394 - val_auc: 0.6308 - val_f1_score: 0.4251
Epoch 3/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0434 - accuracy: 0.4439 - auc: 0.6341 - f1_score: 0.4342 - val_loss: 1.0458 - val_accuracy: 0.4410 - val_auc: 0.6321 - val_f1_score: 0.4344
Epoch 4/50
188/188 [==============================] - 19s 103ms/step - loss: 1.0426 - accuracy: 0.4470 - auc: 0.6355 - f1_score: 0.4412 - val_loss: 1.0466 - val_accuracy: 0.4374 - val_auc: 0.6315 - val_f1_score: 0.4276
Epoch 5/50
188/188 [==============================] - 20s 104ms/step - loss: 1.0408 - accuracy: 0.4486 - auc: 0.6375 - f1_score: 0.4408 - val_loss: 1.0456 - val_accuracy: 0.4393 - val_auc: 0.6328 - val_f1_score: 0.4353
Epoch 6/50
188/188 [==============================] - 20s 107ms/step - loss: 1.0398 - accuracy: 0.4486 - auc: 0.6390 - f1_score: 0.4445 - val_loss: 1.0461 - val_accuracy: 0.4421 - val_auc: 0.6327 - val_f1_score: 0.4332
Epoch 7/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0391 - accuracy: 0.4513 - auc: 0.6404 - f1_score: 0.4463 - val_loss: 1.0455 - val_accuracy: 0.4439 - val_auc: 0.6335 - val_f1_score: 0.4362
Epoch 8/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0381 - accuracy: 0.4505 - auc: 0.6416 - f1_score: 0.4452 - val_loss: 1.0456 - val_accuracy: 0.4415 - val_auc: 0.6330 - val_f1_score: 0.4374
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>140</th>
      <td>0.451428</td>
      <td>0.444303</td>
      <td>0.496081</td>
      <td>0.492145</td>
      <td>0.685261</td>
      <td>0.680838</td>
      <td>0.495120</td>
      <td>0.490604</td>
      <td>10</td>
      <td>Simple</td>
      <td>2</td>
      <td>7</td>
    </tr>
    <tr>
      <th>141</th>
      <td>0.429937</td>
      <td>0.423962</td>
      <td>0.441012</td>
      <td>0.442914</td>
      <td>0.630998</td>
      <td>0.631433</td>
      <td>0.431248</td>
      <td>0.431992</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>142</th>
      <td>0.433087</td>
      <td>0.427183</td>
      <td>0.448152</td>
      <td>0.449942</td>
      <td>0.638633</td>
      <td>0.638949</td>
      <td>0.434724</td>
      <td>0.435165</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>143</th>
      <td>0.432942</td>
      <td>0.427066</td>
      <td>0.451313</td>
      <td>0.452872</td>
      <td>0.640787</td>
      <td>0.641096</td>
      <td>0.444817</td>
      <td>0.445348</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>144</th>
      <td>0.434501</td>
      <td>0.428540</td>
      <td>0.456709</td>
      <td>0.457606</td>
      <td>0.647541</td>
      <td>0.647354</td>
      <td>0.452009</td>
      <td>0.451953</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>144 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          503000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 706,743
Trainable params: 706,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 26s 120ms/step - loss: 1.0717 - accuracy: 0.4138 - auc: 0.5901 - f1_score: 0.4188 - val_loss: 1.0557 - val_accuracy: 0.4307 - val_auc: 0.6166 - val_f1_score: 0.3506
Epoch 2/50
188/188 [==============================] - 22s 115ms/step - loss: 1.0499 - accuracy: 0.4418 - auc: 0.6270 - f1_score: 0.4037 - val_loss: 1.0478 - val_accuracy: 0.4397 - val_auc: 0.6283 - val_f1_score: 0.4142
Epoch 3/50
188/188 [==============================] - 22s 116ms/step - loss: 1.0435 - accuracy: 0.4462 - auc: 0.6346 - f1_score: 0.4315 - val_loss: 1.0459 - val_accuracy: 0.4428 - val_auc: 0.6317 - val_f1_score: 0.4333
Epoch 4/50
188/188 [==============================] - 21s 114ms/step - loss: 1.0412 - accuracy: 0.4479 - auc: 0.6374 - f1_score: 0.4407 - val_loss: 1.0457 - val_accuracy: 0.4411 - val_auc: 0.6317 - val_f1_score: 0.4294
Epoch 5/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0398 - accuracy: 0.4490 - auc: 0.6392 - f1_score: 0.4413 - val_loss: 1.0448 - val_accuracy: 0.4430 - val_auc: 0.6332 - val_f1_score: 0.4361
Epoch 6/50
188/188 [==============================] - 22s 120ms/step - loss: 1.0384 - accuracy: 0.4530 - auc: 0.6414 - f1_score: 0.4467 - val_loss: 1.0460 - val_accuracy: 0.4410 - val_auc: 0.6316 - val_f1_score: 0.4331
Epoch 7/50
188/188 [==============================] - 23s 120ms/step - loss: 1.0375 - accuracy: 0.4517 - auc: 0.6420 - f1_score: 0.4459 - val_loss: 1.0458 - val_accuracy: 0.4397 - val_auc: 0.6327 - val_f1_score: 0.4293
Epoch 8/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0363 - accuracy: 0.4533 - auc: 0.6432 - f1_score: 0.4486 - val_loss: 1.0456 - val_accuracy: 0.4406 - val_auc: 0.6322 - val_f1_score: 0.4320
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>141</th>
      <td>0.429937</td>
      <td>0.423962</td>
      <td>0.441012</td>
      <td>0.442914</td>
      <td>0.630998</td>
      <td>0.631433</td>
      <td>0.431248</td>
      <td>0.431992</td>
      <td>1</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>142</th>
      <td>0.433087</td>
      <td>0.427183</td>
      <td>0.448152</td>
      <td>0.449942</td>
      <td>0.638633</td>
      <td>0.638949</td>
      <td>0.434724</td>
      <td>0.435165</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>143</th>
      <td>0.432942</td>
      <td>0.427066</td>
      <td>0.451313</td>
      <td>0.452872</td>
      <td>0.640787</td>
      <td>0.641096</td>
      <td>0.444817</td>
      <td>0.445348</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>144</th>
      <td>0.434501</td>
      <td>0.428540</td>
      <td>0.456709</td>
      <td>0.457606</td>
      <td>0.647541</td>
      <td>0.647354</td>
      <td>0.452009</td>
      <td>0.451953</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>145</th>
      <td>0.437018</td>
      <td>0.431113</td>
      <td>0.456752</td>
      <td>0.458253</td>
      <td>0.648169</td>
      <td>0.648208</td>
      <td>0.447489</td>
      <td>0.447758</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>145 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1205)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          603000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 806,743
Trainable params: 806,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 32s 148ms/step - loss: 1.0605 - accuracy: 0.4247 - auc: 0.6116 - f1_score: 0.4279 - val_loss: 1.0482 - val_accuracy: 0.4348 - val_auc: 0.6286 - val_f1_score: 0.3885
Epoch 2/50
188/188 [==============================] - 26s 137ms/step - loss: 1.0428 - accuracy: 0.4439 - auc: 0.6350 - f1_score: 0.4188 - val_loss: 1.0458 - val_accuracy: 0.4405 - val_auc: 0.6331 - val_f1_score: 0.4240
Epoch 3/50
188/188 [==============================] - 26s 139ms/step - loss: 1.0412 - accuracy: 0.4487 - auc: 0.6376 - f1_score: 0.4347 - val_loss: 1.0440 - val_accuracy: 0.4450 - val_auc: 0.6350 - val_f1_score: 0.4326
Epoch 4/50
188/188 [==============================] - 26s 140ms/step - loss: 1.0384 - accuracy: 0.4513 - auc: 0.6409 - f1_score: 0.4410 - val_loss: 1.0447 - val_accuracy: 0.4487 - val_auc: 0.6349 - val_f1_score: 0.4387
Epoch 5/50
188/188 [==============================] - 27s 141ms/step - loss: 1.0374 - accuracy: 0.4521 - auc: 0.6424 - f1_score: 0.4431 - val_loss: 1.0452 - val_accuracy: 0.4456 - val_auc: 0.6343 - val_f1_score: 0.4367
Epoch 6/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0358 - accuracy: 0.4527 - auc: 0.6439 - f1_score: 0.4459 - val_loss: 1.0454 - val_accuracy: 0.4453 - val_auc: 0.6347 - val_f1_score: 0.4410
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>142</th>
      <td>0.433087</td>
      <td>0.427183</td>
      <td>0.448152</td>
      <td>0.449942</td>
      <td>0.638633</td>
      <td>0.638949</td>
      <td>0.434724</td>
      <td>0.435165</td>
      <td>2</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>143</th>
      <td>0.432942</td>
      <td>0.427066</td>
      <td>0.451313</td>
      <td>0.452872</td>
      <td>0.640787</td>
      <td>0.641096</td>
      <td>0.444817</td>
      <td>0.445348</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>144</th>
      <td>0.434501</td>
      <td>0.428540</td>
      <td>0.456709</td>
      <td>0.457606</td>
      <td>0.647541</td>
      <td>0.647354</td>
      <td>0.452009</td>
      <td>0.451953</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>145</th>
      <td>0.437018</td>
      <td>0.431113</td>
      <td>0.456752</td>
      <td>0.458253</td>
      <td>0.648169</td>
      <td>0.648208</td>
      <td>0.447489</td>
      <td>0.447758</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.438215</td>
      <td>0.432382</td>
      <td>0.457428</td>
      <td>0.458841</td>
      <td>0.649401</td>
      <td>0.649439</td>
      <td>0.452412</td>
      <td>0.452896</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>146 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1405)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          703000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 906,743
Trainable params: 906,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 37s 163ms/step - loss: 1.0682 - accuracy: 0.4194 - auc: 0.5990 - f1_score: 0.4321 - val_loss: 1.0480 - val_accuracy: 0.4444 - val_auc: 0.6302 - val_f1_score: 0.4407
Epoch 2/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0441 - accuracy: 0.4472 - auc: 0.6347 - f1_score: 0.4427 - val_loss: 1.0456 - val_accuracy: 0.4518 - val_auc: 0.6336 - val_f1_score: 0.4492
Epoch 3/50
188/188 [==============================] - 28s 151ms/step - loss: 1.0407 - accuracy: 0.4494 - auc: 0.6388 - f1_score: 0.4438 - val_loss: 1.0442 - val_accuracy: 0.4484 - val_auc: 0.6350 - val_f1_score: 0.4436
Epoch 4/50
188/188 [==============================] - 34s 182ms/step - loss: 1.0390 - accuracy: 0.4518 - auc: 0.6409 - f1_score: 0.4475 - val_loss: 1.0443 - val_accuracy: 0.4494 - val_auc: 0.6355 - val_f1_score: 0.4431
Epoch 5/50
188/188 [==============================] - 30s 161ms/step - loss: 1.0369 - accuracy: 0.4538 - auc: 0.6431 - f1_score: 0.4489 - val_loss: 1.0439 - val_accuracy: 0.4471 - val_auc: 0.6356 - val_f1_score: 0.4393
Epoch 6/50
188/188 [==============================] - 29s 155ms/step - loss: 1.0357 - accuracy: 0.4558 - auc: 0.6449 - f1_score: 0.4513 - val_loss: 1.0440 - val_accuracy: 0.4466 - val_auc: 0.6356 - val_f1_score: 0.4459
Epoch 7/50
188/188 [==============================] - 29s 152ms/step - loss: 1.0339 - accuracy: 0.4571 - auc: 0.6472 - f1_score: 0.4529 - val_loss: 1.0439 - val_accuracy: 0.4469 - val_auc: 0.6353 - val_f1_score: 0.4428
Epoch 8/50
188/188 [==============================] - 29s 153ms/step - loss: 1.0334 - accuracy: 0.4598 - auc: 0.6486 - f1_score: 0.4566 - val_loss: 1.0441 - val_accuracy: 0.4463 - val_auc: 0.6355 - val_f1_score: 0.4459
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>143</th>
      <td>0.432942</td>
      <td>0.427066</td>
      <td>0.451313</td>
      <td>0.452872</td>
      <td>0.640787</td>
      <td>0.641096</td>
      <td>0.444817</td>
      <td>0.445348</td>
      <td>3</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>144</th>
      <td>0.434501</td>
      <td>0.428540</td>
      <td>0.456709</td>
      <td>0.457606</td>
      <td>0.647541</td>
      <td>0.647354</td>
      <td>0.452009</td>
      <td>0.451953</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>145</th>
      <td>0.437018</td>
      <td>0.431113</td>
      <td>0.456752</td>
      <td>0.458253</td>
      <td>0.648169</td>
      <td>0.648208</td>
      <td>0.447489</td>
      <td>0.447758</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.438215</td>
      <td>0.432382</td>
      <td>0.457428</td>
      <td>0.458841</td>
      <td>0.649401</td>
      <td>0.649439</td>
      <td>0.452412</td>
      <td>0.452896</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.439712</td>
      <td>0.433618</td>
      <td>0.468198</td>
      <td>0.468024</td>
      <td>0.657026</td>
      <td>0.655993</td>
      <td>0.467623</td>
      <td>0.466876</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>147 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1605)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          803000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,006,743
Trainable params: 1,006,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 38s 179ms/step - loss: 1.0796 - accuracy: 0.3802 - auc: 0.5817 - f1_score: 0.4272 - val_loss: 1.0632 - val_accuracy: 0.3915 - val_auc: 0.6076 - val_f1_score: 0.3426
Epoch 2/50
188/188 [==============================] - 31s 163ms/step - loss: 1.0519 - accuracy: 0.4272 - auc: 0.6232 - f1_score: 0.4200 - val_loss: 1.0457 - val_accuracy: 0.4384 - val_auc: 0.6326 - val_f1_score: 0.4131
Epoch 3/50
188/188 [==============================] - 32s 168ms/step - loss: 1.0429 - accuracy: 0.4462 - auc: 0.6352 - f1_score: 0.4309 - val_loss: 1.0438 - val_accuracy: 0.4418 - val_auc: 0.6352 - val_f1_score: 0.4325
Epoch 4/50
188/188 [==============================] - 30s 159ms/step - loss: 1.0398 - accuracy: 0.4503 - auc: 0.6394 - f1_score: 0.4404 - val_loss: 1.0438 - val_accuracy: 0.4493 - val_auc: 0.6361 - val_f1_score: 0.4429
Epoch 5/50
188/188 [==============================] - 31s 163ms/step - loss: 1.0384 - accuracy: 0.4503 - auc: 0.6408 - f1_score: 0.4409 - val_loss: 1.0435 - val_accuracy: 0.4447 - val_auc: 0.6354 - val_f1_score: 0.4370
Epoch 6/50
188/188 [==============================] - 32s 170ms/step - loss: 1.0361 - accuracy: 0.4557 - auc: 0.6444 - f1_score: 0.4466 - val_loss: 1.0443 - val_accuracy: 0.4451 - val_auc: 0.6348 - val_f1_score: 0.4428
Epoch 7/50
188/188 [==============================] - 30s 161ms/step - loss: 1.0350 - accuracy: 0.4568 - auc: 0.6462 - f1_score: 0.4500 - val_loss: 1.0427 - val_accuracy: 0.4502 - val_auc: 0.6373 - val_f1_score: 0.4459
Epoch 8/50
188/188 [==============================] - 31s 163ms/step - loss: 1.0339 - accuracy: 0.4577 - auc: 0.6472 - f1_score: 0.4501 - val_loss: 1.0432 - val_accuracy: 0.4454 - val_auc: 0.6359 - val_f1_score: 0.4425
Epoch 9/50
188/188 [==============================] - 31s 167ms/step - loss: 1.0328 - accuracy: 0.4612 - auc: 0.6490 - f1_score: 0.4545 - val_loss: 1.0440 - val_accuracy: 0.4444 - val_auc: 0.6355 - val_f1_score: 0.4416
Epoch 10/50
188/188 [==============================] - 32s 169ms/step - loss: 1.0307 - accuracy: 0.4621 - auc: 0.6512 - f1_score: 0.4552 - val_loss: 1.0437 - val_accuracy: 0.4483 - val_auc: 0.6360 - val_f1_score: 0.4405
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>144</th>
      <td>0.434501</td>
      <td>0.428540</td>
      <td>0.456709</td>
      <td>0.457606</td>
      <td>0.647541</td>
      <td>0.647354</td>
      <td>0.452009</td>
      <td>0.451953</td>
      <td>4</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>145</th>
      <td>0.437018</td>
      <td>0.431113</td>
      <td>0.456752</td>
      <td>0.458253</td>
      <td>0.648169</td>
      <td>0.648208</td>
      <td>0.447489</td>
      <td>0.447758</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.438215</td>
      <td>0.432382</td>
      <td>0.457428</td>
      <td>0.458841</td>
      <td>0.649401</td>
      <td>0.649439</td>
      <td>0.452412</td>
      <td>0.452896</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.439712</td>
      <td>0.433618</td>
      <td>0.468198</td>
      <td>0.468024</td>
      <td>0.657026</td>
      <td>0.655993</td>
      <td>0.467623</td>
      <td>0.466876</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.440498</td>
      <td>0.434461</td>
      <td>0.468340</td>
      <td>0.468603</td>
      <td>0.659218</td>
      <td>0.658343</td>
      <td>0.460074</td>
      <td>0.459289</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>148 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1805)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          903000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,106,743
Trainable params: 1,106,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 42s 199ms/step - loss: 1.0758 - accuracy: 0.4044 - auc: 0.5765 - f1_score: 0.4307 - val_loss: 1.0665 - val_accuracy: 0.4418 - val_auc: 0.6055 - val_f1_score: 0.4354
Epoch 2/50
188/188 [==============================] - 37s 197ms/step - loss: 1.0613 - accuracy: 0.4451 - auc: 0.6105 - f1_score: 0.4420 - val_loss: 1.0596 - val_accuracy: 0.4421 - val_auc: 0.6162 - val_f1_score: 0.4421
Epoch 3/50
188/188 [==============================] - 39s 205ms/step - loss: 1.0538 - accuracy: 0.4491 - auc: 0.6247 - f1_score: 0.4505 - val_loss: 1.0538 - val_accuracy: 0.4412 - val_auc: 0.6248 - val_f1_score: 0.4413
Epoch 4/50
188/188 [==============================] - 38s 202ms/step - loss: 1.0465 - accuracy: 0.4496 - auc: 0.6341 - f1_score: 0.4502 - val_loss: 1.0471 - val_accuracy: 0.4446 - val_auc: 0.6323 - val_f1_score: 0.4452
Epoch 5/50
188/188 [==============================] - 43s 226ms/step - loss: 1.0403 - accuracy: 0.4535 - auc: 0.6406 - f1_score: 0.4532 - val_loss: 1.0460 - val_accuracy: 0.4411 - val_auc: 0.6322 - val_f1_score: 0.4389
Epoch 6/50
188/188 [==============================] - 37s 199ms/step - loss: 1.0361 - accuracy: 0.4583 - auc: 0.6451 - f1_score: 0.4574 - val_loss: 1.0451 - val_accuracy: 0.4420 - val_auc: 0.6334 - val_f1_score: 0.4398
Epoch 7/50
188/188 [==============================] - 37s 199ms/step - loss: 1.0341 - accuracy: 0.4586 - auc: 0.6473 - f1_score: 0.4576 - val_loss: 1.0467 - val_accuracy: 0.4363 - val_auc: 0.6303 - val_f1_score: 0.4329
Epoch 8/50
188/188 [==============================] - 39s 208ms/step - loss: 1.0321 - accuracy: 0.4619 - auc: 0.6502 - f1_score: 0.4601 - val_loss: 1.0458 - val_accuracy: 0.4432 - val_auc: 0.6320 - val_f1_score: 0.4430
Epoch 9/50
188/188 [==============================] - 38s 203ms/step - loss: 1.0292 - accuracy: 0.4645 - auc: 0.6534 - f1_score: 0.4634 - val_loss: 1.0462 - val_accuracy: 0.4404 - val_auc: 0.6316 - val_f1_score: 0.4404
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>0.437018</td>
      <td>0.431113</td>
      <td>0.456752</td>
      <td>0.458253</td>
      <td>0.648169</td>
      <td>0.648208</td>
      <td>0.447489</td>
      <td>0.447758</td>
      <td>5</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.438215</td>
      <td>0.432382</td>
      <td>0.457428</td>
      <td>0.458841</td>
      <td>0.649401</td>
      <td>0.649439</td>
      <td>0.452412</td>
      <td>0.452896</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.439712</td>
      <td>0.433618</td>
      <td>0.468198</td>
      <td>0.468024</td>
      <td>0.657026</td>
      <td>0.655993</td>
      <td>0.467623</td>
      <td>0.466876</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.440498</td>
      <td>0.434461</td>
      <td>0.468340</td>
      <td>0.468603</td>
      <td>0.659218</td>
      <td>0.658343</td>
      <td>0.460074</td>
      <td>0.459289</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.440332</td>
      <td>0.433895</td>
      <td>0.476003</td>
      <td>0.474522</td>
      <td>0.664681</td>
      <td>0.662718</td>
      <td>0.476024</td>
      <td>0.474121</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>149 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1003000     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,206,743
Trainable params: 1,206,743
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 46s 219ms/step - loss: 1.0718 - accuracy: 0.4067 - auc: 0.5837 - f1_score: 0.4353 - val_loss: 1.0614 - val_accuracy: 0.4460 - val_auc: 0.6164 - val_f1_score: 0.4269
Epoch 2/50
188/188 [==============================] - 43s 228ms/step - loss: 1.0546 - accuracy: 0.4452 - auc: 0.6221 - f1_score: 0.4371 - val_loss: 1.0510 - val_accuracy: 0.4468 - val_auc: 0.6288 - val_f1_score: 0.4439
Epoch 3/50
188/188 [==============================] - 41s 220ms/step - loss: 1.0454 - accuracy: 0.4484 - auc: 0.6338 - f1_score: 0.4455 - val_loss: 1.0453 - val_accuracy: 0.4457 - val_auc: 0.6337 - val_f1_score: 0.4418
Epoch 4/50
188/188 [==============================] - 40s 215ms/step - loss: 1.0409 - accuracy: 0.4505 - auc: 0.6383 - f1_score: 0.4482 - val_loss: 1.0447 - val_accuracy: 0.4487 - val_auc: 0.6340 - val_f1_score: 0.4462
Epoch 5/50
188/188 [==============================] - 39s 209ms/step - loss: 1.0372 - accuracy: 0.4546 - auc: 0.6427 - f1_score: 0.4527 - val_loss: 1.0437 - val_accuracy: 0.4499 - val_auc: 0.6360 - val_f1_score: 0.4449
Epoch 6/50
188/188 [==============================] - 39s 207ms/step - loss: 1.0355 - accuracy: 0.4577 - auc: 0.6456 - f1_score: 0.4562 - val_loss: 1.0446 - val_accuracy: 0.4463 - val_auc: 0.6349 - val_f1_score: 0.4441
Epoch 7/50
188/188 [==============================] - 39s 206ms/step - loss: 1.0331 - accuracy: 0.4595 - auc: 0.6484 - f1_score: 0.4573 - val_loss: 1.0453 - val_accuracy: 0.4453 - val_auc: 0.6341 - val_f1_score: 0.4415
Epoch 8/50
188/188 [==============================] - 40s 211ms/step - loss: 1.0313 - accuracy: 0.4641 - auc: 0.6509 - f1_score: 0.4624 - val_loss: 1.0441 - val_accuracy: 0.4490 - val_auc: 0.6362 - val_f1_score: 0.4466
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>146</th>
      <td>0.438215</td>
      <td>0.432382</td>
      <td>0.457428</td>
      <td>0.458841</td>
      <td>0.649401</td>
      <td>0.649439</td>
      <td>0.452412</td>
      <td>0.452896</td>
      <td>6</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.439712</td>
      <td>0.433618</td>
      <td>0.468198</td>
      <td>0.468024</td>
      <td>0.657026</td>
      <td>0.655993</td>
      <td>0.467623</td>
      <td>0.466876</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.440498</td>
      <td>0.434461</td>
      <td>0.468340</td>
      <td>0.468603</td>
      <td>0.659218</td>
      <td>0.658343</td>
      <td>0.460074</td>
      <td>0.459289</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.440332</td>
      <td>0.433895</td>
      <td>0.476003</td>
      <td>0.474522</td>
      <td>0.664681</td>
      <td>0.662718</td>
      <td>0.476024</td>
      <td>0.474121</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>150</th>
      <td>0.442646</td>
      <td>0.436500</td>
      <td>0.472623</td>
      <td>0.472111</td>
      <td>0.661681</td>
      <td>0.660480</td>
      <td>0.470172</td>
      <td>0.468912</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 205)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          103000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 242,543
Trainable params: 242,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 39ms/step - loss: 1.0696 - accuracy: 0.4108 - auc: 0.5955 - f1_score: 0.4335 - val_loss: 1.0566 - val_accuracy: 0.4287 - val_auc: 0.6168 - val_f1_score: 0.4026
Epoch 2/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0536 - accuracy: 0.4312 - auc: 0.6202 - f1_score: 0.4115 - val_loss: 1.0539 - val_accuracy: 0.4306 - val_auc: 0.6217 - val_f1_score: 0.4213
Epoch 3/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0517 - accuracy: 0.4346 - auc: 0.6227 - f1_score: 0.4228 - val_loss: 1.0547 - val_accuracy: 0.4313 - val_auc: 0.6211 - val_f1_score: 0.4208
Epoch 4/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0503 - accuracy: 0.4361 - auc: 0.6249 - f1_score: 0.4226 - val_loss: 1.0539 - val_accuracy: 0.4321 - val_auc: 0.6224 - val_f1_score: 0.4235
Epoch 5/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0494 - accuracy: 0.4356 - auc: 0.6266 - f1_score: 0.4236 - val_loss: 1.0532 - val_accuracy: 0.4317 - val_auc: 0.6234 - val_f1_score: 0.4218
Epoch 6/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0478 - accuracy: 0.4400 - auc: 0.6286 - f1_score: 0.4304 - val_loss: 1.0538 - val_accuracy: 0.4355 - val_auc: 0.6234 - val_f1_score: 0.4280
Epoch 7/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0482 - accuracy: 0.4393 - auc: 0.6284 - f1_score: 0.4284 - val_loss: 1.0539 - val_accuracy: 0.4341 - val_auc: 0.6232 - val_f1_score: 0.4254
Epoch 8/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0470 - accuracy: 0.4404 - auc: 0.6300 - f1_score: 0.4298 - val_loss: 1.0537 - val_accuracy: 0.4324 - val_auc: 0.6234 - val_f1_score: 0.4238
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>147</th>
      <td>0.439712</td>
      <td>0.433618</td>
      <td>0.468198</td>
      <td>0.468024</td>
      <td>0.657026</td>
      <td>0.655993</td>
      <td>0.467623</td>
      <td>0.466876</td>
      <td>7</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.440498</td>
      <td>0.434461</td>
      <td>0.468340</td>
      <td>0.468603</td>
      <td>0.659218</td>
      <td>0.658343</td>
      <td>0.460074</td>
      <td>0.459289</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.440332</td>
      <td>0.433895</td>
      <td>0.476003</td>
      <td>0.474522</td>
      <td>0.664681</td>
      <td>0.662718</td>
      <td>0.476024</td>
      <td>0.474121</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>150</th>
      <td>0.442646</td>
      <td>0.436500</td>
      <td>0.472623</td>
      <td>0.472111</td>
      <td>0.661681</td>
      <td>0.660480</td>
      <td>0.470172</td>
      <td>0.468912</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.429854</td>
      <td>0.423773</td>
      <td>0.443846</td>
      <td>0.445443</td>
      <td>0.634226</td>
      <td>0.634307</td>
      <td>0.434921</td>
      <td>0.435411</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>151 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 405)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          203000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 342,543
Trainable params: 342,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 46ms/step - loss: 1.0694 - accuracy: 0.4077 - auc: 0.5938 - f1_score: 0.4172 - val_loss: 1.0572 - val_accuracy: 0.4300 - val_auc: 0.6185 - val_f1_score: 0.3866
Epoch 2/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0533 - accuracy: 0.4302 - auc: 0.6197 - f1_score: 0.4062 - val_loss: 1.0551 - val_accuracy: 0.4362 - val_auc: 0.6206 - val_f1_score: 0.4126
Epoch 3/50
188/188 [==============================] - 8s 42ms/step - loss: 1.0497 - accuracy: 0.4386 - auc: 0.6267 - f1_score: 0.4198 - val_loss: 1.0531 - val_accuracy: 0.4331 - val_auc: 0.6229 - val_f1_score: 0.4164
Epoch 4/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0473 - accuracy: 0.4393 - auc: 0.6287 - f1_score: 0.4250 - val_loss: 1.0533 - val_accuracy: 0.4288 - val_auc: 0.6234 - val_f1_score: 0.4141
Epoch 5/50
188/188 [==============================] - 9s 45ms/step - loss: 1.0473 - accuracy: 0.4399 - auc: 0.6289 - f1_score: 0.4287 - val_loss: 1.0526 - val_accuracy: 0.4299 - val_auc: 0.6240 - val_f1_score: 0.4141
Epoch 6/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0453 - accuracy: 0.4420 - auc: 0.6320 - f1_score: 0.4301 - val_loss: 1.0518 - val_accuracy: 0.4330 - val_auc: 0.6248 - val_f1_score: 0.4180
Epoch 7/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0447 - accuracy: 0.4434 - auc: 0.6330 - f1_score: 0.4329 - val_loss: 1.0514 - val_accuracy: 0.4336 - val_auc: 0.6254 - val_f1_score: 0.4221
Epoch 8/50
188/188 [==============================] - 7s 39ms/step - loss: 1.0432 - accuracy: 0.4454 - auc: 0.6349 - f1_score: 0.4359 - val_loss: 1.0519 - val_accuracy: 0.4317 - val_auc: 0.6251 - val_f1_score: 0.4213
Epoch 9/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0426 - accuracy: 0.4455 - auc: 0.6356 - f1_score: 0.4358 - val_loss: 1.0513 - val_accuracy: 0.4303 - val_auc: 0.6256 - val_f1_score: 0.4225
Epoch 10/50
188/188 [==============================] - 7s 40ms/step - loss: 1.0414 - accuracy: 0.4475 - auc: 0.6371 - f1_score: 0.4399 - val_loss: 1.0524 - val_accuracy: 0.4308 - val_auc: 0.6246 - val_f1_score: 0.4217
Epoch 11/50
188/188 [==============================] - 7s 40ms/step - loss: 1.0421 - accuracy: 0.4470 - auc: 0.6368 - f1_score: 0.4392 - val_loss: 1.0515 - val_accuracy: 0.4337 - val_auc: 0.6255 - val_f1_score: 0.4252
Epoch 12/50
188/188 [==============================] - 8s 40ms/step - loss: 1.0416 - accuracy: 0.4462 - auc: 0.6371 - f1_score: 0.4390 - val_loss: 1.0521 - val_accuracy: 0.4329 - val_auc: 0.6253 - val_f1_score: 0.4278
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>148</th>
      <td>0.440498</td>
      <td>0.434461</td>
      <td>0.468340</td>
      <td>0.468603</td>
      <td>0.659218</td>
      <td>0.658343</td>
      <td>0.460074</td>
      <td>0.459289</td>
      <td>8</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.440332</td>
      <td>0.433895</td>
      <td>0.476003</td>
      <td>0.474522</td>
      <td>0.664681</td>
      <td>0.662718</td>
      <td>0.476024</td>
      <td>0.474121</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>150</th>
      <td>0.442646</td>
      <td>0.436500</td>
      <td>0.472623</td>
      <td>0.472111</td>
      <td>0.661681</td>
      <td>0.660480</td>
      <td>0.470172</td>
      <td>0.468912</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.429854</td>
      <td>0.423773</td>
      <td>0.443846</td>
      <td>0.445443</td>
      <td>0.634226</td>
      <td>0.634307</td>
      <td>0.434921</td>
      <td>0.435411</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.435427</td>
      <td>0.429323</td>
      <td>0.453635</td>
      <td>0.454440</td>
      <td>0.645088</td>
      <td>0.644512</td>
      <td>0.448329</td>
      <td>0.448198</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>152 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 605)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          303000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 442,543
Trainable params: 442,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 52ms/step - loss: 1.0865 - accuracy: 0.3627 - auc: 0.5468 - f1_score: 0.4002 - val_loss: 1.0732 - val_accuracy: 0.4266 - val_auc: 0.6010 - val_f1_score: 0.4132
Epoch 2/50
188/188 [==============================] - 9s 48ms/step - loss: 1.0715 - accuracy: 0.4198 - auc: 0.5964 - f1_score: 0.4145 - val_loss: 1.0649 - val_accuracy: 0.4296 - val_auc: 0.6145 - val_f1_score: 0.4245
Epoch 3/50
188/188 [==============================] - 9s 49ms/step - loss: 1.0604 - accuracy: 0.4286 - auc: 0.6148 - f1_score: 0.4217 - val_loss: 1.0548 - val_accuracy: 0.4295 - val_auc: 0.6219 - val_f1_score: 0.4108
Epoch 4/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0506 - accuracy: 0.4377 - auc: 0.6253 - f1_score: 0.4307 - val_loss: 1.0504 - val_accuracy: 0.4345 - val_auc: 0.6271 - val_f1_score: 0.4237
Epoch 5/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0474 - accuracy: 0.4423 - auc: 0.6297 - f1_score: 0.4350 - val_loss: 1.0501 - val_accuracy: 0.4357 - val_auc: 0.6269 - val_f1_score: 0.4233
Epoch 6/50
188/188 [==============================] - 9s 47ms/step - loss: 1.0445 - accuracy: 0.4428 - auc: 0.6329 - f1_score: 0.4341 - val_loss: 1.0493 - val_accuracy: 0.4359 - val_auc: 0.6276 - val_f1_score: 0.4220
Epoch 7/50
188/188 [==============================] - 9s 50ms/step - loss: 1.0423 - accuracy: 0.4472 - auc: 0.6357 - f1_score: 0.4414 - val_loss: 1.0496 - val_accuracy: 0.4356 - val_auc: 0.6274 - val_f1_score: 0.4264
Epoch 8/50
188/188 [==============================] - 10s 51ms/step - loss: 1.0411 - accuracy: 0.4485 - auc: 0.6382 - f1_score: 0.4426 - val_loss: 1.0472 - val_accuracy: 0.4405 - val_auc: 0.6309 - val_f1_score: 0.4349
Epoch 9/50
188/188 [==============================] - 9s 48ms/step - loss: 1.0400 - accuracy: 0.4496 - auc: 0.6394 - f1_score: 0.4453 - val_loss: 1.0477 - val_accuracy: 0.4402 - val_auc: 0.6300 - val_f1_score: 0.4300
Epoch 10/50
188/188 [==============================] - 9s 48ms/step - loss: 1.0387 - accuracy: 0.4507 - auc: 0.6409 - f1_score: 0.4456 - val_loss: 1.0484 - val_accuracy: 0.4406 - val_auc: 0.6299 - val_f1_score: 0.4314
Epoch 11/50
188/188 [==============================] - 9s 48ms/step - loss: 1.0385 - accuracy: 0.4526 - auc: 0.6422 - f1_score: 0.4483 - val_loss: 1.0487 - val_accuracy: 0.4372 - val_auc: 0.6290 - val_f1_score: 0.4276
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>149</th>
      <td>0.440332</td>
      <td>0.433895</td>
      <td>0.476003</td>
      <td>0.474522</td>
      <td>0.664681</td>
      <td>0.662718</td>
      <td>0.476024</td>
      <td>0.474121</td>
      <td>9</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>150</th>
      <td>0.442646</td>
      <td>0.436500</td>
      <td>0.472623</td>
      <td>0.472111</td>
      <td>0.661681</td>
      <td>0.660480</td>
      <td>0.470172</td>
      <td>0.468912</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.429854</td>
      <td>0.423773</td>
      <td>0.443846</td>
      <td>0.445443</td>
      <td>0.634226</td>
      <td>0.634307</td>
      <td>0.434921</td>
      <td>0.435411</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.435427</td>
      <td>0.429323</td>
      <td>0.453635</td>
      <td>0.454440</td>
      <td>0.645088</td>
      <td>0.644512</td>
      <td>0.448329</td>
      <td>0.448198</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.437211</td>
      <td>0.431107</td>
      <td>0.458780</td>
      <td>0.459331</td>
      <td>0.651374</td>
      <td>0.650569</td>
      <td>0.448817</td>
      <td>0.448099</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>153 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 805)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          403000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 542,543
Trainable params: 542,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 14s 63ms/step - loss: 1.0745 - accuracy: 0.4040 - auc: 0.5860 - f1_score: 0.4275 - val_loss: 1.0575 - val_accuracy: 0.4321 - val_auc: 0.6160 - val_f1_score: 0.4169
Epoch 2/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0535 - accuracy: 0.4337 - auc: 0.6203 - f1_score: 0.4255 - val_loss: 1.0516 - val_accuracy: 0.4347 - val_auc: 0.6246 - val_f1_score: 0.4214
Epoch 3/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0485 - accuracy: 0.4382 - auc: 0.6276 - f1_score: 0.4313 - val_loss: 1.0497 - val_accuracy: 0.4398 - val_auc: 0.6264 - val_f1_score: 0.4347
Epoch 4/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0442 - accuracy: 0.4435 - auc: 0.6325 - f1_score: 0.4378 - val_loss: 1.0480 - val_accuracy: 0.4378 - val_auc: 0.6290 - val_f1_score: 0.4300
Epoch 5/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0418 - accuracy: 0.4462 - auc: 0.6362 - f1_score: 0.4404 - val_loss: 1.0475 - val_accuracy: 0.4386 - val_auc: 0.6296 - val_f1_score: 0.4352
Epoch 6/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0400 - accuracy: 0.4497 - auc: 0.6388 - f1_score: 0.4458 - val_loss: 1.0494 - val_accuracy: 0.4379 - val_auc: 0.6279 - val_f1_score: 0.4325
Epoch 7/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0389 - accuracy: 0.4523 - auc: 0.6407 - f1_score: 0.4487 - val_loss: 1.0473 - val_accuracy: 0.4399 - val_auc: 0.6299 - val_f1_score: 0.4359
Epoch 8/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0374 - accuracy: 0.4517 - auc: 0.6418 - f1_score: 0.4477 - val_loss: 1.0484 - val_accuracy: 0.4391 - val_auc: 0.6287 - val_f1_score: 0.4342
Epoch 9/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0356 - accuracy: 0.4548 - auc: 0.6440 - f1_score: 0.4509 - val_loss: 1.0484 - val_accuracy: 0.4423 - val_auc: 0.6289 - val_f1_score: 0.4400
Epoch 10/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0338 - accuracy: 0.4566 - auc: 0.6469 - f1_score: 0.4530 - val_loss: 1.0490 - val_accuracy: 0.4356 - val_auc: 0.6282 - val_f1_score: 0.4314
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>150</th>
      <td>0.442646</td>
      <td>0.436500</td>
      <td>0.472623</td>
      <td>0.472111</td>
      <td>0.661681</td>
      <td>0.660480</td>
      <td>0.470172</td>
      <td>0.468912</td>
      <td>10</td>
      <td>LSTM</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.429854</td>
      <td>0.423773</td>
      <td>0.443846</td>
      <td>0.445443</td>
      <td>0.634226</td>
      <td>0.634307</td>
      <td>0.434921</td>
      <td>0.435411</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.435427</td>
      <td>0.429323</td>
      <td>0.453635</td>
      <td>0.454440</td>
      <td>0.645088</td>
      <td>0.644512</td>
      <td>0.448329</td>
      <td>0.448198</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.437211</td>
      <td>0.431107</td>
      <td>0.458780</td>
      <td>0.459331</td>
      <td>0.651374</td>
      <td>0.650569</td>
      <td>0.448817</td>
      <td>0.448099</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.440201</td>
      <td>0.433845</td>
      <td>0.467282</td>
      <td>0.466662</td>
      <td>0.658935</td>
      <td>0.657230</td>
      <td>0.463241</td>
      <td>0.461745</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>154 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          503000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 642,543
Trainable params: 642,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 70ms/step - loss: 1.0724 - accuracy: 0.4097 - auc: 0.5895 - f1_score: 0.4351 - val_loss: 1.0573 - val_accuracy: 0.4233 - val_auc: 0.6162 - val_f1_score: 0.4190
Epoch 2/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0523 - accuracy: 0.4377 - auc: 0.6229 - f1_score: 0.4357 - val_loss: 1.0509 - val_accuracy: 0.4329 - val_auc: 0.6231 - val_f1_score: 0.4308
Epoch 3/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0452 - accuracy: 0.4466 - auc: 0.6329 - f1_score: 0.4435 - val_loss: 1.0491 - val_accuracy: 0.4381 - val_auc: 0.6262 - val_f1_score: 0.4386
Epoch 4/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0415 - accuracy: 0.4511 - auc: 0.6376 - f1_score: 0.4491 - val_loss: 1.0492 - val_accuracy: 0.4325 - val_auc: 0.6260 - val_f1_score: 0.4301
Epoch 5/50
188/188 [==============================] - 14s 73ms/step - loss: 1.0400 - accuracy: 0.4521 - auc: 0.6393 - f1_score: 0.4499 - val_loss: 1.0482 - val_accuracy: 0.4360 - val_auc: 0.6264 - val_f1_score: 0.4323
Epoch 6/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0382 - accuracy: 0.4539 - auc: 0.6422 - f1_score: 0.4512 - val_loss: 1.0472 - val_accuracy: 0.4376 - val_auc: 0.6294 - val_f1_score: 0.4356
Epoch 7/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0351 - accuracy: 0.4589 - auc: 0.6461 - f1_score: 0.4552 - val_loss: 1.0476 - val_accuracy: 0.4363 - val_auc: 0.6284 - val_f1_score: 0.4350
Epoch 8/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0341 - accuracy: 0.4604 - auc: 0.6472 - f1_score: 0.4575 - val_loss: 1.0481 - val_accuracy: 0.4394 - val_auc: 0.6285 - val_f1_score: 0.4379
Epoch 9/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0324 - accuracy: 0.4626 - auc: 0.6495 - f1_score: 0.4602 - val_loss: 1.0476 - val_accuracy: 0.4385 - val_auc: 0.6292 - val_f1_score: 0.4349
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>151</th>
      <td>0.429854</td>
      <td>0.423773</td>
      <td>0.443846</td>
      <td>0.445443</td>
      <td>0.634226</td>
      <td>0.634307</td>
      <td>0.434921</td>
      <td>0.435411</td>
      <td>1</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.435427</td>
      <td>0.429323</td>
      <td>0.453635</td>
      <td>0.454440</td>
      <td>0.645088</td>
      <td>0.644512</td>
      <td>0.448329</td>
      <td>0.448198</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.437211</td>
      <td>0.431107</td>
      <td>0.458780</td>
      <td>0.459331</td>
      <td>0.651374</td>
      <td>0.650569</td>
      <td>0.448817</td>
      <td>0.448099</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.440201</td>
      <td>0.433845</td>
      <td>0.467282</td>
      <td>0.466662</td>
      <td>0.658935</td>
      <td>0.657230</td>
      <td>0.463241</td>
      <td>0.461745</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>155</th>
      <td>0.440875</td>
      <td>0.434492</td>
      <td>0.475349</td>
      <td>0.474356</td>
      <td>0.662395</td>
      <td>0.660562</td>
      <td>0.471777</td>
      <td>0.469987</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>155 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1205)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          603000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 742,543
Trainable params: 742,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 76ms/step - loss: 1.0715 - accuracy: 0.4063 - auc: 0.5898 - f1_score: 0.4380 - val_loss: 1.0565 - val_accuracy: 0.4289 - val_auc: 0.6176 - val_f1_score: 0.4209
Epoch 2/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0521 - accuracy: 0.4348 - auc: 0.6219 - f1_score: 0.4275 - val_loss: 1.0516 - val_accuracy: 0.4361 - val_auc: 0.6237 - val_f1_score: 0.4332
Epoch 3/50
188/188 [==============================] - 13s 70ms/step - loss: 1.0457 - accuracy: 0.4414 - auc: 0.6313 - f1_score: 0.4364 - val_loss: 1.0496 - val_accuracy: 0.4394 - val_auc: 0.6264 - val_f1_score: 0.4273
Epoch 4/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0427 - accuracy: 0.4458 - auc: 0.6348 - f1_score: 0.4401 - val_loss: 1.0490 - val_accuracy: 0.4408 - val_auc: 0.6278 - val_f1_score: 0.4349
Epoch 5/50
188/188 [==============================] - 14s 74ms/step - loss: 1.0393 - accuracy: 0.4498 - auc: 0.6391 - f1_score: 0.4453 - val_loss: 1.0481 - val_accuracy: 0.4390 - val_auc: 0.6289 - val_f1_score: 0.4365
Epoch 6/50
188/188 [==============================] - 13s 71ms/step - loss: 1.0372 - accuracy: 0.4524 - auc: 0.6423 - f1_score: 0.4479 - val_loss: 1.0471 - val_accuracy: 0.4447 - val_auc: 0.6314 - val_f1_score: 0.4398
Epoch 7/50
188/188 [==============================] - 14s 72ms/step - loss: 1.0350 - accuracy: 0.4544 - auc: 0.6450 - f1_score: 0.4506 - val_loss: 1.0472 - val_accuracy: 0.4387 - val_auc: 0.6303 - val_f1_score: 0.4351
Epoch 8/50
188/188 [==============================] - 13s 70ms/step - loss: 1.0327 - accuracy: 0.4591 - auc: 0.6483 - f1_score: 0.4569 - val_loss: 1.0476 - val_accuracy: 0.4379 - val_auc: 0.6303 - val_f1_score: 0.4361
Epoch 9/50
188/188 [==============================] - 13s 70ms/step - loss: 1.0308 - accuracy: 0.4585 - auc: 0.6504 - f1_score: 0.4553 - val_loss: 1.0478 - val_accuracy: 0.4468 - val_auc: 0.6308 - val_f1_score: 0.4435
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>152</th>
      <td>0.435427</td>
      <td>0.429323</td>
      <td>0.453635</td>
      <td>0.454440</td>
      <td>0.645088</td>
      <td>0.644512</td>
      <td>0.448329</td>
      <td>0.448198</td>
      <td>2</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.437211</td>
      <td>0.431107</td>
      <td>0.458780</td>
      <td>0.459331</td>
      <td>0.651374</td>
      <td>0.650569</td>
      <td>0.448817</td>
      <td>0.448099</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.440201</td>
      <td>0.433845</td>
      <td>0.467282</td>
      <td>0.466662</td>
      <td>0.658935</td>
      <td>0.657230</td>
      <td>0.463241</td>
      <td>0.461745</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>155</th>
      <td>0.440875</td>
      <td>0.434492</td>
      <td>0.475349</td>
      <td>0.474356</td>
      <td>0.662395</td>
      <td>0.660562</td>
      <td>0.471777</td>
      <td>0.469987</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.442461</td>
      <td>0.435934</td>
      <td>0.478564</td>
      <td>0.476610</td>
      <td>0.667523</td>
      <td>0.664923</td>
      <td>0.475283</td>
      <td>0.472490</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>156 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1405)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          703000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 842,543
Trainable params: 842,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 20s 90ms/step - loss: 1.0770 - accuracy: 0.4060 - auc: 0.5822 - f1_score: 0.4430 - val_loss: 1.0588 - val_accuracy: 0.4307 - val_auc: 0.6150 - val_f1_score: 0.4268
Epoch 2/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0525 - accuracy: 0.4350 - auc: 0.6226 - f1_score: 0.4308 - val_loss: 1.0531 - val_accuracy: 0.4329 - val_auc: 0.6224 - val_f1_score: 0.4285
Epoch 3/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0461 - accuracy: 0.4411 - auc: 0.6306 - f1_score: 0.4368 - val_loss: 1.0509 - val_accuracy: 0.4347 - val_auc: 0.6257 - val_f1_score: 0.4285
Epoch 4/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0419 - accuracy: 0.4493 - auc: 0.6369 - f1_score: 0.4456 - val_loss: 1.0483 - val_accuracy: 0.4436 - val_auc: 0.6290 - val_f1_score: 0.4431
Epoch 5/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0386 - accuracy: 0.4516 - auc: 0.6408 - f1_score: 0.4480 - val_loss: 1.0490 - val_accuracy: 0.4378 - val_auc: 0.6282 - val_f1_score: 0.4318
Epoch 6/50
188/188 [==============================] - 16s 88ms/step - loss: 1.0361 - accuracy: 0.4558 - auc: 0.6443 - f1_score: 0.4521 - val_loss: 1.0479 - val_accuracy: 0.4326 - val_auc: 0.6275 - val_f1_score: 0.4253
Epoch 7/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0329 - accuracy: 0.4583 - auc: 0.6482 - f1_score: 0.4545 - val_loss: 1.0477 - val_accuracy: 0.4392 - val_auc: 0.6293 - val_f1_score: 0.4341
Epoch 8/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0310 - accuracy: 0.4634 - auc: 0.6512 - f1_score: 0.4605 - val_loss: 1.0477 - val_accuracy: 0.4438 - val_auc: 0.6306 - val_f1_score: 0.4374
Epoch 9/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0283 - accuracy: 0.4644 - auc: 0.6540 - f1_score: 0.4605 - val_loss: 1.0469 - val_accuracy: 0.4349 - val_auc: 0.6306 - val_f1_score: 0.4332
Epoch 10/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0263 - accuracy: 0.4666 - auc: 0.6561 - f1_score: 0.4633 - val_loss: 1.0487 - val_accuracy: 0.4397 - val_auc: 0.6300 - val_f1_score: 0.4343
Epoch 11/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0231 - accuracy: 0.4723 - auc: 0.6603 - f1_score: 0.4693 - val_loss: 1.0476 - val_accuracy: 0.4382 - val_auc: 0.6301 - val_f1_score: 0.4372
Epoch 12/50
188/188 [==============================] - 16s 87ms/step - loss: 1.0202 - accuracy: 0.4737 - auc: 0.6633 - f1_score: 0.4711 - val_loss: 1.0488 - val_accuracy: 0.4423 - val_auc: 0.6316 - val_f1_score: 0.4392
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>153</th>
      <td>0.437211</td>
      <td>0.431107</td>
      <td>0.458780</td>
      <td>0.459331</td>
      <td>0.651374</td>
      <td>0.650569</td>
      <td>0.448817</td>
      <td>0.448099</td>
      <td>3</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.440201</td>
      <td>0.433845</td>
      <td>0.467282</td>
      <td>0.466662</td>
      <td>0.658935</td>
      <td>0.657230</td>
      <td>0.463241</td>
      <td>0.461745</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>155</th>
      <td>0.440875</td>
      <td>0.434492</td>
      <td>0.475349</td>
      <td>0.474356</td>
      <td>0.662395</td>
      <td>0.660562</td>
      <td>0.471777</td>
      <td>0.469987</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.442461</td>
      <td>0.435934</td>
      <td>0.478564</td>
      <td>0.476610</td>
      <td>0.667523</td>
      <td>0.664923</td>
      <td>0.475283</td>
      <td>0.472490</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.451254</td>
      <td>0.444210</td>
      <td>0.496801</td>
      <td>0.493232</td>
      <td>0.683401</td>
      <td>0.679221</td>
      <td>0.494250</td>
      <td>0.490033</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>157 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1605)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          803000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 942,543
Trainable params: 942,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 21s 99ms/step - loss: 1.0682 - accuracy: 0.4142 - auc: 0.5974 - f1_score: 0.4515 - val_loss: 1.0564 - val_accuracy: 0.4258 - val_auc: 0.6149 - val_f1_score: 0.4048
Epoch 2/50
188/188 [==============================] - 18s 95ms/step - loss: 1.0489 - accuracy: 0.4375 - auc: 0.6261 - f1_score: 0.4225 - val_loss: 1.0517 - val_accuracy: 0.4387 - val_auc: 0.6243 - val_f1_score: 0.4270
Epoch 3/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0432 - accuracy: 0.4442 - auc: 0.6341 - f1_score: 0.4354 - val_loss: 1.0500 - val_accuracy: 0.4398 - val_auc: 0.6273 - val_f1_score: 0.4305
Epoch 4/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0386 - accuracy: 0.4503 - auc: 0.6401 - f1_score: 0.4421 - val_loss: 1.0482 - val_accuracy: 0.4438 - val_auc: 0.6299 - val_f1_score: 0.4405
Epoch 5/50
188/188 [==============================] - 18s 94ms/step - loss: 1.0353 - accuracy: 0.4536 - auc: 0.6449 - f1_score: 0.4483 - val_loss: 1.0473 - val_accuracy: 0.4436 - val_auc: 0.6301 - val_f1_score: 0.4416
Epoch 6/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0323 - accuracy: 0.4579 - auc: 0.6481 - f1_score: 0.4527 - val_loss: 1.0484 - val_accuracy: 0.4426 - val_auc: 0.6283 - val_f1_score: 0.4380
Epoch 7/50
188/188 [==============================] - 17s 91ms/step - loss: 1.0305 - accuracy: 0.4597 - auc: 0.6511 - f1_score: 0.4549 - val_loss: 1.0489 - val_accuracy: 0.4454 - val_auc: 0.6300 - val_f1_score: 0.4409
Epoch 8/50
188/188 [==============================] - 17s 92ms/step - loss: 1.0280 - accuracy: 0.4640 - auc: 0.6540 - f1_score: 0.4594 - val_loss: 1.0493 - val_accuracy: 0.4435 - val_auc: 0.6301 - val_f1_score: 0.4425
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>154</th>
      <td>0.440201</td>
      <td>0.433845</td>
      <td>0.467282</td>
      <td>0.466662</td>
      <td>0.658935</td>
      <td>0.657230</td>
      <td>0.463241</td>
      <td>0.461745</td>
      <td>4</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>155</th>
      <td>0.440875</td>
      <td>0.434492</td>
      <td>0.475349</td>
      <td>0.474356</td>
      <td>0.662395</td>
      <td>0.660562</td>
      <td>0.471777</td>
      <td>0.469987</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.442461</td>
      <td>0.435934</td>
      <td>0.478564</td>
      <td>0.476610</td>
      <td>0.667523</td>
      <td>0.664923</td>
      <td>0.475283</td>
      <td>0.472490</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.451254</td>
      <td>0.444210</td>
      <td>0.496801</td>
      <td>0.493232</td>
      <td>0.683401</td>
      <td>0.679221</td>
      <td>0.494250</td>
      <td>0.490033</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.445663</td>
      <td>0.439072</td>
      <td>0.478128</td>
      <td>0.476257</td>
      <td>0.670942</td>
      <td>0.668244</td>
      <td>0.476659</td>
      <td>0.474131</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>158 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1805)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          903000      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,042,543
Trainable params: 1,042,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 22s 100ms/step - loss: 1.0718 - accuracy: 0.4035 - auc: 0.5892 - f1_score: 0.4423 - val_loss: 1.0575 - val_accuracy: 0.4294 - val_auc: 0.6151 - val_f1_score: 0.4082
Epoch 2/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0508 - accuracy: 0.4354 - auc: 0.6238 - f1_score: 0.4294 - val_loss: 1.0526 - val_accuracy: 0.4384 - val_auc: 0.6245 - val_f1_score: 0.4177
Epoch 3/50
188/188 [==============================] - 19s 103ms/step - loss: 1.0452 - accuracy: 0.4433 - auc: 0.6318 - f1_score: 0.4386 - val_loss: 1.0493 - val_accuracy: 0.4414 - val_auc: 0.6276 - val_f1_score: 0.4359
Epoch 4/50
188/188 [==============================] - 20s 108ms/step - loss: 1.0408 - accuracy: 0.4476 - auc: 0.6367 - f1_score: 0.4436 - val_loss: 1.0482 - val_accuracy: 0.4385 - val_auc: 0.6295 - val_f1_score: 0.4349
Epoch 5/50
188/188 [==============================] - 19s 103ms/step - loss: 1.0372 - accuracy: 0.4540 - auc: 0.6431 - f1_score: 0.4504 - val_loss: 1.0470 - val_accuracy: 0.4465 - val_auc: 0.6310 - val_f1_score: 0.4395
Epoch 6/50
188/188 [==============================] - 18s 98ms/step - loss: 1.0337 - accuracy: 0.4544 - auc: 0.6464 - f1_score: 0.4498 - val_loss: 1.0474 - val_accuracy: 0.4447 - val_auc: 0.6308 - val_f1_score: 0.4360
Epoch 7/50
188/188 [==============================] - 19s 104ms/step - loss: 1.0306 - accuracy: 0.4626 - auc: 0.6509 - f1_score: 0.4596 - val_loss: 1.0479 - val_accuracy: 0.4376 - val_auc: 0.6297 - val_f1_score: 0.4349
Epoch 8/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0276 - accuracy: 0.4639 - auc: 0.6543 - f1_score: 0.4604 - val_loss: 1.0498 - val_accuracy: 0.4385 - val_auc: 0.6287 - val_f1_score: 0.4330
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>155</th>
      <td>0.440875</td>
      <td>0.434492</td>
      <td>0.475349</td>
      <td>0.474356</td>
      <td>0.662395</td>
      <td>0.660562</td>
      <td>0.471777</td>
      <td>0.469987</td>
      <td>5</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.442461</td>
      <td>0.435934</td>
      <td>0.478564</td>
      <td>0.476610</td>
      <td>0.667523</td>
      <td>0.664923</td>
      <td>0.475283</td>
      <td>0.472490</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.451254</td>
      <td>0.444210</td>
      <td>0.496801</td>
      <td>0.493232</td>
      <td>0.683401</td>
      <td>0.679221</td>
      <td>0.494250</td>
      <td>0.490033</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.445663</td>
      <td>0.439072</td>
      <td>0.478128</td>
      <td>0.476257</td>
      <td>0.670942</td>
      <td>0.668244</td>
      <td>0.476659</td>
      <td>0.474131</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.445754</td>
      <td>0.439133</td>
      <td>0.479687</td>
      <td>0.478423</td>
      <td>0.671539</td>
      <td>0.668756</td>
      <td>0.474430</td>
      <td>0.472257</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>159 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 5)
Test shape: (10292, 5)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 5)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2005)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1003000     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,142,543
Trainable params: 1,142,543
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 25s 116ms/step - loss: 1.0665 - accuracy: 0.4183 - auc: 0.5998 - f1_score: 0.4425 - val_loss: 1.0561 - val_accuracy: 0.4294 - val_auc: 0.6166 - val_f1_score: 0.4164
Epoch 2/50
188/188 [==============================] - 21s 112ms/step - loss: 1.0477 - accuracy: 0.4387 - auc: 0.6284 - f1_score: 0.4277 - val_loss: 1.0529 - val_accuracy: 0.4354 - val_auc: 0.6230 - val_f1_score: 0.4245
Epoch 3/50
188/188 [==============================] - 21s 111ms/step - loss: 1.0429 - accuracy: 0.4437 - auc: 0.6341 - f1_score: 0.4352 - val_loss: 1.0507 - val_accuracy: 0.4344 - val_auc: 0.6254 - val_f1_score: 0.4267
Epoch 4/50
188/188 [==============================] - 21s 111ms/step - loss: 1.0384 - accuracy: 0.4528 - auc: 0.6411 - f1_score: 0.4455 - val_loss: 1.0492 - val_accuracy: 0.4433 - val_auc: 0.6279 - val_f1_score: 0.4382
Epoch 5/50
188/188 [==============================] - 21s 113ms/step - loss: 1.0345 - accuracy: 0.4551 - auc: 0.6455 - f1_score: 0.4498 - val_loss: 1.0493 - val_accuracy: 0.4416 - val_auc: 0.6285 - val_f1_score: 0.4380
Epoch 6/50
188/188 [==============================] - 21s 112ms/step - loss: 1.0297 - accuracy: 0.4609 - auc: 0.6519 - f1_score: 0.4560 - val_loss: 1.0506 - val_accuracy: 0.4378 - val_auc: 0.6273 - val_f1_score: 0.4275
Epoch 7/50
188/188 [==============================] - 22s 119ms/step - loss: 1.0261 - accuracy: 0.4652 - auc: 0.6558 - f1_score: 0.4596 - val_loss: 1.0525 - val_accuracy: 0.4409 - val_auc: 0.6271 - val_f1_score: 0.4379
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>156</th>
      <td>0.442461</td>
      <td>0.435934</td>
      <td>0.478564</td>
      <td>0.476610</td>
      <td>0.667523</td>
      <td>0.664923</td>
      <td>0.475283</td>
      <td>0.472490</td>
      <td>6</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.451254</td>
      <td>0.444210</td>
      <td>0.496801</td>
      <td>0.493232</td>
      <td>0.683401</td>
      <td>0.679221</td>
      <td>0.494250</td>
      <td>0.490033</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.445663</td>
      <td>0.439072</td>
      <td>0.478128</td>
      <td>0.476257</td>
      <td>0.670942</td>
      <td>0.668244</td>
      <td>0.476659</td>
      <td>0.474131</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.445754</td>
      <td>0.439133</td>
      <td>0.479687</td>
      <td>0.478423</td>
      <td>0.671539</td>
      <td>0.668756</td>
      <td>0.474430</td>
      <td>0.472257</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.447580</td>
      <td>0.440781</td>
      <td>0.482924</td>
      <td>0.480628</td>
      <td>0.673020</td>
      <td>0.669882</td>
      <td>0.479771</td>
      <td>0.476747</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>160 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 1, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 1, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 208)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          104500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 308,243
Trainable params: 308,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 41ms/step - loss: 1.0815 - accuracy: 0.3735 - auc: 0.5607 - f1_score: 0.4216 - val_loss: 1.0754 - val_accuracy: 0.4255 - val_auc: 0.5937 - val_f1_score: 0.4141
Epoch 2/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0718 - accuracy: 0.4268 - auc: 0.5962 - f1_score: 0.4176 - val_loss: 1.0729 - val_accuracy: 0.4297 - val_auc: 0.6002 - val_f1_score: 0.4251
Epoch 3/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0679 - accuracy: 0.4361 - auc: 0.6061 - f1_score: 0.4273 - val_loss: 1.0688 - val_accuracy: 0.4257 - val_auc: 0.6065 - val_f1_score: 0.4088
Epoch 4/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0601 - accuracy: 0.4364 - auc: 0.6170 - f1_score: 0.4240 - val_loss: 1.0590 - val_accuracy: 0.4363 - val_auc: 0.6184 - val_f1_score: 0.4262
Epoch 5/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0523 - accuracy: 0.4366 - auc: 0.6241 - f1_score: 0.4244 - val_loss: 1.0549 - val_accuracy: 0.4336 - val_auc: 0.6205 - val_f1_score: 0.4194
Epoch 6/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0497 - accuracy: 0.4370 - auc: 0.6262 - f1_score: 0.4215 - val_loss: 1.0541 - val_accuracy: 0.4348 - val_auc: 0.6223 - val_f1_score: 0.4205
Epoch 7/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0493 - accuracy: 0.4397 - auc: 0.6265 - f1_score: 0.4241 - val_loss: 1.0536 - val_accuracy: 0.4336 - val_auc: 0.6230 - val_f1_score: 0.4223
Epoch 8/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0485 - accuracy: 0.4384 - auc: 0.6279 - f1_score: 0.4264 - val_loss: 1.0538 - val_accuracy: 0.4350 - val_auc: 0.6223 - val_f1_score: 0.4285
Epoch 9/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0480 - accuracy: 0.4395 - auc: 0.6283 - f1_score: 0.4287 - val_loss: 1.0536 - val_accuracy: 0.4341 - val_auc: 0.6227 - val_f1_score: 0.4193
Epoch 10/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0477 - accuracy: 0.4392 - auc: 0.6292 - f1_score: 0.4268 - val_loss: 1.0533 - val_accuracy: 0.4350 - val_auc: 0.6233 - val_f1_score: 0.4253
Epoch 11/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0474 - accuracy: 0.4420 - auc: 0.6297 - f1_score: 0.4301 - val_loss: 1.0537 - val_accuracy: 0.4338 - val_auc: 0.6222 - val_f1_score: 0.4261
Epoch 12/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0467 - accuracy: 0.4408 - auc: 0.6306 - f1_score: 0.4296 - val_loss: 1.0536 - val_accuracy: 0.4339 - val_auc: 0.6225 - val_f1_score: 0.4249
Epoch 13/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0470 - accuracy: 0.4394 - auc: 0.6296 - f1_score: 0.4263 - val_loss: 1.0531 - val_accuracy: 0.4350 - val_auc: 0.6240 - val_f1_score: 0.4263
Epoch 14/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0462 - accuracy: 0.4417 - auc: 0.6310 - f1_score: 0.4323 - val_loss: 1.0532 - val_accuracy: 0.4339 - val_auc: 0.6234 - val_f1_score: 0.4271
Epoch 15/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0456 - accuracy: 0.4426 - auc: 0.6321 - f1_score: 0.4321 - val_loss: 1.0533 - val_accuracy: 0.4362 - val_auc: 0.6232 - val_f1_score: 0.4265
Epoch 16/50
188/188 [==============================] - 7s 36ms/step - loss: 1.0453 - accuracy: 0.4432 - auc: 0.6326 - f1_score: 0.4342 - val_loss: 1.0534 - val_accuracy: 0.4348 - val_auc: 0.6230 - val_f1_score: 0.4262
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>157</th>
      <td>0.451254</td>
      <td>0.444210</td>
      <td>0.496801</td>
      <td>0.493232</td>
      <td>0.683401</td>
      <td>0.679221</td>
      <td>0.494250</td>
      <td>0.490033</td>
      <td>7</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.445663</td>
      <td>0.439072</td>
      <td>0.478128</td>
      <td>0.476257</td>
      <td>0.670942</td>
      <td>0.668244</td>
      <td>0.476659</td>
      <td>0.474131</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.445754</td>
      <td>0.439133</td>
      <td>0.479687</td>
      <td>0.478423</td>
      <td>0.671539</td>
      <td>0.668756</td>
      <td>0.474430</td>
      <td>0.472257</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.447580</td>
      <td>0.440781</td>
      <td>0.482924</td>
      <td>0.480628</td>
      <td>0.673020</td>
      <td>0.669882</td>
      <td>0.479771</td>
      <td>0.476747</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.430316</td>
      <td>0.424255</td>
      <td>0.445634</td>
      <td>0.447266</td>
      <td>0.635239</td>
      <td>0.635285</td>
      <td>0.436354</td>
      <td>0.436744</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>161 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 2, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 2, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 408)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          204500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 408,243
Trainable params: 408,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 13s 56ms/step - loss: 1.0700 - accuracy: 0.4210 - auc: 0.5977 - f1_score: 0.4199 - val_loss: 1.0563 - val_accuracy: 0.4293 - val_auc: 0.6188 - val_f1_score: 0.4203
Epoch 2/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0511 - accuracy: 0.4407 - auc: 0.6258 - f1_score: 0.4324 - val_loss: 1.0511 - val_accuracy: 0.4354 - val_auc: 0.6254 - val_f1_score: 0.4281
Epoch 3/50
188/188 [==============================] - 11s 59ms/step - loss: 1.0469 - accuracy: 0.4422 - auc: 0.6303 - f1_score: 0.4358 - val_loss: 1.0506 - val_accuracy: 0.4353 - val_auc: 0.6261 - val_f1_score: 0.4282
Epoch 4/50
188/188 [==============================] - 11s 58ms/step - loss: 1.0463 - accuracy: 0.4411 - auc: 0.6305 - f1_score: 0.4338 - val_loss: 1.0509 - val_accuracy: 0.4372 - val_auc: 0.6256 - val_f1_score: 0.4337
Epoch 5/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0451 - accuracy: 0.4431 - auc: 0.6321 - f1_score: 0.4385 - val_loss: 1.0506 - val_accuracy: 0.4365 - val_auc: 0.6266 - val_f1_score: 0.4263
Epoch 6/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0448 - accuracy: 0.4454 - auc: 0.6332 - f1_score: 0.4398 - val_loss: 1.0503 - val_accuracy: 0.4366 - val_auc: 0.6264 - val_f1_score: 0.4309
Epoch 7/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0434 - accuracy: 0.4486 - auc: 0.6352 - f1_score: 0.4439 - val_loss: 1.0499 - val_accuracy: 0.4393 - val_auc: 0.6275 - val_f1_score: 0.4355
Epoch 8/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0433 - accuracy: 0.4464 - auc: 0.6352 - f1_score: 0.4418 - val_loss: 1.0493 - val_accuracy: 0.4378 - val_auc: 0.6284 - val_f1_score: 0.4378
Epoch 9/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0426 - accuracy: 0.4466 - auc: 0.6360 - f1_score: 0.4435 - val_loss: 1.0496 - val_accuracy: 0.4399 - val_auc: 0.6280 - val_f1_score: 0.4378
Epoch 10/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0417 - accuracy: 0.4482 - auc: 0.6375 - f1_score: 0.4446 - val_loss: 1.0499 - val_accuracy: 0.4369 - val_auc: 0.6276 - val_f1_score: 0.4358
Epoch 11/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0420 - accuracy: 0.4480 - auc: 0.6368 - f1_score: 0.4460 - val_loss: 1.0489 - val_accuracy: 0.4423 - val_auc: 0.6293 - val_f1_score: 0.4396
Epoch 12/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0410 - accuracy: 0.4509 - auc: 0.6385 - f1_score: 0.4475 - val_loss: 1.0493 - val_accuracy: 0.4398 - val_auc: 0.6282 - val_f1_score: 0.4372
Epoch 13/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0412 - accuracy: 0.4481 - auc: 0.6380 - f1_score: 0.4447 - val_loss: 1.0491 - val_accuracy: 0.4363 - val_auc: 0.6287 - val_f1_score: 0.4339
Epoch 14/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0404 - accuracy: 0.4506 - auc: 0.6390 - f1_score: 0.4472 - val_loss: 1.0493 - val_accuracy: 0.4381 - val_auc: 0.6287 - val_f1_score: 0.4341
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>158</th>
      <td>0.445663</td>
      <td>0.439072</td>
      <td>0.478128</td>
      <td>0.476257</td>
      <td>0.670942</td>
      <td>0.668244</td>
      <td>0.476659</td>
      <td>0.474131</td>
      <td>8</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.445754</td>
      <td>0.439133</td>
      <td>0.479687</td>
      <td>0.478423</td>
      <td>0.671539</td>
      <td>0.668756</td>
      <td>0.474430</td>
      <td>0.472257</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.447580</td>
      <td>0.440781</td>
      <td>0.482924</td>
      <td>0.480628</td>
      <td>0.673020</td>
      <td>0.669882</td>
      <td>0.479771</td>
      <td>0.476747</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.430316</td>
      <td>0.424255</td>
      <td>0.445634</td>
      <td>0.447266</td>
      <td>0.635239</td>
      <td>0.635285</td>
      <td>0.436354</td>
      <td>0.436744</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.434370</td>
      <td>0.428352</td>
      <td>0.454071</td>
      <td>0.455009</td>
      <td>0.643919</td>
      <td>0.643636</td>
      <td>0.450414</td>
      <td>0.450479</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>162 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 3, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 3, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 608)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          304500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 508,243
Trainable params: 508,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 20s 89ms/step - loss: 1.0653 - accuracy: 0.4126 - auc: 0.6048 - f1_score: 0.4329 - val_loss: 1.0503 - val_accuracy: 0.4333 - val_auc: 0.6271 - val_f1_score: 0.4087
Epoch 2/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0476 - accuracy: 0.4391 - auc: 0.6291 - f1_score: 0.4173 - val_loss: 1.0482 - val_accuracy: 0.4324 - val_auc: 0.6294 - val_f1_score: 0.4081
Epoch 3/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0448 - accuracy: 0.4435 - auc: 0.6323 - f1_score: 0.4264 - val_loss: 1.0483 - val_accuracy: 0.4342 - val_auc: 0.6294 - val_f1_score: 0.4222
Epoch 4/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0444 - accuracy: 0.4428 - auc: 0.6337 - f1_score: 0.4299 - val_loss: 1.0477 - val_accuracy: 0.4339 - val_auc: 0.6304 - val_f1_score: 0.4264
Epoch 5/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0428 - accuracy: 0.4465 - auc: 0.6352 - f1_score: 0.4384 - val_loss: 1.0481 - val_accuracy: 0.4351 - val_auc: 0.6300 - val_f1_score: 0.4271
Epoch 6/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0426 - accuracy: 0.4479 - auc: 0.6364 - f1_score: 0.4391 - val_loss: 1.0476 - val_accuracy: 0.4363 - val_auc: 0.6311 - val_f1_score: 0.4234
Epoch 7/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0412 - accuracy: 0.4465 - auc: 0.6376 - f1_score: 0.4392 - val_loss: 1.0470 - val_accuracy: 0.4363 - val_auc: 0.6313 - val_f1_score: 0.4290
Epoch 8/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0401 - accuracy: 0.4500 - auc: 0.6391 - f1_score: 0.4432 - val_loss: 1.0472 - val_accuracy: 0.4337 - val_auc: 0.6310 - val_f1_score: 0.4291
Epoch 9/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0400 - accuracy: 0.4504 - auc: 0.6392 - f1_score: 0.4452 - val_loss: 1.0481 - val_accuracy: 0.4338 - val_auc: 0.6309 - val_f1_score: 0.4237
Epoch 10/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0392 - accuracy: 0.4494 - auc: 0.6400 - f1_score: 0.4419 - val_loss: 1.0474 - val_accuracy: 0.4354 - val_auc: 0.6314 - val_f1_score: 0.4305
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>159</th>
      <td>0.445754</td>
      <td>0.439133</td>
      <td>0.479687</td>
      <td>0.478423</td>
      <td>0.671539</td>
      <td>0.668756</td>
      <td>0.474430</td>
      <td>0.472257</td>
      <td>9</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.447580</td>
      <td>0.440781</td>
      <td>0.482924</td>
      <td>0.480628</td>
      <td>0.673020</td>
      <td>0.669882</td>
      <td>0.479771</td>
      <td>0.476747</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.430316</td>
      <td>0.424255</td>
      <td>0.445634</td>
      <td>0.447266</td>
      <td>0.635239</td>
      <td>0.635285</td>
      <td>0.436354</td>
      <td>0.436744</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.434370</td>
      <td>0.428352</td>
      <td>0.454071</td>
      <td>0.455009</td>
      <td>0.643919</td>
      <td>0.643636</td>
      <td>0.450414</td>
      <td>0.450479</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.436295</td>
      <td>0.430374</td>
      <td>0.454550</td>
      <td>0.455891</td>
      <td>0.645891</td>
      <td>0.645874</td>
      <td>0.449897</td>
      <td>0.450247</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>163 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 4, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 4, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 808)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          404500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 608,243
Trainable params: 608,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 24s 111ms/step - loss: 1.0669 - accuracy: 0.4240 - auc: 0.6000 - f1_score: 0.4190 - val_loss: 1.0528 - val_accuracy: 0.4314 - val_auc: 0.6229 - val_f1_score: 0.3462
Epoch 2/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0483 - accuracy: 0.4382 - auc: 0.6276 - f1_score: 0.3770 - val_loss: 1.0499 - val_accuracy: 0.4327 - val_auc: 0.6258 - val_f1_score: 0.3922
Epoch 3/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0447 - accuracy: 0.4424 - auc: 0.6323 - f1_score: 0.4137 - val_loss: 1.0490 - val_accuracy: 0.4350 - val_auc: 0.6270 - val_f1_score: 0.4125
Epoch 4/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0432 - accuracy: 0.4440 - auc: 0.6347 - f1_score: 0.4267 - val_loss: 1.0476 - val_accuracy: 0.4375 - val_auc: 0.6297 - val_f1_score: 0.4272
Epoch 5/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0399 - accuracy: 0.4498 - auc: 0.6389 - f1_score: 0.4399 - val_loss: 1.0473 - val_accuracy: 0.4381 - val_auc: 0.6299 - val_f1_score: 0.4278
Epoch 6/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0396 - accuracy: 0.4482 - auc: 0.6386 - f1_score: 0.4396 - val_loss: 1.0471 - val_accuracy: 0.4385 - val_auc: 0.6299 - val_f1_score: 0.4327
Epoch 7/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0392 - accuracy: 0.4516 - auc: 0.6403 - f1_score: 0.4451 - val_loss: 1.0477 - val_accuracy: 0.4391 - val_auc: 0.6296 - val_f1_score: 0.4326
Epoch 8/50
188/188 [==============================] - 19s 104ms/step - loss: 1.0382 - accuracy: 0.4505 - auc: 0.6413 - f1_score: 0.4443 - val_loss: 1.0473 - val_accuracy: 0.4371 - val_auc: 0.6299 - val_f1_score: 0.4322
Epoch 9/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0374 - accuracy: 0.4521 - auc: 0.6424 - f1_score: 0.4467 - val_loss: 1.0468 - val_accuracy: 0.4390 - val_auc: 0.6306 - val_f1_score: 0.4341
Epoch 10/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0367 - accuracy: 0.4536 - auc: 0.6431 - f1_score: 0.4495 - val_loss: 1.0481 - val_accuracy: 0.4372 - val_auc: 0.6304 - val_f1_score: 0.4275
Epoch 11/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0364 - accuracy: 0.4541 - auc: 0.6439 - f1_score: 0.4493 - val_loss: 1.0479 - val_accuracy: 0.4366 - val_auc: 0.6297 - val_f1_score: 0.4328
Epoch 12/50
188/188 [==============================] - 22s 118ms/step - loss: 1.0362 - accuracy: 0.4543 - auc: 0.6442 - f1_score: 0.4510 - val_loss: 1.0470 - val_accuracy: 0.4391 - val_auc: 0.6309 - val_f1_score: 0.4365
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>160</th>
      <td>0.447580</td>
      <td>0.440781</td>
      <td>0.482924</td>
      <td>0.480628</td>
      <td>0.673020</td>
      <td>0.669882</td>
      <td>0.479771</td>
      <td>0.476747</td>
      <td>10</td>
      <td>Simple</td>
      <td>5</td>
      <td>7</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.430316</td>
      <td>0.424255</td>
      <td>0.445634</td>
      <td>0.447266</td>
      <td>0.635239</td>
      <td>0.635285</td>
      <td>0.436354</td>
      <td>0.436744</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.434370</td>
      <td>0.428352</td>
      <td>0.454071</td>
      <td>0.455009</td>
      <td>0.643919</td>
      <td>0.643636</td>
      <td>0.450414</td>
      <td>0.450479</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.436295</td>
      <td>0.430374</td>
      <td>0.454550</td>
      <td>0.455891</td>
      <td>0.645891</td>
      <td>0.645874</td>
      <td>0.449897</td>
      <td>0.450247</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.437214</td>
      <td>0.431174</td>
      <td>0.460949</td>
      <td>0.461379</td>
      <td>0.650851</td>
      <td>0.650392</td>
      <td>0.458120</td>
      <td>0.457724</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>164 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 5, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 5, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          504500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 708,243
Trainable params: 708,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 27s 121ms/step - loss: 1.0732 - accuracy: 0.4215 - auc: 0.5862 - f1_score: 0.4253 - val_loss: 1.0646 - val_accuracy: 0.4357 - val_auc: 0.6042 - val_f1_score: 0.3984
Epoch 2/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0591 - accuracy: 0.4427 - auc: 0.6158 - f1_score: 0.4278 - val_loss: 1.0578 - val_accuracy: 0.4380 - val_auc: 0.6176 - val_f1_score: 0.4361
Epoch 3/50
188/188 [==============================] - 21s 114ms/step - loss: 1.0515 - accuracy: 0.4451 - auc: 0.6270 - f1_score: 0.4452 - val_loss: 1.0488 - val_accuracy: 0.4337 - val_auc: 0.6275 - val_f1_score: 0.4343
Epoch 4/50
188/188 [==============================] - 22s 115ms/step - loss: 1.0434 - accuracy: 0.4486 - auc: 0.6358 - f1_score: 0.4485 - val_loss: 1.0465 - val_accuracy: 0.4428 - val_auc: 0.6312 - val_f1_score: 0.4396
Epoch 5/50
188/188 [==============================] - 22s 116ms/step - loss: 1.0410 - accuracy: 0.4508 - auc: 0.6385 - f1_score: 0.4484 - val_loss: 1.0464 - val_accuracy: 0.4393 - val_auc: 0.6307 - val_f1_score: 0.4372
Epoch 6/50
188/188 [==============================] - 23s 122ms/step - loss: 1.0388 - accuracy: 0.4529 - auc: 0.6410 - f1_score: 0.4503 - val_loss: 1.0460 - val_accuracy: 0.4417 - val_auc: 0.6314 - val_f1_score: 0.4398
Epoch 7/50
188/188 [==============================] - 22s 119ms/step - loss: 1.0380 - accuracy: 0.4537 - auc: 0.6422 - f1_score: 0.4508 - val_loss: 1.0456 - val_accuracy: 0.4385 - val_auc: 0.6317 - val_f1_score: 0.4354
Epoch 8/50
188/188 [==============================] - 23s 121ms/step - loss: 1.0371 - accuracy: 0.4544 - auc: 0.6433 - f1_score: 0.4514 - val_loss: 1.0460 - val_accuracy: 0.4374 - val_auc: 0.6309 - val_f1_score: 0.4360
Epoch 9/50
188/188 [==============================] - 22s 115ms/step - loss: 1.0359 - accuracy: 0.4566 - auc: 0.6451 - f1_score: 0.4547 - val_loss: 1.0461 - val_accuracy: 0.4391 - val_auc: 0.6311 - val_f1_score: 0.4387
Epoch 10/50
188/188 [==============================] - 22s 117ms/step - loss: 1.0346 - accuracy: 0.4563 - auc: 0.6463 - f1_score: 0.4536 - val_loss: 1.0463 - val_accuracy: 0.4362 - val_auc: 0.6311 - val_f1_score: 0.4350
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>161</th>
      <td>0.430316</td>
      <td>0.424255</td>
      <td>0.445634</td>
      <td>0.447266</td>
      <td>0.635239</td>
      <td>0.635285</td>
      <td>0.436354</td>
      <td>0.436744</td>
      <td>1</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.434370</td>
      <td>0.428352</td>
      <td>0.454071</td>
      <td>0.455009</td>
      <td>0.643919</td>
      <td>0.643636</td>
      <td>0.450414</td>
      <td>0.450479</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.436295</td>
      <td>0.430374</td>
      <td>0.454550</td>
      <td>0.455891</td>
      <td>0.645891</td>
      <td>0.645874</td>
      <td>0.449897</td>
      <td>0.450247</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.437214</td>
      <td>0.431174</td>
      <td>0.460949</td>
      <td>0.461379</td>
      <td>0.650851</td>
      <td>0.650392</td>
      <td>0.458120</td>
      <td>0.457724</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.436993</td>
      <td>0.430871</td>
      <td>0.461440</td>
      <td>0.461624</td>
      <td>0.652264</td>
      <td>0.651544</td>
      <td>0.459997</td>
      <td>0.459480</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>165 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 6, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 6, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1208)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          604500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 808,243
Trainable params: 808,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 31s 146ms/step - loss: 1.0636 - accuracy: 0.4287 - auc: 0.6093 - f1_score: 0.4371 - val_loss: 1.0484 - val_accuracy: 0.4421 - val_auc: 0.6290 - val_f1_score: 0.4253
Epoch 2/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0428 - accuracy: 0.4458 - auc: 0.6349 - f1_score: 0.4330 - val_loss: 1.0459 - val_accuracy: 0.4398 - val_auc: 0.6315 - val_f1_score: 0.4243
Epoch 3/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0410 - accuracy: 0.4470 - auc: 0.6370 - f1_score: 0.4359 - val_loss: 1.0452 - val_accuracy: 0.4434 - val_auc: 0.6329 - val_f1_score: 0.4307
Epoch 4/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0387 - accuracy: 0.4513 - auc: 0.6403 - f1_score: 0.4400 - val_loss: 1.0446 - val_accuracy: 0.4454 - val_auc: 0.6338 - val_f1_score: 0.4376
Epoch 5/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0383 - accuracy: 0.4520 - auc: 0.6414 - f1_score: 0.4445 - val_loss: 1.0451 - val_accuracy: 0.4445 - val_auc: 0.6335 - val_f1_score: 0.4359
Epoch 6/50
188/188 [==============================] - 26s 139ms/step - loss: 1.0370 - accuracy: 0.4524 - auc: 0.6427 - f1_score: 0.4450 - val_loss: 1.0441 - val_accuracy: 0.4444 - val_auc: 0.6349 - val_f1_score: 0.4410
Epoch 7/50
188/188 [==============================] - 27s 144ms/step - loss: 1.0359 - accuracy: 0.4557 - auc: 0.6443 - f1_score: 0.4491 - val_loss: 1.0440 - val_accuracy: 0.4444 - val_auc: 0.6350 - val_f1_score: 0.4361
Epoch 8/50
188/188 [==============================] - 27s 142ms/step - loss: 1.0344 - accuracy: 0.4555 - auc: 0.6457 - f1_score: 0.4476 - val_loss: 1.0450 - val_accuracy: 0.4448 - val_auc: 0.6339 - val_f1_score: 0.4385
Epoch 9/50
188/188 [==============================] - 26s 139ms/step - loss: 1.0332 - accuracy: 0.4581 - auc: 0.6479 - f1_score: 0.4535 - val_loss: 1.0446 - val_accuracy: 0.4417 - val_auc: 0.6336 - val_f1_score: 0.4381
Epoch 10/50
188/188 [==============================] - 26s 138ms/step - loss: 1.0322 - accuracy: 0.4592 - auc: 0.6490 - f1_score: 0.4549 - val_loss: 1.0452 - val_accuracy: 0.4409 - val_auc: 0.6335 - val_f1_score: 0.4351
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>162</th>
      <td>0.434370</td>
      <td>0.428352</td>
      <td>0.454071</td>
      <td>0.455009</td>
      <td>0.643919</td>
      <td>0.643636</td>
      <td>0.450414</td>
      <td>0.450479</td>
      <td>2</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.436295</td>
      <td>0.430374</td>
      <td>0.454550</td>
      <td>0.455891</td>
      <td>0.645891</td>
      <td>0.645874</td>
      <td>0.449897</td>
      <td>0.450247</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.437214</td>
      <td>0.431174</td>
      <td>0.460949</td>
      <td>0.461379</td>
      <td>0.650851</td>
      <td>0.650392</td>
      <td>0.458120</td>
      <td>0.457724</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.436993</td>
      <td>0.430871</td>
      <td>0.461440</td>
      <td>0.461624</td>
      <td>0.652264</td>
      <td>0.651544</td>
      <td>0.459997</td>
      <td>0.459480</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.438791</td>
      <td>0.432752</td>
      <td>0.464612</td>
      <td>0.465339</td>
      <td>0.654390</td>
      <td>0.653797</td>
      <td>0.459289</td>
      <td>0.459105</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>166 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 7, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 7, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1408)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          704500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 908,243
Trainable params: 908,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 35s 161ms/step - loss: 1.0637 - accuracy: 0.4211 - auc: 0.6056 - f1_score: 0.4389 - val_loss: 1.0492 - val_accuracy: 0.4386 - val_auc: 0.6281 - val_f1_score: 0.4204
Epoch 2/50
188/188 [==============================] - 29s 156ms/step - loss: 1.0433 - accuracy: 0.4466 - auc: 0.6351 - f1_score: 0.4339 - val_loss: 1.0457 - val_accuracy: 0.4456 - val_auc: 0.6326 - val_f1_score: 0.4379
Epoch 3/50
188/188 [==============================] - 28s 150ms/step - loss: 1.0400 - accuracy: 0.4498 - auc: 0.6391 - f1_score: 0.4442 - val_loss: 1.0460 - val_accuracy: 0.4408 - val_auc: 0.6328 - val_f1_score: 0.4261
Epoch 4/50
188/188 [==============================] - 29s 152ms/step - loss: 1.0377 - accuracy: 0.4543 - auc: 0.6424 - f1_score: 0.4482 - val_loss: 1.0447 - val_accuracy: 0.4470 - val_auc: 0.6342 - val_f1_score: 0.4433
Epoch 5/50
188/188 [==============================] - 29s 152ms/step - loss: 1.0360 - accuracy: 0.4562 - auc: 0.6446 - f1_score: 0.4515 - val_loss: 1.0446 - val_accuracy: 0.4465 - val_auc: 0.6348 - val_f1_score: 0.4426
Epoch 6/50
188/188 [==============================] - 30s 158ms/step - loss: 1.0344 - accuracy: 0.4567 - auc: 0.6463 - f1_score: 0.4536 - val_loss: 1.0445 - val_accuracy: 0.4466 - val_auc: 0.6350 - val_f1_score: 0.4409
Epoch 7/50
188/188 [==============================] - 29s 156ms/step - loss: 1.0332 - accuracy: 0.4604 - auc: 0.6485 - f1_score: 0.4571 - val_loss: 1.0443 - val_accuracy: 0.4477 - val_auc: 0.6352 - val_f1_score: 0.4445
Epoch 8/50
188/188 [==============================] - 30s 158ms/step - loss: 1.0324 - accuracy: 0.4608 - auc: 0.6494 - f1_score: 0.4579 - val_loss: 1.0438 - val_accuracy: 0.4451 - val_auc: 0.6356 - val_f1_score: 0.4403
Epoch 9/50
188/188 [==============================] - 29s 157ms/step - loss: 1.0319 - accuracy: 0.4617 - auc: 0.6497 - f1_score: 0.4588 - val_loss: 1.0443 - val_accuracy: 0.4486 - val_auc: 0.6360 - val_f1_score: 0.4431
Epoch 10/50
188/188 [==============================] - 29s 156ms/step - loss: 1.0294 - accuracy: 0.4647 - auc: 0.6533 - f1_score: 0.4610 - val_loss: 1.0446 - val_accuracy: 0.4484 - val_auc: 0.6351 - val_f1_score: 0.4495
Epoch 11/50
188/188 [==============================] - 29s 154ms/step - loss: 1.0286 - accuracy: 0.4663 - auc: 0.6542 - f1_score: 0.4639 - val_loss: 1.0452 - val_accuracy: 0.4434 - val_auc: 0.6340 - val_f1_score: 0.4403
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>163</th>
      <td>0.436295</td>
      <td>0.430374</td>
      <td>0.454550</td>
      <td>0.455891</td>
      <td>0.645891</td>
      <td>0.645874</td>
      <td>0.449897</td>
      <td>0.450247</td>
      <td>3</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.437214</td>
      <td>0.431174</td>
      <td>0.460949</td>
      <td>0.461379</td>
      <td>0.650851</td>
      <td>0.650392</td>
      <td>0.458120</td>
      <td>0.457724</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.436993</td>
      <td>0.430871</td>
      <td>0.461440</td>
      <td>0.461624</td>
      <td>0.652264</td>
      <td>0.651544</td>
      <td>0.459997</td>
      <td>0.459480</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.438791</td>
      <td>0.432752</td>
      <td>0.464612</td>
      <td>0.465339</td>
      <td>0.654390</td>
      <td>0.653797</td>
      <td>0.459289</td>
      <td>0.459105</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.442015</td>
      <td>0.435781</td>
      <td>0.476711</td>
      <td>0.475503</td>
      <td>0.663594</td>
      <td>0.662060</td>
      <td>0.473321</td>
      <td>0.471299</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>167 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 8, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 8, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1608)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          804500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,008,243
Trainable params: 1,008,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 41s 184ms/step - loss: 1.0650 - accuracy: 0.4128 - auc: 0.6021 - f1_score: 0.4376 - val_loss: 1.0488 - val_accuracy: 0.4356 - val_auc: 0.6281 - val_f1_score: 0.4075
Epoch 2/50
188/188 [==============================] - 35s 187ms/step - loss: 1.0437 - accuracy: 0.4424 - auc: 0.6326 - f1_score: 0.4118 - val_loss: 1.0465 - val_accuracy: 0.4382 - val_auc: 0.6312 - val_f1_score: 0.4146
Epoch 3/50
188/188 [==============================] - 35s 184ms/step - loss: 1.0406 - accuracy: 0.4480 - auc: 0.6370 - f1_score: 0.4279 - val_loss: 1.0456 - val_accuracy: 0.4408 - val_auc: 0.6321 - val_f1_score: 0.4238
Epoch 4/50
188/188 [==============================] - 33s 178ms/step - loss: 1.0372 - accuracy: 0.4511 - auc: 0.6418 - f1_score: 0.4367 - val_loss: 1.0446 - val_accuracy: 0.4427 - val_auc: 0.6335 - val_f1_score: 0.4343
Epoch 5/50
188/188 [==============================] - 34s 180ms/step - loss: 1.0366 - accuracy: 0.4514 - auc: 0.6423 - f1_score: 0.4405 - val_loss: 1.0451 - val_accuracy: 0.4392 - val_auc: 0.6330 - val_f1_score: 0.4279
Epoch 6/50
188/188 [==============================] - 33s 177ms/step - loss: 1.0350 - accuracy: 0.4529 - auc: 0.6445 - f1_score: 0.4414 - val_loss: 1.0453 - val_accuracy: 0.4415 - val_auc: 0.6333 - val_f1_score: 0.4338
Epoch 7/50
188/188 [==============================] - 34s 179ms/step - loss: 1.0335 - accuracy: 0.4552 - auc: 0.6464 - f1_score: 0.4474 - val_loss: 1.0448 - val_accuracy: 0.4434 - val_auc: 0.6342 - val_f1_score: 0.4372
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>164</th>
      <td>0.437214</td>
      <td>0.431174</td>
      <td>0.460949</td>
      <td>0.461379</td>
      <td>0.650851</td>
      <td>0.650392</td>
      <td>0.458120</td>
      <td>0.457724</td>
      <td>4</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.436993</td>
      <td>0.430871</td>
      <td>0.461440</td>
      <td>0.461624</td>
      <td>0.652264</td>
      <td>0.651544</td>
      <td>0.459997</td>
      <td>0.459480</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.438791</td>
      <td>0.432752</td>
      <td>0.464612</td>
      <td>0.465339</td>
      <td>0.654390</td>
      <td>0.653797</td>
      <td>0.459289</td>
      <td>0.459105</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.442015</td>
      <td>0.435781</td>
      <td>0.476711</td>
      <td>0.475503</td>
      <td>0.663594</td>
      <td>0.662060</td>
      <td>0.473321</td>
      <td>0.471299</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.438048</td>
      <td>0.432084</td>
      <td>0.461331</td>
      <td>0.462369</td>
      <td>0.653282</td>
      <td>0.652929</td>
      <td>0.454237</td>
      <td>0.454276</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>168 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 9, 100)       42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 9, 100)       42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,108,243
Trainable params: 1,108,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 44s 209ms/step - loss: 1.0708 - accuracy: 0.4169 - auc: 0.5963 - f1_score: 0.4343 - val_loss: 1.0481 - val_accuracy: 0.4418 - val_auc: 0.6296 - val_f1_score: 0.4350
Epoch 2/50
188/188 [==============================] - 38s 203ms/step - loss: 1.0458 - accuracy: 0.4446 - auc: 0.6322 - f1_score: 0.4403 - val_loss: 1.0457 - val_accuracy: 0.4450 - val_auc: 0.6326 - val_f1_score: 0.4312
Epoch 3/50
188/188 [==============================] - 36s 194ms/step - loss: 1.0410 - accuracy: 0.4500 - auc: 0.6387 - f1_score: 0.4440 - val_loss: 1.0435 - val_accuracy: 0.4486 - val_auc: 0.6355 - val_f1_score: 0.4473
Epoch 4/50
188/188 [==============================] - 36s 193ms/step - loss: 1.0380 - accuracy: 0.4525 - auc: 0.6421 - f1_score: 0.4479 - val_loss: 1.0445 - val_accuracy: 0.4471 - val_auc: 0.6347 - val_f1_score: 0.4336
Epoch 5/50
188/188 [==============================] - 37s 196ms/step - loss: 1.0361 - accuracy: 0.4573 - auc: 0.6449 - f1_score: 0.4526 - val_loss: 1.0442 - val_accuracy: 0.4450 - val_auc: 0.6354 - val_f1_score: 0.4417
Epoch 6/50
188/188 [==============================] - 38s 201ms/step - loss: 1.0340 - accuracy: 0.4600 - auc: 0.6477 - f1_score: 0.4573 - val_loss: 1.0449 - val_accuracy: 0.4448 - val_auc: 0.6351 - val_f1_score: 0.4318
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>165</th>
      <td>0.436993</td>
      <td>0.430871</td>
      <td>0.461440</td>
      <td>0.461624</td>
      <td>0.652264</td>
      <td>0.651544</td>
      <td>0.459997</td>
      <td>0.459480</td>
      <td>5</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.438791</td>
      <td>0.432752</td>
      <td>0.464612</td>
      <td>0.465339</td>
      <td>0.654390</td>
      <td>0.653797</td>
      <td>0.459289</td>
      <td>0.459105</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.442015</td>
      <td>0.435781</td>
      <td>0.476711</td>
      <td>0.475503</td>
      <td>0.663594</td>
      <td>0.662060</td>
      <td>0.473321</td>
      <td>0.471299</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.438048</td>
      <td>0.432084</td>
      <td>0.461331</td>
      <td>0.462369</td>
      <td>0.653282</td>
      <td>0.652929</td>
      <td>0.454237</td>
      <td>0.454276</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.441689</td>
      <td>0.435859</td>
      <td>0.467380</td>
      <td>0.468720</td>
      <td>0.656002</td>
      <td>0.655920</td>
      <td>0.454019</td>
      <td>0.454023</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>169 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 lstm (LSTM)                    (None, 10, 100)      42800       ['input_1[0][0]']                
                                                                                                  
 lstm_1 (LSTM)                  (None, 10, 100)      42800       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['lstm[0][0]']                   
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['lstm_1[0][0]']                 
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,208,243
Trainable params: 1,208,243
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 48s 226ms/step - loss: 1.0618 - accuracy: 0.4230 - auc: 0.6053 - f1_score: 0.4403 - val_loss: 1.0506 - val_accuracy: 0.4339 - val_auc: 0.6232 - val_f1_score: 0.3898
Epoch 2/50
188/188 [==============================] - 41s 219ms/step - loss: 1.0452 - accuracy: 0.4411 - auc: 0.6292 - f1_score: 0.4002 - val_loss: 1.0473 - val_accuracy: 0.4348 - val_auc: 0.6277 - val_f1_score: 0.3796
Epoch 3/50
188/188 [==============================] - 43s 230ms/step - loss: 1.0411 - accuracy: 0.4458 - auc: 0.6357 - f1_score: 0.4045 - val_loss: 1.0472 - val_accuracy: 0.4371 - val_auc: 0.6303 - val_f1_score: 0.3921
Epoch 4/50
188/188 [==============================] - 42s 225ms/step - loss: 1.0390 - accuracy: 0.4482 - auc: 0.6389 - f1_score: 0.4141 - val_loss: 1.0456 - val_accuracy: 0.4400 - val_auc: 0.6328 - val_f1_score: 0.4087
Epoch 5/50
188/188 [==============================] - 42s 223ms/step - loss: 1.0369 - accuracy: 0.4486 - auc: 0.6416 - f1_score: 0.4195 - val_loss: 1.0452 - val_accuracy: 0.4423 - val_auc: 0.6343 - val_f1_score: 0.4232
Epoch 6/50
188/188 [==============================] - 44s 233ms/step - loss: 1.0342 - accuracy: 0.4549 - auc: 0.6457 - f1_score: 0.4353 - val_loss: 1.0449 - val_accuracy: 0.4439 - val_auc: 0.6345 - val_f1_score: 0.4177
Epoch 7/50
188/188 [==============================] - 43s 227ms/step - loss: 1.0334 - accuracy: 0.4547 - auc: 0.6465 - f1_score: 0.4374 - val_loss: 1.0430 - val_accuracy: 0.4492 - val_auc: 0.6369 - val_f1_score: 0.4368
Epoch 8/50
188/188 [==============================] - 42s 226ms/step - loss: 1.0312 - accuracy: 0.4572 - auc: 0.6491 - f1_score: 0.4442 - val_loss: 1.0439 - val_accuracy: 0.4486 - val_auc: 0.6355 - val_f1_score: 0.4376
Epoch 9/50
188/188 [==============================] - 42s 223ms/step - loss: 1.0292 - accuracy: 0.4601 - auc: 0.6521 - f1_score: 0.4479 - val_loss: 1.0447 - val_accuracy: 0.4448 - val_auc: 0.6343 - val_f1_score: 0.4332
Epoch 10/50
188/188 [==============================] - 41s 220ms/step - loss: 1.0277 - accuracy: 0.4628 - auc: 0.6540 - f1_score: 0.4526 - val_loss: 1.0440 - val_accuracy: 0.4508 - val_auc: 0.6354 - val_f1_score: 0.4404
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>166</th>
      <td>0.438791</td>
      <td>0.432752</td>
      <td>0.464612</td>
      <td>0.465339</td>
      <td>0.654390</td>
      <td>0.653797</td>
      <td>0.459289</td>
      <td>0.459105</td>
      <td>6</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.442015</td>
      <td>0.435781</td>
      <td>0.476711</td>
      <td>0.475503</td>
      <td>0.663594</td>
      <td>0.662060</td>
      <td>0.473321</td>
      <td>0.471299</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.438048</td>
      <td>0.432084</td>
      <td>0.461331</td>
      <td>0.462369</td>
      <td>0.653282</td>
      <td>0.652929</td>
      <td>0.454237</td>
      <td>0.454276</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.441689</td>
      <td>0.435859</td>
      <td>0.467380</td>
      <td>0.468720</td>
      <td>0.656002</td>
      <td>0.655920</td>
      <td>0.454019</td>
      <td>0.454023</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>170</th>
      <td>0.441167</td>
      <td>0.434844</td>
      <td>0.474618</td>
      <td>0.474767</td>
      <td>0.665433</td>
      <td>0.663710</td>
      <td>0.462702</td>
      <td>0.461737</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>170 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 1, 7)
Test shape: (10292, 1, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 1, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 1, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 1, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 100)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 100)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 208)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          104500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 244,043
Trainable params: 244,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 10s 38ms/step - loss: 1.0742 - accuracy: 0.4020 - auc: 0.5869 - f1_score: 0.4394 - val_loss: 1.0573 - val_accuracy: 0.4288 - val_auc: 0.6185 - val_f1_score: 0.3862
Epoch 2/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0568 - accuracy: 0.4270 - auc: 0.6155 - f1_score: 0.4032 - val_loss: 1.0551 - val_accuracy: 0.4335 - val_auc: 0.6211 - val_f1_score: 0.3944
Epoch 3/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0526 - accuracy: 0.4301 - auc: 0.6212 - f1_score: 0.4108 - val_loss: 1.0541 - val_accuracy: 0.4339 - val_auc: 0.6226 - val_f1_score: 0.4044
Epoch 4/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0513 - accuracy: 0.4332 - auc: 0.6232 - f1_score: 0.4167 - val_loss: 1.0540 - val_accuracy: 0.4320 - val_auc: 0.6229 - val_f1_score: 0.4020
Epoch 5/50
188/188 [==============================] - 7s 38ms/step - loss: 1.0502 - accuracy: 0.4368 - auc: 0.6247 - f1_score: 0.4185 - val_loss: 1.0541 - val_accuracy: 0.4366 - val_auc: 0.6231 - val_f1_score: 0.4178
Epoch 6/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0493 - accuracy: 0.4371 - auc: 0.6261 - f1_score: 0.4226 - val_loss: 1.0541 - val_accuracy: 0.4357 - val_auc: 0.6233 - val_f1_score: 0.4076
Epoch 7/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0491 - accuracy: 0.4379 - auc: 0.6270 - f1_score: 0.4220 - val_loss: 1.0538 - val_accuracy: 0.4339 - val_auc: 0.6229 - val_f1_score: 0.4141
Epoch 8/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0478 - accuracy: 0.4389 - auc: 0.6282 - f1_score: 0.4247 - val_loss: 1.0549 - val_accuracy: 0.4345 - val_auc: 0.6219 - val_f1_score: 0.4163
Epoch 9/50
188/188 [==============================] - 7s 35ms/step - loss: 1.0473 - accuracy: 0.4418 - auc: 0.6297 - f1_score: 0.4296 - val_loss: 1.0538 - val_accuracy: 0.4342 - val_auc: 0.6229 - val_f1_score: 0.4184
Epoch 10/50
188/188 [==============================] - 7s 37ms/step - loss: 1.0467 - accuracy: 0.4420 - auc: 0.6301 - f1_score: 0.4306 - val_loss: 1.0536 - val_accuracy: 0.4369 - val_auc: 0.6237 - val_f1_score: 0.4234
Epoch 11/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0470 - accuracy: 0.4412 - auc: 0.6300 - f1_score: 0.4304 - val_loss: 1.0535 - val_accuracy: 0.4348 - val_auc: 0.6235 - val_f1_score: 0.4226
Epoch 12/50
188/188 [==============================] - 6s 33ms/step - loss: 1.0462 - accuracy: 0.4433 - auc: 0.6313 - f1_score: 0.4354 - val_loss: 1.0541 - val_accuracy: 0.4333 - val_auc: 0.6226 - val_f1_score: 0.4203
Epoch 13/50
188/188 [==============================] - 6s 34ms/step - loss: 1.0463 - accuracy: 0.4416 - auc: 0.6311 - f1_score: 0.4331 - val_loss: 1.0544 - val_accuracy: 0.4355 - val_auc: 0.6228 - val_f1_score: 0.4251
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>167</th>
      <td>0.442015</td>
      <td>0.435781</td>
      <td>0.476711</td>
      <td>0.475503</td>
      <td>0.663594</td>
      <td>0.662060</td>
      <td>0.473321</td>
      <td>0.471299</td>
      <td>7</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.438048</td>
      <td>0.432084</td>
      <td>0.461331</td>
      <td>0.462369</td>
      <td>0.653282</td>
      <td>0.652929</td>
      <td>0.454237</td>
      <td>0.454276</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.441689</td>
      <td>0.435859</td>
      <td>0.467380</td>
      <td>0.468720</td>
      <td>0.656002</td>
      <td>0.655920</td>
      <td>0.454019</td>
      <td>0.454023</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>170</th>
      <td>0.441167</td>
      <td>0.434844</td>
      <td>0.474618</td>
      <td>0.474767</td>
      <td>0.665433</td>
      <td>0.663710</td>
      <td>0.462702</td>
      <td>0.461737</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.432137</td>
      <td>0.426055</td>
      <td>0.448392</td>
      <td>0.449677</td>
      <td>0.637223</td>
      <td>0.637144</td>
      <td>0.437986</td>
      <td>0.438061</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>171 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 2, 7)
Test shape: (10292, 2, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 2, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 2, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 2, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 200)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 200)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 408)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          204500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 344,043
Trainable params: 344,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 12s 48ms/step - loss: 1.0876 - accuracy: 0.3592 - auc: 0.5468 - f1_score: 0.4010 - val_loss: 1.0774 - val_accuracy: 0.3838 - val_auc: 0.5811 - val_f1_score: 0.3205
Epoch 2/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0756 - accuracy: 0.3843 - auc: 0.5831 - f1_score: 0.3368 - val_loss: 1.0710 - val_accuracy: 0.3946 - val_auc: 0.5953 - val_f1_score: 0.3542
Epoch 3/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0698 - accuracy: 0.3957 - auc: 0.5964 - f1_score: 0.3693 - val_loss: 1.0682 - val_accuracy: 0.3968 - val_auc: 0.6020 - val_f1_score: 0.3760
Epoch 4/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0662 - accuracy: 0.4032 - auc: 0.6045 - f1_score: 0.3930 - val_loss: 1.0652 - val_accuracy: 0.3984 - val_auc: 0.6044 - val_f1_score: 0.3861
Epoch 5/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0627 - accuracy: 0.4119 - auc: 0.6101 - f1_score: 0.4096 - val_loss: 1.0635 - val_accuracy: 0.4132 - val_auc: 0.6103 - val_f1_score: 0.4059
Epoch 6/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0586 - accuracy: 0.4258 - auc: 0.6171 - f1_score: 0.4139 - val_loss: 1.0590 - val_accuracy: 0.4354 - val_auc: 0.6167 - val_f1_score: 0.3969
Epoch 7/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0527 - accuracy: 0.4388 - auc: 0.6243 - f1_score: 0.4132 - val_loss: 1.0536 - val_accuracy: 0.4374 - val_auc: 0.6229 - val_f1_score: 0.4079
Epoch 8/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0476 - accuracy: 0.4411 - auc: 0.6295 - f1_score: 0.4158 - val_loss: 1.0517 - val_accuracy: 0.4398 - val_auc: 0.6251 - val_f1_score: 0.4276
Epoch 9/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0449 - accuracy: 0.4445 - auc: 0.6327 - f1_score: 0.4277 - val_loss: 1.0525 - val_accuracy: 0.4375 - val_auc: 0.6237 - val_f1_score: 0.4157
Epoch 10/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0439 - accuracy: 0.4457 - auc: 0.6345 - f1_score: 0.4297 - val_loss: 1.0509 - val_accuracy: 0.4415 - val_auc: 0.6261 - val_f1_score: 0.4288
Epoch 11/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0425 - accuracy: 0.4465 - auc: 0.6361 - f1_score: 0.4314 - val_loss: 1.0511 - val_accuracy: 0.4347 - val_auc: 0.6253 - val_f1_score: 0.4161
Epoch 12/50
188/188 [==============================] - 8s 45ms/step - loss: 1.0423 - accuracy: 0.4481 - auc: 0.6368 - f1_score: 0.4342 - val_loss: 1.0502 - val_accuracy: 0.4391 - val_auc: 0.6272 - val_f1_score: 0.4278
Epoch 13/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0416 - accuracy: 0.4484 - auc: 0.6373 - f1_score: 0.4371 - val_loss: 1.0500 - val_accuracy: 0.4403 - val_auc: 0.6273 - val_f1_score: 0.4264
Epoch 14/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0402 - accuracy: 0.4498 - auc: 0.6392 - f1_score: 0.4397 - val_loss: 1.0503 - val_accuracy: 0.4396 - val_auc: 0.6277 - val_f1_score: 0.4284
Epoch 15/50
188/188 [==============================] - 8s 43ms/step - loss: 1.0395 - accuracy: 0.4512 - auc: 0.6399 - f1_score: 0.4411 - val_loss: 1.0500 - val_accuracy: 0.4408 - val_auc: 0.6274 - val_f1_score: 0.4347
Epoch 16/50
188/188 [==============================] - 8s 44ms/step - loss: 1.0394 - accuracy: 0.4533 - auc: 0.6409 - f1_score: 0.4424 - val_loss: 1.0521 - val_accuracy: 0.4405 - val_auc: 0.6249 - val_f1_score: 0.4358
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>168</th>
      <td>0.438048</td>
      <td>0.432084</td>
      <td>0.461331</td>
      <td>0.462369</td>
      <td>0.653282</td>
      <td>0.652929</td>
      <td>0.454237</td>
      <td>0.454276</td>
      <td>8</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.441689</td>
      <td>0.435859</td>
      <td>0.467380</td>
      <td>0.468720</td>
      <td>0.656002</td>
      <td>0.655920</td>
      <td>0.454019</td>
      <td>0.454023</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>170</th>
      <td>0.441167</td>
      <td>0.434844</td>
      <td>0.474618</td>
      <td>0.474767</td>
      <td>0.665433</td>
      <td>0.663710</td>
      <td>0.462702</td>
      <td>0.461737</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.432137</td>
      <td>0.426055</td>
      <td>0.448392</td>
      <td>0.449677</td>
      <td>0.637223</td>
      <td>0.637144</td>
      <td>0.437986</td>
      <td>0.438061</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>172</th>
      <td>0.434884</td>
      <td>0.428571</td>
      <td>0.461592</td>
      <td>0.461419</td>
      <td>0.648309</td>
      <td>0.647022</td>
      <td>0.457222</td>
      <td>0.456181</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>172 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 3, 7)
Test shape: (10292, 3, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 3, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 3, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 3, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 300)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 300)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 608)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          304500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 444,043
Trainable params: 444,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 15s 60ms/step - loss: 1.0857 - accuracy: 0.3679 - auc: 0.5639 - f1_score: 0.4189 - val_loss: 1.0670 - val_accuracy: 0.3898 - val_auc: 0.5994 - val_f1_score: 0.3111
Epoch 2/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0691 - accuracy: 0.3822 - auc: 0.5939 - f1_score: 0.3146 - val_loss: 1.0616 - val_accuracy: 0.3899 - val_auc: 0.6090 - val_f1_score: 0.3402
Epoch 3/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0624 - accuracy: 0.3868 - auc: 0.6053 - f1_score: 0.3605 - val_loss: 1.0587 - val_accuracy: 0.4017 - val_auc: 0.6131 - val_f1_score: 0.4000
Epoch 4/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0576 - accuracy: 0.4081 - auc: 0.6135 - f1_score: 0.4074 - val_loss: 1.0548 - val_accuracy: 0.4287 - val_auc: 0.6191 - val_f1_score: 0.4049
Epoch 5/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0532 - accuracy: 0.4341 - auc: 0.6207 - f1_score: 0.4196 - val_loss: 1.0532 - val_accuracy: 0.4341 - val_auc: 0.6212 - val_f1_score: 0.4043
Epoch 6/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0499 - accuracy: 0.4407 - auc: 0.6251 - f1_score: 0.4238 - val_loss: 1.0524 - val_accuracy: 0.4348 - val_auc: 0.6224 - val_f1_score: 0.4070
Epoch 7/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0468 - accuracy: 0.4446 - auc: 0.6290 - f1_score: 0.4259 - val_loss: 1.0522 - val_accuracy: 0.4372 - val_auc: 0.6231 - val_f1_score: 0.4106
Epoch 8/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0456 - accuracy: 0.4451 - auc: 0.6305 - f1_score: 0.4273 - val_loss: 1.0517 - val_accuracy: 0.4323 - val_auc: 0.6229 - val_f1_score: 0.4087
Epoch 9/50
188/188 [==============================] - 10s 52ms/step - loss: 1.0434 - accuracy: 0.4460 - auc: 0.6337 - f1_score: 0.4298 - val_loss: 1.0505 - val_accuracy: 0.4390 - val_auc: 0.6273 - val_f1_score: 0.4183
Epoch 10/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0425 - accuracy: 0.4488 - auc: 0.6363 - f1_score: 0.4351 - val_loss: 1.0515 - val_accuracy: 0.4347 - val_auc: 0.6264 - val_f1_score: 0.4141
Epoch 11/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0398 - accuracy: 0.4526 - auc: 0.6397 - f1_score: 0.4386 - val_loss: 1.0491 - val_accuracy: 0.4359 - val_auc: 0.6292 - val_f1_score: 0.4219
Epoch 12/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0391 - accuracy: 0.4517 - auc: 0.6402 - f1_score: 0.4392 - val_loss: 1.0489 - val_accuracy: 0.4357 - val_auc: 0.6295 - val_f1_score: 0.4170
Epoch 13/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0375 - accuracy: 0.4526 - auc: 0.6428 - f1_score: 0.4436 - val_loss: 1.0501 - val_accuracy: 0.4371 - val_auc: 0.6293 - val_f1_score: 0.4227
Epoch 14/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0364 - accuracy: 0.4558 - auc: 0.6450 - f1_score: 0.4458 - val_loss: 1.0501 - val_accuracy: 0.4366 - val_auc: 0.6293 - val_f1_score: 0.4204
Epoch 15/50
188/188 [==============================] - 10s 53ms/step - loss: 1.0354 - accuracy: 0.4551 - auc: 0.6452 - f1_score: 0.4464 - val_loss: 1.0483 - val_accuracy: 0.4373 - val_auc: 0.6314 - val_f1_score: 0.4301
Epoch 16/50
188/188 [==============================] - 10s 54ms/step - loss: 1.0342 - accuracy: 0.4552 - auc: 0.6463 - f1_score: 0.4461 - val_loss: 1.0495 - val_accuracy: 0.4433 - val_auc: 0.6304 - val_f1_score: 0.4350
Epoch 17/50
188/188 [==============================] - 11s 57ms/step - loss: 1.0331 - accuracy: 0.4576 - auc: 0.6478 - f1_score: 0.4513 - val_loss: 1.0502 - val_accuracy: 0.4332 - val_auc: 0.6290 - val_f1_score: 0.4223
Epoch 18/50
188/188 [==============================] - 10s 55ms/step - loss: 1.0334 - accuracy: 0.4590 - auc: 0.6479 - f1_score: 0.4512 - val_loss: 1.0499 - val_accuracy: 0.4360 - val_auc: 0.6279 - val_f1_score: 0.4223
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>169</th>
      <td>0.441689</td>
      <td>0.435859</td>
      <td>0.467380</td>
      <td>0.468720</td>
      <td>0.656002</td>
      <td>0.655920</td>
      <td>0.454019</td>
      <td>0.454023</td>
      <td>9</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>170</th>
      <td>0.441167</td>
      <td>0.434844</td>
      <td>0.474618</td>
      <td>0.474767</td>
      <td>0.665433</td>
      <td>0.663710</td>
      <td>0.462702</td>
      <td>0.461737</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.432137</td>
      <td>0.426055</td>
      <td>0.448392</td>
      <td>0.449677</td>
      <td>0.637223</td>
      <td>0.637144</td>
      <td>0.437986</td>
      <td>0.438061</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>172</th>
      <td>0.434884</td>
      <td>0.428571</td>
      <td>0.461592</td>
      <td>0.461419</td>
      <td>0.648309</td>
      <td>0.647022</td>
      <td>0.457222</td>
      <td>0.456181</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.439088</td>
      <td>0.432631</td>
      <td>0.472841</td>
      <td>0.471886</td>
      <td>0.662121</td>
      <td>0.660001</td>
      <td>0.460382</td>
      <td>0.458165</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>173 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 4, 7)
Test shape: (10292, 4, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 4, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 4, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 4, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 400)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 400)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 808)          0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          404500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 544,043
Trainable params: 544,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 17s 70ms/step - loss: 1.0831 - accuracy: 0.3963 - auc: 0.5622 - f1_score: 0.4201 - val_loss: 1.0713 - val_accuracy: 0.4234 - val_auc: 0.6031 - val_f1_score: 0.3971
Epoch 2/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0592 - accuracy: 0.4277 - auc: 0.6131 - f1_score: 0.4017 - val_loss: 1.0551 - val_accuracy: 0.4282 - val_auc: 0.6185 - val_f1_score: 0.3939
Epoch 3/50
188/188 [==============================] - 12s 63ms/step - loss: 1.0495 - accuracy: 0.4376 - auc: 0.6265 - f1_score: 0.4093 - val_loss: 1.0509 - val_accuracy: 0.4336 - val_auc: 0.6242 - val_f1_score: 0.4114
Epoch 4/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0452 - accuracy: 0.4428 - auc: 0.6323 - f1_score: 0.4225 - val_loss: 1.0495 - val_accuracy: 0.4373 - val_auc: 0.6267 - val_f1_score: 0.4189
Epoch 5/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0423 - accuracy: 0.4468 - auc: 0.6355 - f1_score: 0.4328 - val_loss: 1.0479 - val_accuracy: 0.4386 - val_auc: 0.6284 - val_f1_score: 0.4246
Epoch 6/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0401 - accuracy: 0.4496 - auc: 0.6389 - f1_score: 0.4379 - val_loss: 1.0475 - val_accuracy: 0.4359 - val_auc: 0.6292 - val_f1_score: 0.4248
Epoch 7/50
188/188 [==============================] - 11s 60ms/step - loss: 1.0389 - accuracy: 0.4530 - auc: 0.6399 - f1_score: 0.4422 - val_loss: 1.0480 - val_accuracy: 0.4375 - val_auc: 0.6286 - val_f1_score: 0.4309
Epoch 8/50
188/188 [==============================] - 12s 62ms/step - loss: 1.0371 - accuracy: 0.4533 - auc: 0.6422 - f1_score: 0.4421 - val_loss: 1.0488 - val_accuracy: 0.4380 - val_auc: 0.6283 - val_f1_score: 0.4301
Epoch 9/50
188/188 [==============================] - 11s 61ms/step - loss: 1.0356 - accuracy: 0.4557 - auc: 0.6451 - f1_score: 0.4479 - val_loss: 1.0480 - val_accuracy: 0.4366 - val_auc: 0.6282 - val_f1_score: 0.4286
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>170</th>
      <td>0.441167</td>
      <td>0.434844</td>
      <td>0.474618</td>
      <td>0.474767</td>
      <td>0.665433</td>
      <td>0.663710</td>
      <td>0.462702</td>
      <td>0.461737</td>
      <td>10</td>
      <td>LSTM</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.432137</td>
      <td>0.426055</td>
      <td>0.448392</td>
      <td>0.449677</td>
      <td>0.637223</td>
      <td>0.637144</td>
      <td>0.437986</td>
      <td>0.438061</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>172</th>
      <td>0.434884</td>
      <td>0.428571</td>
      <td>0.461592</td>
      <td>0.461419</td>
      <td>0.648309</td>
      <td>0.647022</td>
      <td>0.457222</td>
      <td>0.456181</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.439088</td>
      <td>0.432631</td>
      <td>0.472841</td>
      <td>0.471886</td>
      <td>0.662121</td>
      <td>0.660001</td>
      <td>0.460382</td>
      <td>0.458165</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.438077</td>
      <td>0.431860</td>
      <td>0.464241</td>
      <td>0.464800</td>
      <td>0.654350</td>
      <td>0.653258</td>
      <td>0.455784</td>
      <td>0.455303</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>174 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 5, 7)
Test shape: (10292, 5, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 5, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 5, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 5, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 500)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 500)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          504500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 644,043
Trainable params: 644,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 16s 70ms/step - loss: 1.0678 - accuracy: 0.4105 - auc: 0.5973 - f1_score: 0.4331 - val_loss: 1.0545 - val_accuracy: 0.4255 - val_auc: 0.6198 - val_f1_score: 0.3913
Epoch 2/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0505 - accuracy: 0.4375 - auc: 0.6246 - f1_score: 0.4198 - val_loss: 1.0501 - val_accuracy: 0.4391 - val_auc: 0.6274 - val_f1_score: 0.4171
Epoch 3/50
188/188 [==============================] - 12s 65ms/step - loss: 1.0451 - accuracy: 0.4422 - auc: 0.6319 - f1_score: 0.4284 - val_loss: 1.0481 - val_accuracy: 0.4414 - val_auc: 0.6297 - val_f1_score: 0.4296
Epoch 4/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0410 - accuracy: 0.4499 - auc: 0.6381 - f1_score: 0.4391 - val_loss: 1.0487 - val_accuracy: 0.4378 - val_auc: 0.6294 - val_f1_score: 0.4258
Epoch 5/50
188/188 [==============================] - 13s 69ms/step - loss: 1.0387 - accuracy: 0.4506 - auc: 0.6405 - f1_score: 0.4399 - val_loss: 1.0468 - val_accuracy: 0.4411 - val_auc: 0.6307 - val_f1_score: 0.4327
Epoch 6/50
188/188 [==============================] - 12s 64ms/step - loss: 1.0361 - accuracy: 0.4539 - auc: 0.6445 - f1_score: 0.4469 - val_loss: 1.0459 - val_accuracy: 0.4412 - val_auc: 0.6316 - val_f1_score: 0.4373
Epoch 7/50
188/188 [==============================] - 12s 66ms/step - loss: 1.0352 - accuracy: 0.4543 - auc: 0.6456 - f1_score: 0.4497 - val_loss: 1.0467 - val_accuracy: 0.4405 - val_auc: 0.6306 - val_f1_score: 0.4333
Epoch 8/50
188/188 [==============================] - 13s 67ms/step - loss: 1.0320 - accuracy: 0.4599 - auc: 0.6492 - f1_score: 0.4545 - val_loss: 1.0476 - val_accuracy: 0.4436 - val_auc: 0.6308 - val_f1_score: 0.4395
Epoch 9/50
188/188 [==============================] - 13s 68ms/step - loss: 1.0314 - accuracy: 0.4591 - auc: 0.6496 - f1_score: 0.4555 - val_loss: 1.0482 - val_accuracy: 0.4361 - val_auc: 0.6286 - val_f1_score: 0.4293
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>171</th>
      <td>0.432137</td>
      <td>0.426055</td>
      <td>0.448392</td>
      <td>0.449677</td>
      <td>0.637223</td>
      <td>0.637144</td>
      <td>0.437986</td>
      <td>0.438061</td>
      <td>1</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>172</th>
      <td>0.434884</td>
      <td>0.428571</td>
      <td>0.461592</td>
      <td>0.461419</td>
      <td>0.648309</td>
      <td>0.647022</td>
      <td>0.457222</td>
      <td>0.456181</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.439088</td>
      <td>0.432631</td>
      <td>0.472841</td>
      <td>0.471886</td>
      <td>0.662121</td>
      <td>0.660001</td>
      <td>0.460382</td>
      <td>0.458165</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.438077</td>
      <td>0.431860</td>
      <td>0.464241</td>
      <td>0.464800</td>
      <td>0.654350</td>
      <td>0.653258</td>
      <td>0.455784</td>
      <td>0.455303</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.441899</td>
      <td>0.435564</td>
      <td>0.472874</td>
      <td>0.472494</td>
      <td>0.664032</td>
      <td>0.662458</td>
      <td>0.465522</td>
      <td>0.464089</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>175 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 6, 7)
Test shape: (10292, 6, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 6, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 6, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 6, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 600)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 600)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1208)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          604500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 744,043
Trainable params: 744,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 19s 83ms/step - loss: 1.0703 - accuracy: 0.4115 - auc: 0.5918 - f1_score: 0.4304 - val_loss: 1.0559 - val_accuracy: 0.4342 - val_auc: 0.6179 - val_f1_score: 0.4292
Epoch 2/50
188/188 [==============================] - 14s 76ms/step - loss: 1.0510 - accuracy: 0.4355 - auc: 0.6236 - f1_score: 0.4203 - val_loss: 1.0510 - val_accuracy: 0.4384 - val_auc: 0.6250 - val_f1_score: 0.4286
Epoch 3/50
188/188 [==============================] - 14s 77ms/step - loss: 1.0446 - accuracy: 0.4451 - auc: 0.6327 - f1_score: 0.4331 - val_loss: 1.0490 - val_accuracy: 0.4408 - val_auc: 0.6274 - val_f1_score: 0.4352
Epoch 4/50
188/188 [==============================] - 15s 79ms/step - loss: 1.0421 - accuracy: 0.4463 - auc: 0.6358 - f1_score: 0.4355 - val_loss: 1.0487 - val_accuracy: 0.4388 - val_auc: 0.6285 - val_f1_score: 0.4328
Epoch 5/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0389 - accuracy: 0.4512 - auc: 0.6399 - f1_score: 0.4421 - val_loss: 1.0477 - val_accuracy: 0.4424 - val_auc: 0.6291 - val_f1_score: 0.4390
Epoch 6/50
188/188 [==============================] - 16s 83ms/step - loss: 1.0363 - accuracy: 0.4531 - auc: 0.6427 - f1_score: 0.4438 - val_loss: 1.0472 - val_accuracy: 0.4423 - val_auc: 0.6300 - val_f1_score: 0.4392
Epoch 7/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0336 - accuracy: 0.4570 - auc: 0.6467 - f1_score: 0.4503 - val_loss: 1.0464 - val_accuracy: 0.4438 - val_auc: 0.6314 - val_f1_score: 0.4418
Epoch 8/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0317 - accuracy: 0.4580 - auc: 0.6485 - f1_score: 0.4517 - val_loss: 1.0481 - val_accuracy: 0.4414 - val_auc: 0.6292 - val_f1_score: 0.4381
Epoch 9/50
188/188 [==============================] - 15s 82ms/step - loss: 1.0301 - accuracy: 0.4601 - auc: 0.6509 - f1_score: 0.4529 - val_loss: 1.0453 - val_accuracy: 0.4421 - val_auc: 0.6322 - val_f1_score: 0.4412
Epoch 10/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0283 - accuracy: 0.4642 - auc: 0.6529 - f1_score: 0.4583 - val_loss: 1.0460 - val_accuracy: 0.4442 - val_auc: 0.6319 - val_f1_score: 0.4439
Epoch 11/50
188/188 [==============================] - 15s 82ms/step - loss: 1.0270 - accuracy: 0.4638 - auc: 0.6548 - f1_score: 0.4578 - val_loss: 1.0472 - val_accuracy: 0.4424 - val_auc: 0.6316 - val_f1_score: 0.4402
Epoch 12/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0254 - accuracy: 0.4661 - auc: 0.6558 - f1_score: 0.4609 - val_loss: 1.0469 - val_accuracy: 0.4399 - val_auc: 0.6303 - val_f1_score: 0.4391
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>172</th>
      <td>0.434884</td>
      <td>0.428571</td>
      <td>0.461592</td>
      <td>0.461419</td>
      <td>0.648309</td>
      <td>0.647022</td>
      <td>0.457222</td>
      <td>0.456181</td>
      <td>2</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.439088</td>
      <td>0.432631</td>
      <td>0.472841</td>
      <td>0.471886</td>
      <td>0.662121</td>
      <td>0.660001</td>
      <td>0.460382</td>
      <td>0.458165</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.438077</td>
      <td>0.431860</td>
      <td>0.464241</td>
      <td>0.464800</td>
      <td>0.654350</td>
      <td>0.653258</td>
      <td>0.455784</td>
      <td>0.455303</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.441899</td>
      <td>0.435564</td>
      <td>0.472874</td>
      <td>0.472494</td>
      <td>0.664032</td>
      <td>0.662458</td>
      <td>0.465522</td>
      <td>0.464089</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.444031</td>
      <td>0.437242</td>
      <td>0.482227</td>
      <td>0.479923</td>
      <td>0.672538</td>
      <td>0.669067</td>
      <td>0.481344</td>
      <td>0.478470</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>176 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 7, 7)
Test shape: (10292, 7, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 7, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 7, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 7, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 700)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 700)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1408)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          704500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 844,043
Trainable params: 844,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 23s 97ms/step - loss: 1.0771 - accuracy: 0.3976 - auc: 0.5819 - f1_score: 0.4450 - val_loss: 1.0593 - val_accuracy: 0.4308 - val_auc: 0.6133 - val_f1_score: 0.4112
Epoch 2/50
188/188 [==============================] - 18s 97ms/step - loss: 1.0526 - accuracy: 0.4321 - auc: 0.6210 - f1_score: 0.4242 - val_loss: 1.0532 - val_accuracy: 0.4336 - val_auc: 0.6212 - val_f1_score: 0.4160
Epoch 3/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0465 - accuracy: 0.4393 - auc: 0.6303 - f1_score: 0.4329 - val_loss: 1.0501 - val_accuracy: 0.4445 - val_auc: 0.6258 - val_f1_score: 0.4277
Epoch 4/50
188/188 [==============================] - 18s 95ms/step - loss: 1.0421 - accuracy: 0.4473 - auc: 0.6367 - f1_score: 0.4391 - val_loss: 1.0494 - val_accuracy: 0.4386 - val_auc: 0.6266 - val_f1_score: 0.4219
Epoch 5/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0384 - accuracy: 0.4502 - auc: 0.6409 - f1_score: 0.4440 - val_loss: 1.0483 - val_accuracy: 0.4427 - val_auc: 0.6289 - val_f1_score: 0.4242
Epoch 6/50
188/188 [==============================] - 15s 82ms/step - loss: 1.0351 - accuracy: 0.4538 - auc: 0.6446 - f1_score: 0.4482 - val_loss: 1.0483 - val_accuracy: 0.4471 - val_auc: 0.6306 - val_f1_score: 0.4364
Epoch 7/50
188/188 [==============================] - 15s 81ms/step - loss: 1.0324 - accuracy: 0.4596 - auc: 0.6492 - f1_score: 0.4544 - val_loss: 1.0468 - val_accuracy: 0.4438 - val_auc: 0.6309 - val_f1_score: 0.4328
Epoch 8/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0309 - accuracy: 0.4610 - auc: 0.6508 - f1_score: 0.4557 - val_loss: 1.0497 - val_accuracy: 0.4382 - val_auc: 0.6273 - val_f1_score: 0.4252
Epoch 9/50
188/188 [==============================] - 16s 85ms/step - loss: 1.0295 - accuracy: 0.4629 - auc: 0.6521 - f1_score: 0.4569 - val_loss: 1.0474 - val_accuracy: 0.4409 - val_auc: 0.6294 - val_f1_score: 0.4276
Epoch 10/50
188/188 [==============================] - 16s 84ms/step - loss: 1.0252 - accuracy: 0.4661 - auc: 0.6569 - f1_score: 0.4606 - val_loss: 1.0493 - val_accuracy: 0.4414 - val_auc: 0.6289 - val_f1_score: 0.4274
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>173</th>
      <td>0.439088</td>
      <td>0.432631</td>
      <td>0.472841</td>
      <td>0.471886</td>
      <td>0.662121</td>
      <td>0.660001</td>
      <td>0.460382</td>
      <td>0.458165</td>
      <td>3</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.438077</td>
      <td>0.431860</td>
      <td>0.464241</td>
      <td>0.464800</td>
      <td>0.654350</td>
      <td>0.653258</td>
      <td>0.455784</td>
      <td>0.455303</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.441899</td>
      <td>0.435564</td>
      <td>0.472874</td>
      <td>0.472494</td>
      <td>0.664032</td>
      <td>0.662458</td>
      <td>0.465522</td>
      <td>0.464089</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.444031</td>
      <td>0.437242</td>
      <td>0.482227</td>
      <td>0.479923</td>
      <td>0.672538</td>
      <td>0.669067</td>
      <td>0.481344</td>
      <td>0.478470</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.447461</td>
      <td>0.440868</td>
      <td>0.484178</td>
      <td>0.482863</td>
      <td>0.674146</td>
      <td>0.671411</td>
      <td>0.470896</td>
      <td>0.468189</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>177 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8, 7)
Test shape: (10292, 8, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 8, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 8, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 8, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 800)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 800)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1608)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          804500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 944,043
Trainable params: 944,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 22s 100ms/step - loss: 1.0711 - accuracy: 0.4033 - auc: 0.5912 - f1_score: 0.4362 - val_loss: 1.0597 - val_accuracy: 0.4234 - val_auc: 0.6136 - val_f1_score: 0.3696
Epoch 2/50
188/188 [==============================] - 18s 96ms/step - loss: 1.0507 - accuracy: 0.4348 - auc: 0.6235 - f1_score: 0.4109 - val_loss: 1.0535 - val_accuracy: 0.4312 - val_auc: 0.6212 - val_f1_score: 0.4131
Epoch 3/50
188/188 [==============================] - 18s 97ms/step - loss: 1.0447 - accuracy: 0.4429 - auc: 0.6327 - f1_score: 0.4284 - val_loss: 1.0507 - val_accuracy: 0.4411 - val_auc: 0.6268 - val_f1_score: 0.4227
Epoch 4/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0405 - accuracy: 0.4500 - auc: 0.6387 - f1_score: 0.4388 - val_loss: 1.0510 - val_accuracy: 0.4378 - val_auc: 0.6261 - val_f1_score: 0.4245
Epoch 5/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0371 - accuracy: 0.4513 - auc: 0.6425 - f1_score: 0.4420 - val_loss: 1.0494 - val_accuracy: 0.4433 - val_auc: 0.6280 - val_f1_score: 0.4274
Epoch 6/50
188/188 [==============================] - 20s 104ms/step - loss: 1.0332 - accuracy: 0.4545 - auc: 0.6469 - f1_score: 0.4446 - val_loss: 1.0486 - val_accuracy: 0.4410 - val_auc: 0.6284 - val_f1_score: 0.4252
Epoch 7/50
188/188 [==============================] - 16s 86ms/step - loss: 1.0315 - accuracy: 0.4583 - auc: 0.6493 - f1_score: 0.4490 - val_loss: 1.0493 - val_accuracy: 0.4380 - val_auc: 0.6288 - val_f1_score: 0.4199
Epoch 8/50
188/188 [==============================] - 17s 93ms/step - loss: 1.0293 - accuracy: 0.4601 - auc: 0.6520 - f1_score: 0.4521 - val_loss: 1.0482 - val_accuracy: 0.4362 - val_auc: 0.6280 - val_f1_score: 0.4219
Epoch 9/50
188/188 [==============================] - 17s 88ms/step - loss: 1.0259 - accuracy: 0.4650 - auc: 0.6563 - f1_score: 0.4574 - val_loss: 1.0494 - val_accuracy: 0.4396 - val_auc: 0.6288 - val_f1_score: 0.4318
Epoch 10/50
188/188 [==============================] - 17s 90ms/step - loss: 1.0237 - accuracy: 0.4683 - auc: 0.6588 - f1_score: 0.4622 - val_loss: 1.0490 - val_accuracy: 0.4411 - val_auc: 0.6280 - val_f1_score: 0.4322
Epoch 11/50
188/188 [==============================] - 17s 89ms/step - loss: 1.0208 - accuracy: 0.4712 - auc: 0.6620 - f1_score: 0.4648 - val_loss: 1.0521 - val_accuracy: 0.4336 - val_auc: 0.6249 - val_f1_score: 0.4271
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>174</th>
      <td>0.438077</td>
      <td>0.431860</td>
      <td>0.464241</td>
      <td>0.464800</td>
      <td>0.654350</td>
      <td>0.653258</td>
      <td>0.455784</td>
      <td>0.455303</td>
      <td>4</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.441899</td>
      <td>0.435564</td>
      <td>0.472874</td>
      <td>0.472494</td>
      <td>0.664032</td>
      <td>0.662458</td>
      <td>0.465522</td>
      <td>0.464089</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.444031</td>
      <td>0.437242</td>
      <td>0.482227</td>
      <td>0.479923</td>
      <td>0.672538</td>
      <td>0.669067</td>
      <td>0.481344</td>
      <td>0.478470</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.447461</td>
      <td>0.440868</td>
      <td>0.484178</td>
      <td>0.482863</td>
      <td>0.674146</td>
      <td>0.671411</td>
      <td>0.470896</td>
      <td>0.468189</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>178</th>
      <td>0.448383</td>
      <td>0.441318</td>
      <td>0.490936</td>
      <td>0.488175</td>
      <td>0.682273</td>
      <td>0.678204</td>
      <td>0.484347</td>
      <td>0.480575</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>178 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 24s 112ms/step - loss: 1.0848 - accuracy: 0.3709 - auc: 0.5587 - f1_score: 0.4355 - val_loss: 1.0751 - val_accuracy: 0.3823 - val_auc: 0.5831 - val_f1_score: 0.3282
Epoch 2/50
188/188 [==============================] - 22s 119ms/step - loss: 1.0704 - accuracy: 0.3884 - auc: 0.5892 - f1_score: 0.3321 - val_loss: 1.0709 - val_accuracy: 0.3896 - val_auc: 0.5918 - val_f1_score: 0.3315
Epoch 3/50
188/188 [==============================] - 20s 107ms/step - loss: 1.0648 - accuracy: 0.3961 - auc: 0.6009 - f1_score: 0.3526 - val_loss: 1.0693 - val_accuracy: 0.3831 - val_auc: 0.5934 - val_f1_score: 0.3512
Epoch 4/50
188/188 [==============================] - 21s 109ms/step - loss: 1.0597 - accuracy: 0.4009 - auc: 0.6095 - f1_score: 0.3643 - val_loss: 1.0660 - val_accuracy: 0.3906 - val_auc: 0.5989 - val_f1_score: 0.3555
Epoch 5/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0550 - accuracy: 0.4103 - auc: 0.6187 - f1_score: 0.3881 - val_loss: 1.0643 - val_accuracy: 0.4050 - val_auc: 0.6040 - val_f1_score: 0.3896
Epoch 6/50
188/188 [==============================] - 19s 103ms/step - loss: 1.0512 - accuracy: 0.4212 - auc: 0.6245 - f1_score: 0.4070 - val_loss: 1.0630 - val_accuracy: 0.4040 - val_auc: 0.6052 - val_f1_score: 0.3960
Epoch 7/50
188/188 [==============================] - 20s 107ms/step - loss: 1.0457 - accuracy: 0.4306 - auc: 0.6328 - f1_score: 0.4241 - val_loss: 1.0619 - val_accuracy: 0.4121 - val_auc: 0.6094 - val_f1_score: 0.4066
Epoch 8/50
188/188 [==============================] - 20s 104ms/step - loss: 1.0419 - accuracy: 0.4376 - auc: 0.6385 - f1_score: 0.4350 - val_loss: 1.0617 - val_accuracy: 0.4271 - val_auc: 0.6128 - val_f1_score: 0.4208
Epoch 9/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0377 - accuracy: 0.4525 - auc: 0.6459 - f1_score: 0.4513 - val_loss: 1.0611 - val_accuracy: 0.4314 - val_auc: 0.6143 - val_f1_score: 0.4195
Epoch 10/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0307 - accuracy: 0.4658 - auc: 0.6542 - f1_score: 0.4587 - val_loss: 1.0563 - val_accuracy: 0.4347 - val_auc: 0.6202 - val_f1_score: 0.4139
Epoch 11/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0227 - accuracy: 0.4711 - auc: 0.6613 - f1_score: 0.4617 - val_loss: 1.0537 - val_accuracy: 0.4359 - val_auc: 0.6240 - val_f1_score: 0.4123
Epoch 12/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0175 - accuracy: 0.4772 - auc: 0.6667 - f1_score: 0.4691 - val_loss: 1.0551 - val_accuracy: 0.4333 - val_auc: 0.6229 - val_f1_score: 0.4173
Epoch 13/50
188/188 [==============================] - 20s 104ms/step - loss: 1.0122 - accuracy: 0.4831 - auc: 0.6728 - f1_score: 0.4769 - val_loss: 1.0560 - val_accuracy: 0.4349 - val_auc: 0.6242 - val_f1_score: 0.4282
Epoch 14/50
188/188 [==============================] - 19s 103ms/step - loss: 1.0081 - accuracy: 0.4878 - auc: 0.6770 - f1_score: 0.4827 - val_loss: 1.0582 - val_accuracy: 0.4355 - val_auc: 0.6219 - val_f1_score: 0.4118
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>175</th>
      <td>0.441899</td>
      <td>0.435564</td>
      <td>0.472874</td>
      <td>0.472494</td>
      <td>0.664032</td>
      <td>0.662458</td>
      <td>0.465522</td>
      <td>0.464089</td>
      <td>5</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.444031</td>
      <td>0.437242</td>
      <td>0.482227</td>
      <td>0.479923</td>
      <td>0.672538</td>
      <td>0.669067</td>
      <td>0.481344</td>
      <td>0.478470</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.447461</td>
      <td>0.440868</td>
      <td>0.484178</td>
      <td>0.482863</td>
      <td>0.674146</td>
      <td>0.671411</td>
      <td>0.470896</td>
      <td>0.468189</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>178</th>
      <td>0.448383</td>
      <td>0.441318</td>
      <td>0.490936</td>
      <td>0.488175</td>
      <td>0.682273</td>
      <td>0.678204</td>
      <td>0.484347</td>
      <td>0.480575</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>179</th>
      <td>0.459260</td>
      <td>0.451711</td>
      <td>0.511080</td>
      <td>0.507385</td>
      <td>0.701745</td>
      <td>0.696496</td>
      <td>0.491997</td>
      <td>0.486663</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>179 rows × 12 columns</p>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 25s 118ms/step - loss: 1.0724 - accuracy: 0.4037 - auc: 0.5888 - f1_score: 0.4524 - val_loss: 1.0573 - val_accuracy: 0.4235 - val_auc: 0.6146 - val_f1_score: 0.4219
Epoch 2/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0494 - accuracy: 0.4368 - auc: 0.6260 - f1_score: 0.4356 - val_loss: 1.0506 - val_accuracy: 0.4405 - val_auc: 0.6270 - val_f1_score: 0.4369
Epoch 3/50
188/188 [==============================] - 20s 108ms/step - loss: 1.0422 - accuracy: 0.4492 - auc: 0.6360 - f1_score: 0.4479 - val_loss: 1.0493 - val_accuracy: 0.4421 - val_auc: 0.6285 - val_f1_score: 0.4425
Epoch 4/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0381 - accuracy: 0.4532 - auc: 0.6414 - f1_score: 0.4510 - val_loss: 1.0490 - val_accuracy: 0.4414 - val_auc: 0.6300 - val_f1_score: 0.4395
Epoch 5/50
188/188 [==============================] - 20s 108ms/step - loss: 1.0337 - accuracy: 0.4581 - auc: 0.6476 - f1_score: 0.4563 - val_loss: 1.0483 - val_accuracy: 0.4440 - val_auc: 0.6310 - val_f1_score: 0.4438
Epoch 6/50
188/188 [==============================] - 20s 109ms/step - loss: 1.0301 - accuracy: 0.4612 - auc: 0.6514 - f1_score: 0.4597 - val_loss: 1.0464 - val_accuracy: 0.4453 - val_auc: 0.6322 - val_f1_score: 0.4456
Epoch 7/50
188/188 [==============================] - 20s 107ms/step - loss: 1.0263 - accuracy: 0.4672 - auc: 0.6558 - f1_score: 0.4655 - val_loss: 1.0475 - val_accuracy: 0.4462 - val_auc: 0.6322 - val_f1_score: 0.4461
Epoch 8/50
188/188 [==============================] - 23s 123ms/step - loss: 1.0234 - accuracy: 0.4714 - auc: 0.6597 - f1_score: 0.4695 - val_loss: 1.0462 - val_accuracy: 0.4457 - val_auc: 0.6334 - val_f1_score: 0.4456
Epoch 9/50
188/188 [==============================] - 21s 113ms/step - loss: 1.0187 - accuracy: 0.4770 - auc: 0.6649 - f1_score: 0.4756 - val_loss: 1.0486 - val_accuracy: 0.4372 - val_auc: 0.6298 - val_f1_score: 0.4376
Epoch 10/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0142 - accuracy: 0.4805 - auc: 0.6693 - f1_score: 0.4799 - val_loss: 1.0503 - val_accuracy: 0.4393 - val_auc: 0.6295 - val_f1_score: 0.4389
Epoch 11/50
188/188 [==============================] - 22s 116ms/step - loss: 1.0112 - accuracy: 0.4835 - auc: 0.6734 - f1_score: 0.4829 - val_loss: 1.0492 - val_accuracy: 0.4392 - val_auc: 0.6299 - val_f1_score: 0.4392
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.444031</td>
      <td>0.437242</td>
      <td>0.482227</td>
      <td>0.479923</td>
      <td>0.672538</td>
      <td>0.669067</td>
      <td>0.481344</td>
      <td>0.478470</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.447461</td>
      <td>0.440868</td>
      <td>0.484178</td>
      <td>0.482863</td>
      <td>0.674146</td>
      <td>0.671411</td>
      <td>0.470896</td>
      <td>0.468189</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>178</th>
      <td>0.448383</td>
      <td>0.441318</td>
      <td>0.490936</td>
      <td>0.488175</td>
      <td>0.682273</td>
      <td>0.678204</td>
      <td>0.484347</td>
      <td>0.480575</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>179</th>
      <td>0.459260</td>
      <td>0.451711</td>
      <td>0.511080</td>
      <td>0.507385</td>
      <td>0.701745</td>
      <td>0.696496</td>
      <td>0.491997</td>
      <td>0.486663</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>180</th>
      <td>0.454417</td>
      <td>0.446841</td>
      <td>0.511266</td>
      <td>0.505680</td>
      <td>0.698243</td>
      <td>0.692201</td>
      <td>0.511400</td>
      <td>0.505325</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>180 rows × 12 columns</p>
</div>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>El resultado se condensó en el siguiente <em>dataframe</em>:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">display</span><span class="p">(</span><span class="n">Report</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.409215</td>
      <td>0.402452</td>
      <td>0.390826</td>
      <td>0.394498</td>
      <td>0.569866</td>
      <td>0.570228</td>
      <td>0.354709</td>
      <td>0.356641</td>
      <td>1</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.412872</td>
      <td>0.406182</td>
      <td>0.401225</td>
      <td>0.404161</td>
      <td>0.582815</td>
      <td>0.582820</td>
      <td>0.366837</td>
      <td>0.368093</td>
      <td>2</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.414844</td>
      <td>0.408316</td>
      <td>0.408670</td>
      <td>0.411257</td>
      <td>0.591682</td>
      <td>0.592314</td>
      <td>0.384350</td>
      <td>0.385402</td>
      <td>3</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.417667</td>
      <td>0.411080</td>
      <td>0.419287</td>
      <td>0.420617</td>
      <td>0.602573</td>
      <td>0.602214</td>
      <td>0.407251</td>
      <td>0.407376</td>
      <td>4</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.419681</td>
      <td>0.413222</td>
      <td>0.421249</td>
      <td>0.423352</td>
      <td>0.605344</td>
      <td>0.605543</td>
      <td>0.404745</td>
      <td>0.405495</td>
      <td>5</td>
      <td>LSTM</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>176</th>
      <td>0.444031</td>
      <td>0.437242</td>
      <td>0.482227</td>
      <td>0.479923</td>
      <td>0.672538</td>
      <td>0.669067</td>
      <td>0.481344</td>
      <td>0.478470</td>
      <td>6</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>177</th>
      <td>0.447461</td>
      <td>0.440868</td>
      <td>0.484178</td>
      <td>0.482863</td>
      <td>0.674146</td>
      <td>0.671411</td>
      <td>0.470896</td>
      <td>0.468189</td>
      <td>7</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>178</th>
      <td>0.448383</td>
      <td>0.441318</td>
      <td>0.490936</td>
      <td>0.488175</td>
      <td>0.682273</td>
      <td>0.678204</td>
      <td>0.484347</td>
      <td>0.480575</td>
      <td>8</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>179</th>
      <td>0.459260</td>
      <td>0.451711</td>
      <td>0.511080</td>
      <td>0.507385</td>
      <td>0.701745</td>
      <td>0.696496</td>
      <td>0.491997</td>
      <td>0.486663</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
    <tr>
      <th>180</th>
      <td>0.454417</td>
      <td>0.446841</td>
      <td>0.511266</td>
      <td>0.505680</td>
      <td>0.698243</td>
      <td>0.692201</td>
      <td>0.511400</td>
      <td>0.505325</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
    </tr>
  </tbody>
</table>
<p>180 rows × 12 columns</p>
</div>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>A continuación podemos ver una visualización de esta tabla. Se pueden ver los resultados en función de: el numero de partidos considerados (eje x), el tipo de red recurrente (tipo de RNN), el número de variables históricas por partido (# V x $P_h$), y el número de variables para el partido a modelar (# V x $P_m$). A través de ello vemos que el mejor resultado se obtiene para una red con:</p>
<ul>
<li>Tipo de red recurrente: Simple</li>
<li>Número de partidos a considerar: 9 o 10 (a priori elegiría 9, para tener menos parámetros)</li>
<li>
<em>Features</em> por cada partidos del historial (para la RNN): 'play_home', 'is_cup', 'rating_diff', 'goal_diff', 'coach_continuity', 'relevance', 'is_friendly'</li>
<li>
<em>Features</em> del partido a modelar: 'Rating_diff', 'EqA_Local', 'is_cup', 'Equipo_A_coach_continuity', 'Equipo_B_coach_continuity', 'diff_num_partidos_diez_dias', 'diff_num_partidos_tres_semanas', 'is_friendly'</li>
</ul>
<p>Algunas observaciones que podemos hacer son:</p>
<ul>
<li>La precisión lograda por la red con RNN LSTM parece crecer de forma más "suave" a medida que considero más encuentros</li>
<li>Si miramos a las cuatro curvas con menor precisión podemos ver una comparación entre considerar la diferencia de gol (rojo para la LSTM, verde para la Simple) o simplemente el resultado nominal (azul para la LSTM, cyan para la Simple). En ambos casos, considerar la diferencia de gol se ve reflejado en una mejora en los resultados del modelo. Esto tiene bastante sentido si se lo piensa, dado que la diferencia de gol no solo codifica la información cualitativa del resultado (Victoria, Derrota o Empate) sino que también la cuantitativa. Deinitivamente parece razonable pensar que un equipo que gana todos sus partidos anteriores por goleada tiene más chances de ganar el siguiente encuentro que uno que lo ha hecho por la mínima.</li>
</ul>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fffmp</span><span class="o">.</span><span class="n">plot_report_table</span><span class="p">(</span><span class="n">Report</span><span class="p">,</span><span class="s1">'accuracy test'</span><span class="p">,</span><span class="s1">'Test Accuracy'</span><span class="p">,[</span><span class="s1">'LSTM'</span><span class="p">,</span><span class="s1">'Simple'</span><span class="p">],[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">7</span><span class="p">],[</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">8</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Finalmente, hay una última cosa que quise probar: el efecto del <em>batch size</em>. Para el entrenamiento de los 180 modelos elegí un <em>batch size</em> relativamente grande para reducir los tiempos de entrenamiento. Esperaría que reducir el mismo permitiera mejorar la precisión final del modelo, sin embargo he ledio que tamaños muy pequeños de <em>batch</em> pueden hacer que finalmente no converja al mínimo global.</p>
<p>Para analizar esto utilizaré 7 valores de <em>batch size</em> para los cuales entrenaré modelos con los parámetros seleccionados en el paso anterior. Aprovecharé igualmente la oportunidad para terminar de decidir que número de partidos considerar para el modelo final: $9$ o $10$.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">Report2</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">index</span><span class="o">=</span><span class="mi">0</span>
<span class="k">for</span> <span class="n">n_partidos</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">]:</span>
    <span class="k">for</span> <span class="n">bsz</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">16</span><span class="p">,</span><span class="mi">32</span><span class="p">,</span><span class="mi">128</span><span class="p">,</span><span class="mi">400</span><span class="p">,</span><span class="mi">512</span><span class="p">,</span><span class="mi">1024</span><span class="p">,</span><span class="mi">2048</span><span class="p">]:</span>
        <span class="n">M_A</span><span class="p">,</span><span class="n">M_A_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_RNN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">F_h_3</span><span class="p">,[</span><span class="s1">'Equipo_A'</span><span class="p">],</span><span class="n">n_partidos</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">M_B</span><span class="p">,</span><span class="n">M_B_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_RNN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">F_h_3</span><span class="p">,[</span><span class="s1">'Equipo_B'</span><span class="p">],</span><span class="n">n_partidos</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">H_Features</span><span class="o">=</span><span class="p">[]</span>
        <span class="n">A_train</span><span class="p">,</span><span class="n">A_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_NN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">H_Features</span><span class="p">,</span><span class="n">N_Features_3</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 

        <span class="n">fffmp</span><span class="o">.</span><span class="n">clean_all_models</span><span class="p">()</span>
        <span class="n">time_steps</span><span class="o">=</span><span class="n">n_partidos</span>
        <span class="n">predictors</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">F_h_3</span><span class="p">)</span>
        <span class="n">tipo_RNN</span><span class="o">=</span><span class="s1">'Simple'</span>
        <span class="n">recurrent_units</span><span class="o">=</span><span class="mi">100</span>
        <span class="n">dense_units</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span>
        <span class="n">dense_drop_out</span><span class="o">=</span><span class="mf">0.1</span>
        <span class="n">out_shape</span><span class="o">=</span><span class="mi">3</span>
        <span class="n">input_shape_history</span><span class="o">=</span><span class="p">(</span><span class="n">time_steps</span><span class="p">,</span><span class="n">predictors</span><span class="p">)</span>
        <span class="n">input_shape_now</span><span class="o">=</span><span class="n">A_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">,</span><span class="s1">'AUC'</span><span class="p">,</span><span class="n">F1</span><span class="p">]</span>
        <span class="n">IFA</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Create_Mix_Model</span><span class="p">(</span><span class="n">tipo_RNN</span> <span class="p">,</span><span class="n">recurrent_units</span><span class="p">,</span> <span class="n">dense_units</span><span class="p">,</span><span class="n">dense_drop_out</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">input_shape_history</span><span class="p">,</span><span class="n">input_shape_now</span><span class="p">,</span><span class="n">metrics</span><span class="p">)</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">bsz</span>
        <span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
        <span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span>
        <span class="n">y_train_pred</span><span class="p">,</span><span class="n">y_test_pred</span><span class="p">,</span><span class="n">Results</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Full_train_and_Report</span><span class="p">(</span><span class="n">IFA</span><span class="p">,[</span><span class="n">M_A</span><span class="p">,</span><span class="n">M_B</span><span class="p">,</span><span class="n">A_train</span><span class="p">],</span><span class="n">y_train</span><span class="p">,[</span><span class="n">M_A_test</span><span class="p">,</span><span class="n">M_B_test</span><span class="p">,</span><span class="n">A_test</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">validation_split</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">history_plot</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span><span class="n">Report_print</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="o">+</span><span class="mi">1</span>
        <span class="n">name</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">index</span><span class="p">)</span>
        <span class="n">Report2</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Append_line_to_report_table_2</span><span class="p">(</span><span class="n">Report2</span><span class="p">,</span><span class="n">Results</span><span class="p">,</span><span class="n">name</span><span class="p">,</span><span class="n">tipo_de_RNN</span><span class="p">,</span><span class="n">n_partidos</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">F_h</span><span class="p">),</span><span class="nb">len</span><span class="p">(</span><span class="n">N_Features</span><span class="p">),</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Output" data-close="Show Output"></summary>
        <p>
</p>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
4692/4692 [==============================] - 112s 23ms/step - loss: 1.0633 - accuracy: 0.4204 - auc: 0.6039 - f1_score: 0.4484 - val_loss: 1.0532 - val_accuracy: 0.4303 - val_auc: 0.6210 - val_f1_score: 0.4132
Epoch 2/50
4692/4692 [==============================] - 99s 21ms/step - loss: 1.0460 - accuracy: 0.4422 - auc: 0.6304 - f1_score: 0.4385 - val_loss: 1.0509 - val_accuracy: 0.4414 - val_auc: 0.6271 - val_f1_score: 0.4413
Epoch 3/50
4692/4692 [==============================] - 113s 24ms/step - loss: 1.0410 - accuracy: 0.4471 - auc: 0.6370 - f1_score: 0.4446 - val_loss: 1.0489 - val_accuracy: 0.4329 - val_auc: 0.6253 - val_f1_score: 0.4325
Epoch 4/50
4692/4692 [==============================] - 105s 22ms/step - loss: 1.0367 - accuracy: 0.4522 - auc: 0.6422 - f1_score: 0.4494 - val_loss: 1.0512 - val_accuracy: 0.4372 - val_auc: 0.6248 - val_f1_score: 0.4377
Epoch 5/50
4692/4692 [==============================] - 103s 22ms/step - loss: 1.0329 - accuracy: 0.4548 - auc: 0.6471 - f1_score: 0.4510 - val_loss: 1.0500 - val_accuracy: 0.4367 - val_auc: 0.6252 - val_f1_score: 0.4289
Epoch 6/50
4692/4692 [==============================] - 101s 22ms/step - loss: 1.0297 - accuracy: 0.4588 - auc: 0.6509 - f1_score: 0.4546 - val_loss: 1.0491 - val_accuracy: 0.4385 - val_auc: 0.6276 - val_f1_score: 0.4349
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.47216</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
2346/2346 [==============================] - 72s 29ms/step - loss: 1.0583 - accuracy: 0.4255 - auc: 0.6125 - f1_score: 0.4458 - val_loss: 1.0538 - val_accuracy: 0.4333 - val_auc: 0.6209 - val_f1_score: 0.4321
Epoch 2/50
2346/2346 [==============================] - 69s 30ms/step - loss: 1.0464 - accuracy: 0.4417 - auc: 0.6302 - f1_score: 0.4386 - val_loss: 1.0490 - val_accuracy: 0.4393 - val_auc: 0.6269 - val_f1_score: 0.4340
Epoch 3/50
2346/2346 [==============================] - 68s 29ms/step - loss: 1.0402 - accuracy: 0.4465 - auc: 0.6381 - f1_score: 0.4430 - val_loss: 1.0484 - val_accuracy: 0.4392 - val_auc: 0.6277 - val_f1_score: 0.4343
Epoch 4/50
2346/2346 [==============================] - 68s 29ms/step - loss: 1.0368 - accuracy: 0.4521 - auc: 0.6422 - f1_score: 0.4489 - val_loss: 1.0484 - val_accuracy: 0.4372 - val_auc: 0.6278 - val_f1_score: 0.4375
Epoch 5/50
2346/2346 [==============================] - 71s 30ms/step - loss: 1.0315 - accuracy: 0.4573 - auc: 0.6486 - f1_score: 0.4557 - val_loss: 1.0494 - val_accuracy: 0.4438 - val_auc: 0.6296 - val_f1_score: 0.4376
Epoch 6/50
2346/2346 [==============================] - 72s 31ms/step - loss: 1.0272 - accuracy: 0.4617 - auc: 0.6534 - f1_score: 0.4607 - val_loss: 1.0507 - val_accuracy: 0.4412 - val_auc: 0.6257 - val_f1_score: 0.4407
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
587/587 [==============================] - 33s 51ms/step - loss: 1.0649 - accuracy: 0.4171 - auc: 0.6015 - f1_score: 0.4478 - val_loss: 1.0536 - val_accuracy: 0.4317 - val_auc: 0.6196 - val_f1_score: 0.4285
Epoch 2/50
587/587 [==============================] - 28s 48ms/step - loss: 1.0467 - accuracy: 0.4407 - auc: 0.6298 - f1_score: 0.4362 - val_loss: 1.0506 - val_accuracy: 0.4376 - val_auc: 0.6235 - val_f1_score: 0.4326
Epoch 3/50
587/587 [==============================] - 37s 63ms/step - loss: 1.0394 - accuracy: 0.4499 - auc: 0.6391 - f1_score: 0.4457 - val_loss: 1.0491 - val_accuracy: 0.4381 - val_auc: 0.6270 - val_f1_score: 0.4318
Epoch 4/50
587/587 [==============================] - 35s 60ms/step - loss: 1.0353 - accuracy: 0.4552 - auc: 0.6443 - f1_score: 0.4516 - val_loss: 1.0506 - val_accuracy: 0.4331 - val_auc: 0.6261 - val_f1_score: 0.4306
Epoch 5/50
587/587 [==============================] - 32s 55ms/step - loss: 1.0324 - accuracy: 0.4574 - auc: 0.6483 - f1_score: 0.4540 - val_loss: 1.0494 - val_accuracy: 0.4380 - val_auc: 0.6277 - val_f1_score: 0.4352
Epoch 6/50
587/587 [==============================] - 36s 61ms/step - loss: 1.0270 - accuracy: 0.4640 - auc: 0.6544 - f1_score: 0.4609 - val_loss: 1.0496 - val_accuracy: 0.4421 - val_auc: 0.6295 - val_f1_score: 0.4418
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 22s 103ms/step - loss: 1.0849 - accuracy: 0.3706 - auc: 0.5502 - f1_score: 0.4271 - val_loss: 1.0712 - val_accuracy: 0.4155 - val_auc: 0.5979 - val_f1_score: 0.4133
Epoch 2/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0597 - accuracy: 0.4232 - auc: 0.6096 - f1_score: 0.4180 - val_loss: 1.0550 - val_accuracy: 0.4297 - val_auc: 0.6197 - val_f1_score: 0.4082
Epoch 3/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0497 - accuracy: 0.4352 - auc: 0.6246 - f1_score: 0.4189 - val_loss: 1.0502 - val_accuracy: 0.4373 - val_auc: 0.6270 - val_f1_score: 0.4162
Epoch 4/50
188/188 [==============================] - 19s 101ms/step - loss: 1.0439 - accuracy: 0.4413 - auc: 0.6326 - f1_score: 0.4235 - val_loss: 1.0494 - val_accuracy: 0.4408 - val_auc: 0.6280 - val_f1_score: 0.4219
Epoch 5/50
188/188 [==============================] - 20s 107ms/step - loss: 1.0394 - accuracy: 0.4464 - auc: 0.6387 - f1_score: 0.4321 - val_loss: 1.0489 - val_accuracy: 0.4463 - val_auc: 0.6292 - val_f1_score: 0.4359
Epoch 6/50
188/188 [==============================] - 21s 113ms/step - loss: 1.0353 - accuracy: 0.4512 - auc: 0.6440 - f1_score: 0.4364 - val_loss: 1.0489 - val_accuracy: 0.4343 - val_auc: 0.6270 - val_f1_score: 0.4182
Epoch 7/50
188/188 [==============================] - 21s 110ms/step - loss: 1.0328 - accuracy: 0.4538 - auc: 0.6467 - f1_score: 0.4393 - val_loss: 1.0501 - val_accuracy: 0.4368 - val_auc: 0.6274 - val_f1_score: 0.4222
Epoch 8/50
188/188 [==============================] - 20s 106ms/step - loss: 1.0295 - accuracy: 0.4568 - auc: 0.6507 - f1_score: 0.4427 - val_loss: 1.0493 - val_accuracy: 0.4418 - val_auc: 0.6297 - val_f1_score: 0.4335
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
147/147 [==============================] - 21s 125ms/step - loss: 1.0777 - accuracy: 0.3938 - auc: 0.5798 - f1_score: 0.4332 - val_loss: 1.0619 - val_accuracy: 0.4209 - val_auc: 0.6059 - val_f1_score: 0.3650
Epoch 2/50
147/147 [==============================] - 18s 124ms/step - loss: 1.0532 - accuracy: 0.4310 - auc: 0.6198 - f1_score: 0.4129 - val_loss: 1.0565 - val_accuracy: 0.4265 - val_auc: 0.6149 - val_f1_score: 0.3995
Epoch 3/50
147/147 [==============================] - 19s 127ms/step - loss: 1.0462 - accuracy: 0.4406 - auc: 0.6303 - f1_score: 0.4286 - val_loss: 1.0530 - val_accuracy: 0.4331 - val_auc: 0.6216 - val_f1_score: 0.4096
Epoch 4/50
147/147 [==============================] - 22s 146ms/step - loss: 1.0406 - accuracy: 0.4482 - auc: 0.6377 - f1_score: 0.4399 - val_loss: 1.0506 - val_accuracy: 0.4359 - val_auc: 0.6248 - val_f1_score: 0.4134
Epoch 5/50
147/147 [==============================] - 19s 132ms/step - loss: 1.0365 - accuracy: 0.4511 - auc: 0.6422 - f1_score: 0.4422 - val_loss: 1.0512 - val_accuracy: 0.4356 - val_auc: 0.6265 - val_f1_score: 0.4173
Epoch 6/50
147/147 [==============================] - 21s 141ms/step - loss: 1.0330 - accuracy: 0.4559 - auc: 0.6473 - f1_score: 0.4484 - val_loss: 1.0489 - val_accuracy: 0.4368 - val_auc: 0.6281 - val_f1_score: 0.4220
Epoch 7/50
147/147 [==============================] - 19s 131ms/step - loss: 1.0293 - accuracy: 0.4599 - auc: 0.6519 - f1_score: 0.4542 - val_loss: 1.0489 - val_accuracy: 0.4433 - val_auc: 0.6294 - val_f1_score: 0.4344
Epoch 8/50
147/147 [==============================] - 19s 131ms/step - loss: 1.0272 - accuracy: 0.4625 - auc: 0.6547 - f1_score: 0.4574 - val_loss: 1.0500 - val_accuracy: 0.4376 - val_auc: 0.6278 - val_f1_score: 0.4289
Epoch 9/50
147/147 [==============================] - 17s 113ms/step - loss: 1.0230 - accuracy: 0.4679 - auc: 0.6591 - f1_score: 0.4639 - val_loss: 1.0485 - val_accuracy: 0.4440 - val_auc: 0.6309 - val_f1_score: 0.4378
Epoch 10/50
147/147 [==============================] - 18s 124ms/step - loss: 1.0203 - accuracy: 0.4705 - auc: 0.6626 - f1_score: 0.4672 - val_loss: 1.0512 - val_accuracy: 0.4381 - val_auc: 0.6283 - val_f1_score: 0.4317
Epoch 11/50
147/147 [==============================] - 20s 136ms/step - loss: 1.0171 - accuracy: 0.4762 - auc: 0.6668 - f1_score: 0.4732 - val_loss: 1.0496 - val_accuracy: 0.4429 - val_auc: 0.6293 - val_f1_score: 0.4406
Epoch 12/50
147/147 [==============================] - 18s 120ms/step - loss: 1.0128 - accuracy: 0.4787 - auc: 0.6708 - f1_score: 0.4764 - val_loss: 1.0534 - val_accuracy: 0.4335 - val_auc: 0.6274 - val_f1_score: 0.4308
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
74/74 [==============================] - 21s 245ms/step - loss: 1.0715 - accuracy: 0.4061 - auc: 0.5895 - f1_score: 0.4525 - val_loss: 1.0560 - val_accuracy: 0.4278 - val_auc: 0.6171 - val_f1_score: 0.4215
Epoch 2/50
74/74 [==============================] - 15s 199ms/step - loss: 1.0514 - accuracy: 0.4345 - auc: 0.6220 - f1_score: 0.4320 - val_loss: 1.0518 - val_accuracy: 0.4381 - val_auc: 0.6235 - val_f1_score: 0.4361
Epoch 3/50
74/74 [==============================] - 15s 203ms/step - loss: 1.0461 - accuracy: 0.4410 - auc: 0.6303 - f1_score: 0.4379 - val_loss: 1.0506 - val_accuracy: 0.4366 - val_auc: 0.6251 - val_f1_score: 0.4338
Epoch 4/50
74/74 [==============================] - 16s 223ms/step - loss: 1.0434 - accuracy: 0.4448 - auc: 0.6338 - f1_score: 0.4430 - val_loss: 1.0487 - val_accuracy: 0.4351 - val_auc: 0.6268 - val_f1_score: 0.4344
Epoch 5/50
74/74 [==============================] - 15s 204ms/step - loss: 1.0403 - accuracy: 0.4491 - auc: 0.6383 - f1_score: 0.4475 - val_loss: 1.0479 - val_accuracy: 0.4409 - val_auc: 0.6290 - val_f1_score: 0.4341
Epoch 6/50
74/74 [==============================] - 14s 190ms/step - loss: 1.0352 - accuracy: 0.4539 - auc: 0.6444 - f1_score: 0.4522 - val_loss: 1.0471 - val_accuracy: 0.4397 - val_auc: 0.6297 - val_f1_score: 0.4391
Epoch 7/50
74/74 [==============================] - 14s 185ms/step - loss: 1.0332 - accuracy: 0.4577 - auc: 0.6475 - f1_score: 0.4550 - val_loss: 1.0461 - val_accuracy: 0.4442 - val_auc: 0.6321 - val_f1_score: 0.4433
Epoch 8/50
74/74 [==============================] - 14s 194ms/step - loss: 1.0309 - accuracy: 0.4611 - auc: 0.6501 - f1_score: 0.4590 - val_loss: 1.0472 - val_accuracy: 0.4420 - val_auc: 0.6311 - val_f1_score: 0.4385
Epoch 9/50
74/74 [==============================] - 15s 199ms/step - loss: 1.0274 - accuracy: 0.4642 - auc: 0.6542 - f1_score: 0.4623 - val_loss: 1.0461 - val_accuracy: 0.4453 - val_auc: 0.6320 - val_f1_score: 0.4456
Epoch 10/50
74/74 [==============================] - 15s 201ms/step - loss: 1.0251 - accuracy: 0.4687 - auc: 0.6572 - f1_score: 0.4669 - val_loss: 1.0485 - val_accuracy: 0.4440 - val_auc: 0.6307 - val_f1_score: 0.4411
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 9, 7)
Test shape: (10292, 9, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 9, 7)]       0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 9, 100)       10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 9, 100)       10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 900)          0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 900)          0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 1808)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          904500      ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,044,043
Trainable params: 1,044,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
37/37 [==============================] - 16s 346ms/step - loss: 1.0914 - accuracy: 0.3702 - auc: 0.5444 - f1_score: 0.4270 - val_loss: 1.0764 - val_accuracy: 0.4055 - val_auc: 0.5880 - val_f1_score: 0.3882
Epoch 2/50
37/37 [==============================] - 13s 342ms/step - loss: 1.0693 - accuracy: 0.4153 - auc: 0.5985 - f1_score: 0.4068 - val_loss: 1.0645 - val_accuracy: 0.4191 - val_auc: 0.6064 - val_f1_score: 0.4067
Epoch 3/50
37/37 [==============================] - 12s 328ms/step - loss: 1.0582 - accuracy: 0.4291 - auc: 0.6151 - f1_score: 0.4196 - val_loss: 1.0582 - val_accuracy: 0.4249 - val_auc: 0.6141 - val_f1_score: 0.3989
Epoch 4/50
37/37 [==============================] - 12s 314ms/step - loss: 1.0518 - accuracy: 0.4363 - auc: 0.6234 - f1_score: 0.4250 - val_loss: 1.0547 - val_accuracy: 0.4344 - val_auc: 0.6190 - val_f1_score: 0.4162
Epoch 5/50
37/37 [==============================] - 13s 359ms/step - loss: 1.0474 - accuracy: 0.4389 - auc: 0.6288 - f1_score: 0.4273 - val_loss: 1.0530 - val_accuracy: 0.4373 - val_auc: 0.6221 - val_f1_score: 0.4218
Epoch 6/50
37/37 [==============================] - 12s 329ms/step - loss: 1.0442 - accuracy: 0.4476 - auc: 0.6343 - f1_score: 0.4351 - val_loss: 1.0511 - val_accuracy: 0.4365 - val_auc: 0.6244 - val_f1_score: 0.4235
Epoch 7/50
37/37 [==============================] - 11s 309ms/step - loss: 1.0413 - accuracy: 0.4475 - auc: 0.6377 - f1_score: 0.4363 - val_loss: 1.0503 - val_accuracy: 0.4359 - val_auc: 0.6252 - val_f1_score: 0.4237
Epoch 8/50
37/37 [==============================] - 11s 310ms/step - loss: 1.0397 - accuracy: 0.4499 - auc: 0.6398 - f1_score: 0.4381 - val_loss: 1.0494 - val_accuracy: 0.4390 - val_auc: 0.6269 - val_f1_score: 0.4264
Epoch 9/50
37/37 [==============================] - 13s 355ms/step - loss: 1.0379 - accuracy: 0.4532 - auc: 0.6428 - f1_score: 0.4417 - val_loss: 1.0488 - val_accuracy: 0.4423 - val_auc: 0.6282 - val_f1_score: 0.4315
Epoch 10/50
37/37 [==============================] - 12s 326ms/step - loss: 1.0346 - accuracy: 0.4560 - auc: 0.6461 - f1_score: 0.4463 - val_loss: 1.0476 - val_accuracy: 0.4406 - val_auc: 0.6293 - val_f1_score: 0.4301
Epoch 11/50
37/37 [==============================] - 12s 327ms/step - loss: 1.0322 - accuracy: 0.4597 - auc: 0.6496 - f1_score: 0.4518 - val_loss: 1.0488 - val_accuracy: 0.4405 - val_auc: 0.6292 - val_f1_score: 0.4309
Epoch 12/50
37/37 [==============================] - 12s 316ms/step - loss: 1.0299 - accuracy: 0.4604 - auc: 0.6515 - f1_score: 0.4519 - val_loss: 1.0480 - val_accuracy: 0.4436 - val_auc: 0.6300 - val_f1_score: 0.4306
Epoch 13/50
37/37 [==============================] - 12s 324ms/step - loss: 1.0288 - accuracy: 0.4634 - auc: 0.6533 - f1_score: 0.4541 - val_loss: 1.0472 - val_accuracy: 0.4422 - val_auc: 0.6302 - val_f1_score: 0.4334
Epoch 14/50
37/37 [==============================] - 12s 333ms/step - loss: 1.0263 - accuracy: 0.4671 - auc: 0.6566 - f1_score: 0.4598 - val_loss: 1.0463 - val_accuracy: 0.4475 - val_auc: 0.6319 - val_f1_score: 0.4410
Epoch 15/50
37/37 [==============================] - 12s 337ms/step - loss: 1.0251 - accuracy: 0.4668 - auc: 0.6579 - f1_score: 0.4593 - val_loss: 1.0470 - val_accuracy: 0.4429 - val_auc: 0.6319 - val_f1_score: 0.4350
Epoch 16/50
37/37 [==============================] - 12s 318ms/step - loss: 1.0225 - accuracy: 0.4690 - auc: 0.6604 - f1_score: 0.4630 - val_loss: 1.0462 - val_accuracy: 0.4464 - val_auc: 0.6323 - val_f1_score: 0.4403
Epoch 17/50
37/37 [==============================] - 14s 371ms/step - loss: 1.0197 - accuracy: 0.4741 - auc: 0.6642 - f1_score: 0.4688 - val_loss: 1.0472 - val_accuracy: 0.4454 - val_auc: 0.6326 - val_f1_score: 0.4422
Epoch 18/50
37/37 [==============================] - 13s 345ms/step - loss: 1.0184 - accuracy: 0.4759 - auc: 0.6658 - f1_score: 0.4713 - val_loss: 1.0467 - val_accuracy: 0.4442 - val_auc: 0.6328 - val_f1_score: 0.4407
Epoch 19/50
37/37 [==============================] - 12s 316ms/step - loss: 1.0162 - accuracy: 0.4790 - auc: 0.6684 - f1_score: 0.4744 - val_loss: 1.0481 - val_accuracy: 0.4402 - val_auc: 0.6307 - val_f1_score: 0.4356
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
4692/4692 [==============================] - 117s 24ms/step - loss: 1.0617 - accuracy: 0.4217 - auc: 0.6066 - f1_score: 0.4583 - val_loss: 1.0544 - val_accuracy: 0.4345 - val_auc: 0.6204 - val_f1_score: 0.4273
Epoch 2/50
4692/4692 [==============================] - 111s 24ms/step - loss: 1.0468 - accuracy: 0.4382 - auc: 0.6287 - f1_score: 0.4380 - val_loss: 1.0520 - val_accuracy: 0.4362 - val_auc: 0.6239 - val_f1_score: 0.4339
Epoch 3/50
4692/4692 [==============================] - 112s 24ms/step - loss: 1.0410 - accuracy: 0.4475 - auc: 0.6375 - f1_score: 0.4468 - val_loss: 1.0512 - val_accuracy: 0.4398 - val_auc: 0.6259 - val_f1_score: 0.4352
Epoch 4/50
4692/4692 [==============================] - 110s 23ms/step - loss: 1.0359 - accuracy: 0.4526 - auc: 0.6435 - f1_score: 0.4508 - val_loss: 1.0483 - val_accuracy: 0.4397 - val_auc: 0.6281 - val_f1_score: 0.4351
Epoch 5/50
4692/4692 [==============================] - 108s 23ms/step - loss: 1.0317 - accuracy: 0.4572 - auc: 0.6486 - f1_score: 0.4558 - val_loss: 1.0492 - val_accuracy: 0.4399 - val_auc: 0.6279 - val_f1_score: 0.4350
Epoch 6/50
4692/4692 [==============================] - 113s 24ms/step - loss: 1.0264 - accuracy: 0.4615 - auc: 0.6543 - f1_score: 0.4601 - val_loss: 1.0501 - val_accuracy: 0.4384 - val_auc: 0.6257 - val_f1_score: 0.4382
Epoch 7/50
4692/4692 [==============================] - 109s 23ms/step - loss: 1.0210 - accuracy: 0.4693 - auc: 0.6608 - f1_score: 0.4681 - val_loss: 1.0510 - val_accuracy: 0.4374 - val_auc: 0.6260 - val_f1_score: 0.4346
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.448821</td>
      <td>0.441714</td>
      <td>0.490696</td>
      <td>0.487371</td>
      <td>0.682879</td>
      <td>0.678741</td>
      <td>0.487946</td>
      <td>0.483883</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
2346/2346 [==============================] - 70s 29ms/step - loss: 1.0621 - accuracy: 0.4179 - auc: 0.6059 - f1_score: 0.4509 - val_loss: 1.0565 - val_accuracy: 0.4302 - val_auc: 0.6199 - val_f1_score: 0.4152
Epoch 2/50
2346/2346 [==============================] - 66s 28ms/step - loss: 1.0466 - accuracy: 0.4425 - auc: 0.6299 - f1_score: 0.4335 - val_loss: 1.0520 - val_accuracy: 0.4251 - val_auc: 0.6211 - val_f1_score: 0.4215
Epoch 3/50
2346/2346 [==============================] - 62s 26ms/step - loss: 1.0392 - accuracy: 0.4481 - auc: 0.6394 - f1_score: 0.4424 - val_loss: 1.0522 - val_accuracy: 0.4382 - val_auc: 0.6253 - val_f1_score: 0.4333
Epoch 4/50
2346/2346 [==============================] - 56s 24ms/step - loss: 1.0361 - accuracy: 0.4506 - auc: 0.6429 - f1_score: 0.4446 - val_loss: 1.0487 - val_accuracy: 0.4361 - val_auc: 0.6266 - val_f1_score: 0.4333
Epoch 5/50
2346/2346 [==============================] - 56s 24ms/step - loss: 1.0324 - accuracy: 0.4574 - auc: 0.6479 - f1_score: 0.4523 - val_loss: 1.0498 - val_accuracy: 0.4418 - val_auc: 0.6272 - val_f1_score: 0.4403
Epoch 6/50
2346/2346 [==============================] - 55s 24ms/step - loss: 1.0280 - accuracy: 0.4628 - auc: 0.6529 - f1_score: 0.4592 - val_loss: 1.0491 - val_accuracy: 0.4400 - val_auc: 0.6287 - val_f1_score: 0.4389
Epoch 7/50
2346/2346 [==============================] - 56s 24ms/step - loss: 1.0233 - accuracy: 0.4666 - auc: 0.6581 - f1_score: 0.4641 - val_loss: 1.0509 - val_accuracy: 0.4373 - val_auc: 0.6251 - val_f1_score: 0.4350
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.448821</td>
      <td>0.441714</td>
      <td>0.490696</td>
      <td>0.487371</td>
      <td>0.682879</td>
      <td>0.678741</td>
      <td>0.487946</td>
      <td>0.483883</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.443511</td>
      <td>0.436343</td>
      <td>0.484538</td>
      <td>0.480040</td>
      <td>0.674689</td>
      <td>0.669694</td>
      <td>0.483001</td>
      <td>0.478206</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
587/587 [==============================] - 29s 45ms/step - loss: 1.0637 - accuracy: 0.4174 - auc: 0.6031 - f1_score: 0.4491 - val_loss: 1.0537 - val_accuracy: 0.4338 - val_auc: 0.6210 - val_f1_score: 0.4320
Epoch 2/50
587/587 [==============================] - 25s 43ms/step - loss: 1.0461 - accuracy: 0.4421 - auc: 0.6308 - f1_score: 0.4367 - val_loss: 1.0521 - val_accuracy: 0.4423 - val_auc: 0.6252 - val_f1_score: 0.4387
Epoch 3/50
587/587 [==============================] - 27s 46ms/step - loss: 1.0400 - accuracy: 0.4481 - auc: 0.6383 - f1_score: 0.4445 - val_loss: 1.0506 - val_accuracy: 0.4416 - val_auc: 0.6271 - val_f1_score: 0.4403
Epoch 4/50
587/587 [==============================] - 27s 46ms/step - loss: 1.0352 - accuracy: 0.4536 - auc: 0.6439 - f1_score: 0.4507 - val_loss: 1.0491 - val_accuracy: 0.4438 - val_auc: 0.6303 - val_f1_score: 0.4415
Epoch 5/50
587/587 [==============================] - 27s 46ms/step - loss: 1.0301 - accuracy: 0.4599 - auc: 0.6504 - f1_score: 0.4558 - val_loss: 1.0500 - val_accuracy: 0.4406 - val_auc: 0.6299 - val_f1_score: 0.4377
Epoch 6/50
587/587 [==============================] - 26s 44ms/step - loss: 1.0262 - accuracy: 0.4644 - auc: 0.6553 - f1_score: 0.4617 - val_loss: 1.0497 - val_accuracy: 0.4384 - val_auc: 0.6282 - val_f1_score: 0.4372
Epoch 7/50
587/587 [==============================] - 26s 44ms/step - loss: 1.0230 - accuracy: 0.4673 - auc: 0.6587 - f1_score: 0.4645 - val_loss: 1.0487 - val_accuracy: 0.4421 - val_auc: 0.6308 - val_f1_score: 0.4409
Epoch 8/50
587/587 [==============================] - 25s 43ms/step - loss: 1.0188 - accuracy: 0.4726 - auc: 0.6633 - f1_score: 0.4710 - val_loss: 1.0493 - val_accuracy: 0.4446 - val_auc: 0.6308 - val_f1_score: 0.4406
Epoch 9/50
587/587 [==============================] - 26s 44ms/step - loss: 1.0139 - accuracy: 0.4792 - auc: 0.6692 - f1_score: 0.4774 - val_loss: 1.0498 - val_accuracy: 0.4373 - val_auc: 0.6292 - val_f1_score: 0.4364
Epoch 10/50
587/587 [==============================] - 26s 44ms/step - loss: 1.0093 - accuracy: 0.4825 - auc: 0.6734 - f1_score: 0.4810 - val_loss: 1.0527 - val_accuracy: 0.4451 - val_auc: 0.6305 - val_f1_score: 0.4449
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.448821</td>
      <td>0.441714</td>
      <td>0.490696</td>
      <td>0.487371</td>
      <td>0.682879</td>
      <td>0.678741</td>
      <td>0.487946</td>
      <td>0.483883</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.443511</td>
      <td>0.436343</td>
      <td>0.484538</td>
      <td>0.480040</td>
      <td>0.674689</td>
      <td>0.669694</td>
      <td>0.483001</td>
      <td>0.478206</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.459900</td>
      <td>0.451976</td>
      <td>0.511004</td>
      <td>0.505062</td>
      <td>0.700253</td>
      <td>0.693634</td>
      <td>0.511145</td>
      <td>0.504673</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
188/188 [==============================] - 22s 105ms/step - loss: 1.0841 - accuracy: 0.3700 - auc: 0.5683 - f1_score: 0.4450 - val_loss: 1.0717 - val_accuracy: 0.3778 - val_auc: 0.5892 - val_f1_score: 0.3202
Epoch 2/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0670 - accuracy: 0.3826 - auc: 0.5981 - f1_score: 0.3450 - val_loss: 1.0665 - val_accuracy: 0.3765 - val_auc: 0.6002 - val_f1_score: 0.3600
Epoch 3/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0582 - accuracy: 0.4004 - auc: 0.6125 - f1_score: 0.3956 - val_loss: 1.0607 - val_accuracy: 0.4303 - val_auc: 0.6102 - val_f1_score: 0.3923
Epoch 4/50
188/188 [==============================] - 19s 99ms/step - loss: 1.0505 - accuracy: 0.4353 - auc: 0.6233 - f1_score: 0.4198 - val_loss: 1.0562 - val_accuracy: 0.4315 - val_auc: 0.6163 - val_f1_score: 0.3852
Epoch 5/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0442 - accuracy: 0.4453 - auc: 0.6313 - f1_score: 0.4248 - val_loss: 1.0540 - val_accuracy: 0.4388 - val_auc: 0.6197 - val_f1_score: 0.4087
Epoch 6/50
188/188 [==============================] - 20s 105ms/step - loss: 1.0383 - accuracy: 0.4513 - auc: 0.6389 - f1_score: 0.4311 - val_loss: 1.0536 - val_accuracy: 0.4399 - val_auc: 0.6223 - val_f1_score: 0.4096
Epoch 7/50
188/188 [==============================] - 19s 103ms/step - loss: 1.0337 - accuracy: 0.4545 - auc: 0.6458 - f1_score: 0.4369 - val_loss: 1.0508 - val_accuracy: 0.4450 - val_auc: 0.6276 - val_f1_score: 0.4128
Epoch 8/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0290 - accuracy: 0.4607 - auc: 0.6515 - f1_score: 0.4459 - val_loss: 1.0494 - val_accuracy: 0.4422 - val_auc: 0.6278 - val_f1_score: 0.4220
Epoch 9/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0260 - accuracy: 0.4648 - auc: 0.6558 - f1_score: 0.4555 - val_loss: 1.0495 - val_accuracy: 0.4438 - val_auc: 0.6285 - val_f1_score: 0.4221
Epoch 10/50
188/188 [==============================] - 19s 100ms/step - loss: 1.0213 - accuracy: 0.4700 - auc: 0.6610 - f1_score: 0.4600 - val_loss: 1.0512 - val_accuracy: 0.4394 - val_auc: 0.6275 - val_f1_score: 0.4244
Epoch 11/50
188/188 [==============================] - 19s 102ms/step - loss: 1.0173 - accuracy: 0.4731 - auc: 0.6653 - f1_score: 0.4650 - val_loss: 1.0510 - val_accuracy: 0.4416 - val_auc: 0.6282 - val_f1_score: 0.4293
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.448821</td>
      <td>0.441714</td>
      <td>0.490696</td>
      <td>0.487371</td>
      <td>0.682879</td>
      <td>0.678741</td>
      <td>0.487946</td>
      <td>0.483883</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.443511</td>
      <td>0.436343</td>
      <td>0.484538</td>
      <td>0.480040</td>
      <td>0.674689</td>
      <td>0.669694</td>
      <td>0.483001</td>
      <td>0.478206</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.459900</td>
      <td>0.451976</td>
      <td>0.511004</td>
      <td>0.505062</td>
      <td>0.700253</td>
      <td>0.693634</td>
      <td>0.511145</td>
      <td>0.504673</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>197</th>
      <td>0.449845</td>
      <td>0.442440</td>
      <td>0.501510</td>
      <td>0.497780</td>
      <td>0.690488</td>
      <td>0.685359</td>
      <td>0.490910</td>
      <td>0.485880</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
147/147 [==============================] - 20s 124ms/step - loss: 1.0701 - accuracy: 0.4071 - auc: 0.5927 - f1_score: 0.4456 - val_loss: 1.0597 - val_accuracy: 0.4192 - val_auc: 0.6106 - val_f1_score: 0.3781
Epoch 2/50
147/147 [==============================] - 17s 116ms/step - loss: 1.0513 - accuracy: 0.4345 - auc: 0.6227 - f1_score: 0.4189 - val_loss: 1.0553 - val_accuracy: 0.4281 - val_auc: 0.6187 - val_f1_score: 0.4006
Epoch 3/50
147/147 [==============================] - 17s 115ms/step - loss: 1.0448 - accuracy: 0.4420 - auc: 0.6323 - f1_score: 0.4304 - val_loss: 1.0540 - val_accuracy: 0.4319 - val_auc: 0.6199 - val_f1_score: 0.4197
Epoch 4/50
147/147 [==============================] - 17s 113ms/step - loss: 1.0385 - accuracy: 0.4483 - auc: 0.6398 - f1_score: 0.4403 - val_loss: 1.0515 - val_accuracy: 0.4368 - val_auc: 0.6244 - val_f1_score: 0.4299
Epoch 5/50
147/147 [==============================] - 17s 113ms/step - loss: 1.0367 - accuracy: 0.4512 - auc: 0.6424 - f1_score: 0.4444 - val_loss: 1.0504 - val_accuracy: 0.4400 - val_auc: 0.6258 - val_f1_score: 0.4358
Epoch 6/50
147/147 [==============================] - 18s 120ms/step - loss: 1.0317 - accuracy: 0.4589 - auc: 0.6490 - f1_score: 0.4537 - val_loss: 1.0496 - val_accuracy: 0.4416 - val_auc: 0.6282 - val_f1_score: 0.4364
Epoch 7/50
147/147 [==============================] - 17s 116ms/step - loss: 1.0265 - accuracy: 0.4645 - auc: 0.6555 - f1_score: 0.4609 - val_loss: 1.0516 - val_accuracy: 0.4442 - val_auc: 0.6262 - val_f1_score: 0.4355
Epoch 8/50
147/147 [==============================] - 17s 113ms/step - loss: 1.0233 - accuracy: 0.4681 - auc: 0.6594 - f1_score: 0.4643 - val_loss: 1.0523 - val_accuracy: 0.4387 - val_auc: 0.6255 - val_f1_score: 0.4325
Epoch 9/50
147/147 [==============================] - 17s 113ms/step - loss: 1.0201 - accuracy: 0.4745 - auc: 0.6630 - f1_score: 0.4717 - val_loss: 1.0505 - val_accuracy: 0.4369 - val_auc: 0.6276 - val_f1_score: 0.4311
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.448821</td>
      <td>0.441714</td>
      <td>0.490696</td>
      <td>0.487371</td>
      <td>0.682879</td>
      <td>0.678741</td>
      <td>0.487946</td>
      <td>0.483883</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.443511</td>
      <td>0.436343</td>
      <td>0.484538</td>
      <td>0.480040</td>
      <td>0.674689</td>
      <td>0.669694</td>
      <td>0.483001</td>
      <td>0.478206</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.459900</td>
      <td>0.451976</td>
      <td>0.511004</td>
      <td>0.505062</td>
      <td>0.700253</td>
      <td>0.693634</td>
      <td>0.511145</td>
      <td>0.504673</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>197</th>
      <td>0.449845</td>
      <td>0.442440</td>
      <td>0.501510</td>
      <td>0.497780</td>
      <td>0.690488</td>
      <td>0.685359</td>
      <td>0.490910</td>
      <td>0.485880</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>198</th>
      <td>0.450055</td>
      <td>0.442938</td>
      <td>0.497825</td>
      <td>0.494075</td>
      <td>0.686454</td>
      <td>0.681993</td>
      <td>0.493459</td>
      <td>0.488841</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
74/74 [==============================] - 17s 196ms/step - loss: 1.0857 - accuracy: 0.3809 - auc: 0.5550 - f1_score: 0.4414 - val_loss: 1.0718 - val_accuracy: 0.4206 - val_auc: 0.5938 - val_f1_score: 0.4183
Epoch 2/50
74/74 [==============================] - 18s 246ms/step - loss: 1.0642 - accuracy: 0.4290 - auc: 0.6056 - f1_score: 0.4272 - val_loss: 1.0591 - val_accuracy: 0.4306 - val_auc: 0.6151 - val_f1_score: 0.4280
Epoch 3/50
74/74 [==============================] - 17s 228ms/step - loss: 1.0520 - accuracy: 0.4395 - auc: 0.6253 - f1_score: 0.4389 - val_loss: 1.0521 - val_accuracy: 0.4343 - val_auc: 0.6227 - val_f1_score: 0.4315
Epoch 4/50
74/74 [==============================] - 16s 210ms/step - loss: 1.0441 - accuracy: 0.4473 - auc: 0.6346 - f1_score: 0.4462 - val_loss: 1.0499 - val_accuracy: 0.4393 - val_auc: 0.6260 - val_f1_score: 0.4355
Epoch 5/50
74/74 [==============================] - 15s 199ms/step - loss: 1.0395 - accuracy: 0.4502 - auc: 0.6405 - f1_score: 0.4480 - val_loss: 1.0483 - val_accuracy: 0.4433 - val_auc: 0.6287 - val_f1_score: 0.4373
Epoch 6/50
74/74 [==============================] - 15s 199ms/step - loss: 1.0366 - accuracy: 0.4561 - auc: 0.6445 - f1_score: 0.4540 - val_loss: 1.0468 - val_accuracy: 0.4429 - val_auc: 0.6299 - val_f1_score: 0.4408
Epoch 7/50
74/74 [==============================] - 15s 200ms/step - loss: 1.0318 - accuracy: 0.4632 - auc: 0.6501 - f1_score: 0.4618 - val_loss: 1.0474 - val_accuracy: 0.4417 - val_auc: 0.6303 - val_f1_score: 0.4407
Epoch 8/50
74/74 [==============================] - 16s 213ms/step - loss: 1.0287 - accuracy: 0.4670 - auc: 0.6541 - f1_score: 0.4656 - val_loss: 1.0472 - val_accuracy: 0.4415 - val_auc: 0.6307 - val_f1_score: 0.4398
Epoch 9/50
74/74 [==============================] - 15s 198ms/step - loss: 1.0256 - accuracy: 0.4715 - auc: 0.6579 - f1_score: 0.4698 - val_loss: 1.0481 - val_accuracy: 0.4408 - val_auc: 0.6301 - val_f1_score: 0.4413
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.448821</td>
      <td>0.441714</td>
      <td>0.490696</td>
      <td>0.487371</td>
      <td>0.682879</td>
      <td>0.678741</td>
      <td>0.487946</td>
      <td>0.483883</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.443511</td>
      <td>0.436343</td>
      <td>0.484538</td>
      <td>0.480040</td>
      <td>0.674689</td>
      <td>0.669694</td>
      <td>0.483001</td>
      <td>0.478206</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.459900</td>
      <td>0.451976</td>
      <td>0.511004</td>
      <td>0.505062</td>
      <td>0.700253</td>
      <td>0.693634</td>
      <td>0.511145</td>
      <td>0.504673</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>197</th>
      <td>0.449845</td>
      <td>0.442440</td>
      <td>0.501510</td>
      <td>0.497780</td>
      <td>0.690488</td>
      <td>0.685359</td>
      <td>0.490910</td>
      <td>0.485880</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>198</th>
      <td>0.450055</td>
      <td>0.442938</td>
      <td>0.497825</td>
      <td>0.494075</td>
      <td>0.686454</td>
      <td>0.681993</td>
      <td>0.493459</td>
      <td>0.488841</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>199</th>
      <td>0.446104</td>
      <td>0.439218</td>
      <td>0.489334</td>
      <td>0.486293</td>
      <td>0.676767</td>
      <td>0.673071</td>
      <td>0.489919</td>
      <td>0.486513</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
37/37 [==============================] - 16s 365ms/step - loss: 1.0839 - accuracy: 0.3832 - auc: 0.5671 - f1_score: 0.4423 - val_loss: 1.0659 - val_accuracy: 0.4182 - val_auc: 0.6016 - val_f1_score: 0.4021
Epoch 2/50
37/37 [==============================] - 13s 347ms/step - loss: 1.0592 - accuracy: 0.4237 - auc: 0.6109 - f1_score: 0.4114 - val_loss: 1.0595 - val_accuracy: 0.4308 - val_auc: 0.6124 - val_f1_score: 0.4074
Epoch 3/50
37/37 [==============================] - 13s 348ms/step - loss: 1.0521 - accuracy: 0.4328 - auc: 0.6215 - f1_score: 0.4221 - val_loss: 1.0569 - val_accuracy: 0.4323 - val_auc: 0.6174 - val_f1_score: 0.4189
Epoch 4/50
37/37 [==============================] - 13s 347ms/step - loss: 1.0474 - accuracy: 0.4375 - auc: 0.6276 - f1_score: 0.4293 - val_loss: 1.0562 - val_accuracy: 0.4313 - val_auc: 0.6196 - val_f1_score: 0.4128
Epoch 5/50
37/37 [==============================] - 13s 350ms/step - loss: 1.0449 - accuracy: 0.4401 - auc: 0.6315 - f1_score: 0.4326 - val_loss: 1.0541 - val_accuracy: 0.4337 - val_auc: 0.6217 - val_f1_score: 0.4201
Epoch 6/50
37/37 [==============================] - 15s 398ms/step - loss: 1.0421 - accuracy: 0.4431 - auc: 0.6347 - f1_score: 0.4356 - val_loss: 1.0524 - val_accuracy: 0.4381 - val_auc: 0.6245 - val_f1_score: 0.4256
Epoch 7/50
37/37 [==============================] - 14s 370ms/step - loss: 1.0385 - accuracy: 0.4476 - auc: 0.6394 - f1_score: 0.4419 - val_loss: 1.0516 - val_accuracy: 0.4375 - val_auc: 0.6257 - val_f1_score: 0.4272
Epoch 8/50
37/37 [==============================] - 13s 358ms/step - loss: 1.0365 - accuracy: 0.4529 - auc: 0.6428 - f1_score: 0.4480 - val_loss: 1.0519 - val_accuracy: 0.4379 - val_auc: 0.6253 - val_f1_score: 0.4290
Epoch 9/50
37/37 [==============================] - 14s 372ms/step - loss: 1.0344 - accuracy: 0.4560 - auc: 0.6453 - f1_score: 0.4508 - val_loss: 1.0509 - val_accuracy: 0.4374 - val_auc: 0.6273 - val_f1_score: 0.4248
Epoch 10/50
37/37 [==============================] - 15s 406ms/step - loss: 1.0315 - accuracy: 0.4580 - auc: 0.6486 - f1_score: 0.4536 - val_loss: 1.0495 - val_accuracy: 0.4420 - val_auc: 0.6292 - val_f1_score: 0.4346
Epoch 11/50
37/37 [==============================] - 15s 394ms/step - loss: 1.0290 - accuracy: 0.4623 - auc: 0.6525 - f1_score: 0.4578 - val_loss: 1.0497 - val_accuracy: 0.4417 - val_auc: 0.6286 - val_f1_score: 0.4331
Epoch 12/50
37/37 [==============================] - 14s 371ms/step - loss: 1.0260 - accuracy: 0.4635 - auc: 0.6550 - f1_score: 0.4597 - val_loss: 1.0496 - val_accuracy: 0.4422 - val_auc: 0.6291 - val_f1_score: 0.4345
Epoch 13/50
37/37 [==============================] - 15s 397ms/step - loss: 1.0245 - accuracy: 0.4671 - auc: 0.6574 - f1_score: 0.4633 - val_loss: 1.0518 - val_accuracy: 0.4399 - val_auc: 0.6285 - val_f1_score: 0.4310
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
      <th>Partidos Considerados</th>
      <th>RNN</th>
      <th>N_Features_m</th>
      <th>N_Features_h</th>
      <th>Batch_size</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>187</th>
      <td>0.441131</td>
      <td>0.434578</td>
      <td>0.473245</td>
      <td>0.472160</td>
      <td>0.666454</td>
      <td>0.664029</td>
      <td>0.469595</td>
      <td>0.467718</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>188</th>
      <td>0.442311</td>
      <td>0.435448</td>
      <td>0.478902</td>
      <td>0.477179</td>
      <td>0.672710</td>
      <td>0.669219</td>
      <td>0.477678</td>
      <td>0.475327</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>189</th>
      <td>0.446093</td>
      <td>0.439337</td>
      <td>0.480308</td>
      <td>0.478325</td>
      <td>0.672880</td>
      <td>0.669636</td>
      <td>0.479657</td>
      <td>0.477065</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>190</th>
      <td>0.443027</td>
      <td>0.436428</td>
      <td>0.474978</td>
      <td>0.473915</td>
      <td>0.667663</td>
      <td>0.665021</td>
      <td>0.464835</td>
      <td>0.462748</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>191</th>
      <td>0.456781</td>
      <td>0.449321</td>
      <td>0.501564</td>
      <td>0.496869</td>
      <td>0.694333</td>
      <td>0.688926</td>
      <td>0.498937</td>
      <td>0.493470</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>192</th>
      <td>0.447441</td>
      <td>0.440803</td>
      <td>0.485399</td>
      <td>0.483510</td>
      <td>0.674612</td>
      <td>0.671808</td>
      <td>0.482636</td>
      <td>0.480019</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>193</th>
      <td>0.450135</td>
      <td>0.442987</td>
      <td>0.497422</td>
      <td>0.493428</td>
      <td>0.687756</td>
      <td>0.683027</td>
      <td>0.493345</td>
      <td>0.488634</td>
      <td>9</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
    <tr>
      <th>194</th>
      <td>0.448821</td>
      <td>0.441714</td>
      <td>0.490696</td>
      <td>0.487371</td>
      <td>0.682879</td>
      <td>0.678741</td>
      <td>0.487946</td>
      <td>0.483883</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>16</td>
    </tr>
    <tr>
      <th>195</th>
      <td>0.443511</td>
      <td>0.436343</td>
      <td>0.484538</td>
      <td>0.480040</td>
      <td>0.674689</td>
      <td>0.669694</td>
      <td>0.483001</td>
      <td>0.478206</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>32</td>
    </tr>
    <tr>
      <th>196</th>
      <td>0.459900</td>
      <td>0.451976</td>
      <td>0.511004</td>
      <td>0.505062</td>
      <td>0.700253</td>
      <td>0.693634</td>
      <td>0.511145</td>
      <td>0.504673</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>128</td>
    </tr>
    <tr>
      <th>197</th>
      <td>0.449845</td>
      <td>0.442440</td>
      <td>0.501510</td>
      <td>0.497780</td>
      <td>0.690488</td>
      <td>0.685359</td>
      <td>0.490910</td>
      <td>0.485880</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>400</td>
    </tr>
    <tr>
      <th>198</th>
      <td>0.450055</td>
      <td>0.442938</td>
      <td>0.497825</td>
      <td>0.494075</td>
      <td>0.686454</td>
      <td>0.681993</td>
      <td>0.493459</td>
      <td>0.488841</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>512</td>
    </tr>
    <tr>
      <th>199</th>
      <td>0.446104</td>
      <td>0.439218</td>
      <td>0.489334</td>
      <td>0.486293</td>
      <td>0.676767</td>
      <td>0.673071</td>
      <td>0.489919</td>
      <td>0.486513</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>1024</td>
    </tr>
    <tr>
      <th>200</th>
      <td>0.449941</td>
      <td>0.443273</td>
      <td>0.483142</td>
      <td>0.481716</td>
      <td>0.675012</td>
      <td>0.672317</td>
      <td>0.474202</td>
      <td>0.471665</td>
      <td>10</td>
      <td>Simple</td>
      <td>8</td>
      <td>7</td>
      <td>2048</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

    </details>
</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Si bien la tendencia no es estremadamente clara, de las pruebas realizadas podemos ver que el mejor conjunto de parámetros es:</p>
<ul>
<li>Tipo de red recurrente: Simple</li>
<li>Número de partidos a considerar: 10 </li>
<li>
<em>Features</em> por cada partidos del historial (para la RNN): 'play_home', 'is_cup', 'rating_diff', 'goal_diff', 'coach_continuity', 'relevance', 'is_friendly'</li>
<li>
<em>Features</em> del partido a modelar: 'Rating_diff', 'EqA_Local', 'is_cup', 'Equipo_A_coach_continuity', 'Equipo_B_coach_continuity', 'diff_num_partidos_diez_dias', 'diff_num_partidos_tres_semanas', 'is_friendly'</li>
<li>Batch size: 128</li>
</ul>
<p>Entrenemos entonces un último modelo con estos parámetros para analizar más en detalle su rendimiento:</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">n_partidos</span><span class="o">=</span><span class="mi">10</span>
<span class="n">M_A</span><span class="p">,</span><span class="n">M_A_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_RNN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">F_h_3</span><span class="p">,[</span><span class="s1">'Equipo_A'</span><span class="p">],</span><span class="n">n_partidos</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">M_B</span><span class="p">,</span><span class="n">M_B_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_RNN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">F_h_3</span><span class="p">,[</span><span class="s1">'Equipo_B'</span><span class="p">],</span><span class="n">n_partidos</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">H_Features</span><span class="o">=</span><span class="p">[]</span>
<span class="n">A_train</span><span class="p">,</span><span class="n">A_test</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">generate_input_for_NN</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">X_test</span><span class="p">,</span><span class="n">H_Features</span><span class="p">,</span><span class="n">N_Features_3</span><span class="p">,</span><span class="n">standarize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> 

<span class="n">fffmp</span><span class="o">.</span><span class="n">clean_all_models</span><span class="p">()</span>
<span class="n">time_steps</span><span class="o">=</span><span class="n">n_partidos</span>
<span class="n">predictors</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">F_h_3</span><span class="p">)</span>
<span class="n">tipo_RNN</span><span class="o">=</span><span class="s1">'Simple'</span>
<span class="n">recurrent_units</span><span class="o">=</span><span class="mi">100</span>
<span class="n">dense_units</span><span class="o">=</span><span class="p">[</span><span class="mi">500</span><span class="p">,</span><span class="mi">200</span><span class="p">,</span><span class="mi">80</span><span class="p">,</span><span class="mi">20</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span>
<span class="n">dense_drop_out</span><span class="o">=</span><span class="mf">0.1</span>
<span class="n">out_shape</span><span class="o">=</span><span class="mi">3</span>
<span class="n">input_shape_history</span><span class="o">=</span><span class="p">(</span><span class="n">time_steps</span><span class="p">,</span><span class="n">predictors</span><span class="p">)</span>
<span class="n">input_shape_now</span><span class="o">=</span><span class="n">A_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">'accuracy'</span><span class="p">,</span><span class="s1">'AUC'</span><span class="p">,</span><span class="n">F1</span><span class="p">]</span>
<span class="n">IFA</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Create_Mix_Model</span><span class="p">(</span><span class="n">tipo_RNN</span> <span class="p">,</span><span class="n">recurrent_units</span><span class="p">,</span> <span class="n">dense_units</span><span class="p">,</span><span class="n">dense_drop_out</span><span class="p">,</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">input_shape_history</span><span class="p">,</span><span class="n">input_shape_now</span><span class="p">,</span><span class="n">metrics</span><span class="p">)</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">validation_split</span><span class="o">=</span><span class="mf">0.1</span>
<span class="n">y_train_pred</span><span class="p">,</span><span class="n">y_test_pred</span><span class="p">,</span><span class="n">Results</span><span class="o">=</span><span class="n">fffmp</span><span class="o">.</span><span class="n">Full_train_and_Report</span><span class="p">(</span><span class="n">IFA</span><span class="p">,[</span><span class="n">M_A</span><span class="p">,</span><span class="n">M_B</span><span class="p">,</span><span class="n">A_train</span><span class="p">],</span><span class="n">y_train</span><span class="p">,[</span><span class="n">M_A_test</span><span class="p">,</span><span class="n">M_B_test</span><span class="p">,</span><span class="n">A_test</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">batch_size</span><span class="p">,</span><span class="n">epochs</span><span class="p">,</span><span class="n">validation_split</span><span class="p">,</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span><span class="n">history_plot</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">confusion_matrix</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span><span class="n">Report_print</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 10, 7)
Test shape: (10292, 10, 7)
Train shape: (83399, 8)
Test shape: (10292, 8)
Model: "Modelo_IFA"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 input_2 (InputLayer)           [(None, 10, 7)]      0           []                               
                                                                                                  
 simple_rnn (SimpleRNN)         (None, 10, 100)      10700       ['input_1[0][0]']                
                                                                                                  
 simple_rnn_1 (SimpleRNN)       (None, 10, 100)      10700       ['input_2[0][0]']                
                                                                                                  
 reshape (Reshape)              (None, 1000)         0           ['simple_rnn[0][0]']             
                                                                                                  
 reshape_1 (Reshape)            (None, 1000)         0           ['simple_rnn_1[0][0]']           
                                                                                                  
 input_3 (InputLayer)           [(None, 8)]          0           []                               
                                                                                                  
 concatenate (Concatenate)      (None, 2008)         0           ['reshape[0][0]',                
                                                                  'reshape_1[0][0]',              
                                                                  'input_3[0][0]']                
                                                                                                  
 dense (Dense)                  (None, 500)          1004500     ['concatenate[0][0]']            
                                                                                                  
 dropout (Dropout)              (None, 500)          0           ['dense[0][0]']                  
                                                                                                  
 dense_1 (Dense)                (None, 200)          100200      ['dropout[0][0]']                
                                                                                                  
 dropout_1 (Dropout)            (None, 200)          0           ['dense_1[0][0]']                
                                                                                                  
 dense_2 (Dense)                (None, 80)           16080       ['dropout_1[0][0]']              
                                                                                                  
 dropout_2 (Dropout)            (None, 80)           0           ['dense_2[0][0]']                
                                                                                                  
 dense_3 (Dense)                (None, 20)           1620        ['dropout_2[0][0]']              
                                                                                                  
 dropout_3 (Dropout)            (None, 20)           0           ['dense_3[0][0]']                
                                                                                                  
 dense_4 (Dense)                (None, 10)           210         ['dropout_3[0][0]']              
                                                                                                  
 dense_5 (Dense)                (None, 3)            33          ['dense_4[0][0]']                
                                                                                                  
==================================================================================================
Total params: 1,144,043
Trainable params: 1,144,043
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/50
587/587 [==============================] - 40s 63ms/step - loss: 1.0693 - accuracy: 0.4092 - auc: 0.5944 - f1_score: 0.4479 - val_loss: 1.0561 - val_accuracy: 0.4307 - val_auc: 0.6174 - val_f1_score: 0.4179
Epoch 2/50
587/587 [==============================] - 37s 63ms/step - loss: 1.0470 - accuracy: 0.4383 - auc: 0.6287 - f1_score: 0.4344 - val_loss: 1.0512 - val_accuracy: 0.4374 - val_auc: 0.6233 - val_f1_score: 0.4271
Epoch 3/50
587/587 [==============================] - 39s 67ms/step - loss: 1.0407 - accuracy: 0.4478 - auc: 0.6375 - f1_score: 0.4433 - val_loss: 1.0494 - val_accuracy: 0.4361 - val_auc: 0.6257 - val_f1_score: 0.4349
Epoch 4/50
587/587 [==============================] - 38s 64ms/step - loss: 1.0352 - accuracy: 0.4545 - auc: 0.6448 - f1_score: 0.4518 - val_loss: 1.0486 - val_accuracy: 0.4440 - val_auc: 0.6276 - val_f1_score: 0.4385
Epoch 5/50
587/587 [==============================] - 35s 59ms/step - loss: 1.0299 - accuracy: 0.4620 - auc: 0.6514 - f1_score: 0.4602 - val_loss: 1.0480 - val_accuracy: 0.4411 - val_auc: 0.6298 - val_f1_score: 0.4362
Epoch 6/50
587/587 [==============================] - 34s 58ms/step - loss: 1.0261 - accuracy: 0.4651 - auc: 0.6557 - f1_score: 0.4619 - val_loss: 1.0481 - val_accuracy: 0.4456 - val_auc: 0.6304 - val_f1_score: 0.4428
Epoch 7/50
587/587 [==============================] - 35s 59ms/step - loss: 1.0215 - accuracy: 0.4714 - auc: 0.6608 - f1_score: 0.4688 - val_loss: 1.0486 - val_accuracy: 0.4422 - val_auc: 0.6296 - val_f1_score: 0.4410
Epoch 8/50
587/587 [==============================] - 36s 61ms/step - loss: 1.0169 - accuracy: 0.4752 - auc: 0.6661 - f1_score: 0.4730 - val_loss: 1.0487 - val_accuracy: 0.4494 - val_auc: 0.6299 - val_f1_score: 0.4457
</pre>
</div>
</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,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%0A">
</div>

</div>

</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p></p>
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">Results</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,])</span>
</pre></div>

    </div>
</div>
</div>

    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>loss train</th>
      <th>loss test</th>
      <th>accuracy train</th>
      <th>accuracy test</th>
      <th>auc train</th>
      <th>auc test</th>
      <th>f1_score train</th>
      <th>f1_score test</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>1</th>
      <td>0.451779</td>
      <td>0.444569</td>
      <td>0.503722</td>
      <td>0.499809</td>
      <td>0.691521</td>
      <td>0.68665</td>
      <td>0.500161</td>
      <td>0.49546</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Finalmente, hemos obtenido un modelo con una precisión de practicamente 0.5 (considerando la prueba realizada sobre el <em>dataset</em> de <em>test</em>). Esto es una mejora considerable en relación a un clasificador aleatorio donde la precisión esperada es de 0.33.</strong></p>
<p>NOTA: Una estimación correcta del <em>accuracy</em> de nuestro modelo seleccionado requeriría la utilización de un <em>dataset</em> adicional cuyos datos nunca hayan sido utilizados a lo largo del proceso de selección del modelo. Originalmente la idea era utilizar el dataset adicional provisto como <em>test</em> por el desafío "<a href="https://www.kaggle.com/c/football-match-probability-prediction">Football Match Probability Prediction</a>" publicado en la plataforma Kaggle. Pero al momento de terminar el projecto el desafío ya había cerrado, por lo cual pensé que ya no sería posible. Sin embargo, he visto que aún queda la opción de hacer una <em>late submission</em>. Analizaré eventualmente si entrenar nuevamente el modelo y hacerlo, ya que en su momento no había guardado sus parámetros.</p>

</div>
</div>
</div>
</div>



  </div><a class="u-url" href="/FMP_UB_final_project_2022/2022/06/30/5_Modelado_de_los_datos.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/FMP_UB_final_project_2022/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
      </div>
      <div class="footer-col">
        <p>Proyecto Final del posgrado de Data Science de la UB 21/22</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Faegru" target="_blank" title="Faegru"><svg class="svg-icon grey"><use xlink:href="/FMP_UB_final_project_2022/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://www.linkedin.com/in/fernandez-aguirre" target="_blank" title="fernandez-aguirre"><svg class="svg-icon grey"><use xlink:href="/FMP_UB_final_project_2022/assets/minima-social-icons.svg#linkedin"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
